
#+TITLE: 370
#+AUTHOR: Andreas
* Introduction
- We are going for a minimal understanding of operating system
- kernel, process/thread management, communications between processes, memory
  management, file management, micro-kernels
** OS Themes
*** Manager model
 - The OS is a collection of independent mangers
 - Each manger only manages itself
 - Preventing improper use of devices

 #+DOWNLOADED: https://upload.wikimedia.org/wikipedia/commons/thumb/8/8f/Kernel_Layout.svg/440px-Kernel_Layout.svg.png @ 2021-03-01 14:48:33
 #+attr_org: :width 400px
 [[file:images/OS_Themes/2021-03-01_14-48-33_440px-Kernel_Layout.svg.png]]

*** Onion model
 - The OS is a series of layers
 - Outer layers can access resources contained in inner layers, but not inside to
   outside.
 - Basically the opposite of the ring security system

 #+DOWNLOADED: https://flylib.com/books/2/332/1/html/2/images/f11vs05.jpg @ 2021-03-01 15:06:17
 #+attr_org: :width 400px
 [[file:images/OS_Themes/2021-03-01_15-06-17_f11vs05.jpg]]

*** Resource allocator model
 - Related to the manager model, with mephasis on prividing the services programs
   need. The model must be fair resource wise.

*** Dustbin model
 This sees the OS as all the bits no one else wants to program/work with. Can be
 limiting because it limits you to what you can change below

** OS Design
*** All in one
 - MS-DOS
 - Early UNIX
*** Seperate layers
 This simplifies the verification and debugging because you only have to worry
 about what is on your layer. Although the model is not easy to change because
 once a layer has been created and is required to change, that means that all the
 layers that are further out then have to change (because the interface has
 changed). This can be generally pretty inefficient because we are having to
 rewrite a whole lot of code where we are only looking to have made a few
 changes. For example let's look at the depreciated function syntax in Microsoft
 XP, there can sometimes be hundreds in a given header. Very messy.
*** Modules
 - Much like the all-in-one but only loaded when necessary
 Linux and Windows, OSX has depreciated these sadly (the beacon of extensibility
 for OSX)
*** Microkernels
 - Use the client/server model much like X11
 - Many modern general purpose OSs use this approach:
   - Mach (basis for OSX)
   - QNX RT-OS
   - Exokernels - more radical microkernels
 - Virtual Machines
   - VM/CMS
   - Java
*** MS-DOS
 Written to provide the most functionality in the least amount of space. This
 means that MS-DOS is not divided into modules and is not well seperated. MS-DOS
 comes back to modernity in the form of exokernels.

 #+DOWNLOADED: screenshot @ 2021-03-01 17:12:30
 #+attr_org: :width 400px
 [[file:images/OS_Design/2021-03-01_17-12-30_screenshot.png]]

 **Early UNIX
 The UNIX OS consists of two separable parts:
 nn- System programs
 - The kernel
   - Consists of everything below the system-call interface and above the physical hardware
   - Provides the file sytem, CPI scheduling, memory mangement, and other
     operating-system functions; a large number of functions for one level

 #+DOWNLOADED: screenshot @ 2021-03-01 17:14:55
 #+attr_org: :width 400px
 [[file:images/OS_Design/2021-03-01_17-14-55_screenshot.png]]

*** THE Multiprogramming system
 A layered design was first used in THE operating system. It has six layers

 #+DOWNLOADED: screenshot @ 2021-03-01 17:16:06
 #+attr_org: :width 400px
 [[file:images/OS_Design/2021-03-01_17-16-06_screenshot.png]]
 
*** Windows NT client/server
 - Windows NT provided environmental subsystems to run code written to differnt
   OS APIs. This means that it is a hybrid system (you can run multiple types of
   binaries on it)
 - Parts are layered but some have been merged to increase performance. And many
   OS services are provided by user-level servers (less permissioned)
   - eg: environmental subsystems

 #+DOWNLOADED: screenshot @ 2021-03-01 18:33:07
 #+attr_org: :width 400px
 [[file:images/OS_Design/2021-03-01_18-33-07_screenshot.png]]

** Reading <2021-03-01 Mon>
 Computer system divided into 4:
 - hardware
 - OS
 - application programs
 - user
 OS provides no useful function by itself. It is simply an environment within
 whcih other programs can do userful work. From a system view we can look at an
 OS as a resource allocator, the OS acting as the manger of these resources. An
 OS is a control programs which manages the execution of user programs to prevent
 errors and improper use of the computer. We have no formal definition of what is
 part of the operating system.
 The number of features we have in a OS is increasing. We now also have
 middleware which is a set of softaer frameworks that provide additional services
 to application developers.

* History
** UNIVAC
In the 1950s we had the expensive UNIVAC. Users booked booked the machine in
which they setup the program data cards, did all the setup and loading,
controlled the computer though console switches and debugged using console
lights/switches. Generally this required a team of people to operate and a large
amount of knowledge about the computer. The booking system lead of inefficient
use of the machine (idle around early hours). Running a program took a whole lot
of effort and has many steps.
*** OS
Most of the OS at that stage was comprised of the decisions and actions of the
user. There were rudimentary components such as standard IO which were the
forerunner of device drivers and sys calls. It had the following features:
- Only one program at a time
- User interface was almost bare machine
- Some standard IO routines (some useful code to read and write to tape and printers)
- IO was polled for (no need for anything faster
*** Off-lining
Off-lining offered multiple seed ups for the operators allowing for multiple
operators and batching jobs together (called phasing) which keeps the programmer
away from the computer
- Magnetic tape arrival improved IO
- small cheap computers (SCC) did the slow IO from paper to mag tape
- big expensive computer (BEC) used mag tapes for IO
- Several programs submitted to the BEC on one tape
- The first parallelism in computing

#+DOWNLOADED: screenshot @ 2021-03-03 08:44:43
#+attr_org: :width 400px
[[file:images/History/2021-03-03_08-44-43_screenshot.png]]
**** Changes
- No hardware/OS changes
- procedures more formal
- The first command UIs were instruction sheets to the operators
- The next step being to automate some of these procedures
*** Resident monitors
- The computer operators had formal procedures
- Get the computer to help with these procedures
**** What it needs:
- A programs always in memory (hence "resident")
- A control language (ie commands to give to the resident monitor)
- The starting point of OSs
**** The Resident monitor could:
- Clear memory used by the last program (with operator help)
- Load the next program
- Find the data for the program
- Jump to the start address of new program, returning to the resident monitor
  when finished
- it also maintained the standard IO routines in memory
**** What's changed?
- No memory manement - every address still reachable
- No FS
- IO still polled for
- Programmers now forced to use the standard IO routines
- Only one program at a time but can have two programs in memory
- Problems with bad programs
***** Changes in hardware
- Disc drives (very much faster and far more storage)
- Interruptible processors which changed the way that IO was performed
- Development from single location return addresses to the use of a stack
- IO devices and teh CPU can execute concurrently
- IO from the device to local buffer of controller
- CPU moves data to main memory from local buffer of controller
- Device controller informs CPU that it has finished its operation by causing an interrupt
** SPOOLing
- Simultaneous Peripheral Operation On-Line
- Means that time waiting for IO can be used, no longer need the small SCC for
  IO
*** Memory now holds
- A running program
- Interrupt driven card reader control program
- interrup driven printer control program
- disk control software
- buffers for data being transferred between the computer and devices
- A rudimentary file system

#+DOWNLOADED: screenshot @ 2021-03-03 09:10:01
#+attr_org: :width 400px
[[file:images/History/2021-03-03_09-10-01_screenshot.png]]

** Multiprogramming
We are doing things simultaneously (processing program, reading cards for
another program, printing data for another program). Thus the next step is
obvious. We should have several programs in memory at once. What do we need?
- A lot more memory
- A program scheduler
- A way of keeping track of programs in memory
- Where data is
- Better error handling
- A way to preserve memory of each program
** Memory Protection
This can be provided by software but done more efficiently and safely by
hardware. There are two requirements:
- Operating modes and privileged instructions
- Limited address range
*Provide hardware support to differentiate between atleast two modes of
 operation*
1. User mode - execution done on behalf of user
2. Kernel mode - execute done on behalf of the OS
We need both of these modes because:
1. If we had modes and privileged instructions but full memory access then
   there's no memory protections and also no protection of privileged
   instructions. Meaning that you could put any code in the system areas of
   memory
2. If we could limit memory accesses but there were no modes and privileged
   instructions then a user could call the /privileged/ instructions to edit the
   area of memory available to everyone
** Processor Modes
Mode bit added to the hardware processor status register to indicate which mode
the process is operating in, meaning that interrupts, faults and sys calls cause
the processor to change mode and jump to a location, privileged instructions
cannot be executed in user mode
#+DOWNLOADED: screenshot @ 2021-03-03 09:27:19
#+attr_org: :width 400px
[[file:images/History/2021-03-03_09-27-19_screenshot.png]]

** Memory Protection
Each process gets allocated an area of memory which it can access and all access
outside that memory space causes an exception. Processes were designed to load
in a particular partition (base-limit registers, memory pages). Devices can be
protected using memory potection or privileged instructions.
#+DOWNLOADED: screenshot @ 2021-03-03 11:11:56
#+attr_org: :width 400px
[[file:images/History/2021-03-03_11-11-56_screenshot.png]]
*** Batch systems
With memory protection and processor modes we can safely have multiple programs
in memory
#+DOWNLOADED: screenshot @ 2021-03-03 11:14:07
#+attr_org: :width 400px
[[file:images/History/2021-03-03_11-14-07_screenshot.png]]
Batch system has had a number of innovations:
- Disks with file systems where files are associated with owners
- Scheduling became automated. In order to effectively utilise all of the
  hardware all the time we had to automate scheduling of executing. The
  operators became too slow and individual jobs could be suspended or killed
  which allowed other jobs to progress
- Accounting could now be done automatically but programmers job hasn't rea
* Scheduling
To maximize the use of our computer we need to schedule jobs. We needed to know
certain properties about how many devices a process would user, what/how many
files, how much output it was likely to produce, how long it's expected to
take. Some of this information is submitted on control cards. Some of this
information was *inferred* by the job queue the job was submitted on.
* Time-sharing systems
After personal machines came out people started to share systems with multiple
users per machine. Things are still running synchronized at this point so we
have to share resources. People will want good response times (<5 seconds). The
people working at the machines are unpredictable, they will want to use files
and devices at any time and it is hard for the OS to work out what processes
will be compute/IO intensive.
We split each user to a process where there is usually atleast one process to
deal with commands for each user, plus the normal work that they requested
In a time share system the devices and CPU can't be used to capacity otherwise
the response time will take too long, thus there has to be some /slack/ in the
system such that the scheduler can do its thing.
** Time-sharing changes
- Over demand for resources can happen easily (but we hope occasionally)
- High-level scheduling decisions are made by the users (as the scheduler
  doesn't know how long something will take by itself)
- Security a problem
- More user administration
  - Profiles, defaults, rsource limits
  - Different types of users (sysadmins and users)
* Multiprocessor systems
- Tightly coupled system: processors share memory and a clock
  - communication (between processors) takes place through the shared memory
  - almost all computers are now considered tightly coupled systems
** Advantages of parallel systems
- Increased thoughput
- Economical for the increase in performance
- increased reliability
- graceful degradation
- fail-soft systems
** Symmetric
* Tutorial 1
** What is kernel mode? How does it differ from user mode?
Kernel mode is a CPU mode which has access to a privileged set of instructions
while user mode must do syscalls to access these. A lot of these privileged
instructions are associated with the hardware and virtual memory. We have these
two modes so that user modes don't have access to hardware and virtual memory,
otherwise it could just overwrite memory anywhere.
- Hardware mapping
- virtual memory
- Scheduling
Are some of the privileged instructions
** How do ordinary programs get access to privileged instructions?
Though *system calls*. It's a hardware mechanism that causes the processor to
jump to a predefined location and enter kernel mode. This location will contain
some special code that is part of the kernel to handle the call. The kernel will
then use privileged instructions. The user level program has no control until
the syscall returns. The kernel can run checks to make sure that the caller has
permission to do what they want to do. 
* Virtual Machines
** Introduction
Normally we think of the OS as being the first layer above hardware, but this
isn't always true as many virtual machine sysetms provide a layer beneath the OS
and then the OS runs ontop of that
Virtual machine systems can give everyone on the OS and hardware that they
want.
*** Advantages
- You can chose you own OS
- You can modify/develop things and crash without crashing other guests or
  having to reboot
- It adds an extra level of saftey where each users machine is completely
  separate from all the others
- Virtual servers. sometimes when you run server software it will asume that this
  is the only thing that the machine will be doing is running that piece of
  sever software. With virtualisation you can make it so that you can run
  mulitple servers that assume the whole server is for them
*** Disadvtanages
- Resources are not fluid between VMs (they cannot be shared between each other)
  so you might have some ram allocated for one machine and not used which measn
  it is essentially wasted
- Adds an extra layer of complexity
- Getting things to run fast is difficult (syscalls have to be trapped and
  processed instead of just doing a normal call which adds computational
  overhead. Furthermore with virtualising a non compatable architecture the
  instructions have to be translated as well as translating syscalls (because
  the syscalls on OSX are not the same as on windows for example...)
** Virtual Servers
- Many applications (eg databases) assume that they are the only thing that is
  going to run on the machine
- Using virtual servers means that multiple servers can be running
  simultaneously on one machine which means that errors on one VM do not affect
  the other servers on the VM
- As long as the expected loads are not oging to overwhelm the machine this is
  definitely a cost effective solution (no longer need multiple machines to buy)
- You have the added flexibility of migrating these machines without having to
  reboot
- You can also copy machines because it's trivial to create a new VM
- IBM where all the terminals where just being run on a single machine, it also
  means that they were able to pick what OS they would like
- - disadvantage that can't share virtual networking hardware
- some programs on VM are a problem in terms of speed (where it might not run as
  fast
** Virtualisation
Popek and Goldbeg. There are a number of  properties that virtualization should
hold. These two people theorized what it was:
1. Fidelity - software should run identically (expect for speed) on the VM as on
   a bare metal machine
2. Performance - most instructions in a VM must run directly on the hardware
   (therefor at the same speed as on the real machine). Where as in an emulator
   you would simulate a CPU which would emulate the instructions.
3. Safety (aka resource control) - the VM/VMM is in complete control of system
   resources and must be safe from any actions of the VM. Also VMs must be
   isolated from each others resources.
** Design of IBM's VM
#+DOWNLOADED: screenshot @ 2021-03-08 15:59:20
[[file:images/Virtual_Machines/2021-03-08_15-59-20_screenshot.png]]
- a) is a normal machine
- b) is a VM with a number of guest hosts
Cpu scheduling can creating the appearance that users have their own
processor. Where spooling and FS can provide virtual line printers and virtual
disc.Virtual user and kernel modes are provided and the VM did very little
emulation. The kernel in these VMs where actually run in usermode (instead of
kernel mode) so there had to be something called *traps* which detected syscalls
and redirected them to the true kernel (hypervisor). Only the privileged
instructions needed to be emulated
** Hypervisor Types
There are three types
*** Type 0
implemented in hardware and firmware, it loads at boot time. The guest OSs load
into partitions separated by the hardware. They are allocated
dedicated resources (eg processors, memory, devices. Guests Oss are native
with a subset of the hardware.
#+DOWNLOADED: screenshot @ 2021-03-08 16:05:32
[[file:images/Virtual_Machines/2021-03-08_16-05-32_screenshot.png]]
*** Type 1
- Special prupose OSs where they also load at boot time and provide the
  optimised environment to run guest Oss. Now supported by hardware on Intel and
  AMD processors
- Run in kernel mode or below
- They implement device drivers and the guests access the devices though them
  (manged hardware)
- They also provide services to manage the guests (this is usually the monetized
  part) where backup, monitoring. So these VMs used in data centres of the cloud
- Some standard OSs can also be made to run as Type 1 (Windows hyperV and
  enterprise Linux)
*** Type 2
- These run as applications on the host OS (eg VMWare/Virtualbox)
- Morder type 2 VMs part of the VMM runs in kernel mode
** Virtualization problems
Until 2006 all x86 Cups had problems with classical *trap and emulate*
virtualization. Some instructions ran in both user and kernel modes (/they just
worked differently in kernel mode/). So they were not privileged instructinos
and executing them did not cause a trap into the VMM. So we have basically
missed a syscall. There are a set of instructions allowed of the program to
determine if it was running in kernel mode. They kenel of a guest OS should be
privileged however if it checked all it would see is user mode (because the
kenel for the guest OS runs in usermode). This broke the fidelity
requirement. These were labelled as *sensitive instructions*. Even worse there
were problems with protecting the page table information and keeping it all
consistent which is critical for ram functionality.
*** Solution
- User level code is fine. It will act normally if an exception occurs (caught
  be the VMM and passed back to the guest OS kernel)
- Which makes the only problem at the kernel of the guest OS
- The solution: *Binary translation*:
  - Code running in kernel mode is translated at run time into something which
    doesn't have these problems
  - The translation is very simple (and hence efficient) which did not have the
    expected hit on performance that you would expect
  - It only translated code which is actually run where much of the code is
    exactly the same as the original
  - The translated code is cached and resued which means that we don't have to
    re translate routines that we are using multiple times
  - Uses all sorts of tracks to speed up emulation
- This solution performed very well compare to true hardware virtualization
  which struggled with page table modifications
#+DOWNLOADED: screenshot @ 2021-03-08 16:24:26
[[file:images/Virtual_Machines/2021-03-08_16-24-26_screenshot.png]]
** Hardware Virtualization (x86)
- Both Intel and AMD have developed their own solutions to deal with
  virtualization
- Intel VT and AMDV which are extra high privilege area for the VMM. OS still
  runs ring 0 (kernel mode)
- Hardware transitions from ring0 to the VMM
- Processor state is maintained for each guest OS (and the VMM) in seperate
  address spaces. Context switching between guests is done in hardware. VCPUs.
- AMD-V included tagged translation lookaside buffers (so virtual memory didn't
  ahve a hit when changing virtual machines)
- Both AMD and Intel processors now do Second Level Address Translation (SLAT)
  - determine the guest physical address from the guest virtual address using
    hardware
  - then turn the guest physical address into the host physical address also
    using hardware
#+DOWNLOADED: screenshot @ 2021-03-08 16:30:09
[[file:images/Virtual_Machines/2021-03-08_16-30-09_screenshot.png]]
** OS level virtualization (Application containment)
- If virtualizing servers we can often use the same OS. This means that we
  vitalize less
- *Containers* look like servers - they can be rebooted separately, ahve their
  own IP addresses, root, programs, etc.
- But they all use the same underlying kernel but they are *still separate from
  each other*
#+DOWNLOADED: screenshot @ 2021-03-08 16:42:37
[[file:images/Virtual_Machines/2021-03-08_16-42-37_screenshot.png]]
** More VM styles
- Para virtualization - Xen - software approach. Requires modifications to the
  OS source code to use the Xen layer. eg to read from a File the guest directly
  calls host read routines
- Application virtualization - and application runs on a layer which provies the
  resources it needs even though it may be running on a different OS = eg WINE
  or running programs from old versions of windows on a newer one
  - Example as CrossOver. There is an extra level of softawre but it is *not*
    providing an virtual machine
- Programming environment virtualization:
  - Eg: java
  - implement a different architecutre on top of any hardware/OS
  - Programs are compiled to the Java VM or CLR architecture then run by either
    compiling or interpreting that code
** WSL
- Remember that the NT kernel (underneath all versions of WIndows for the last
  25 years or more) was designed to run programs written for more than 1 OS API
  (Win32, OS/2, Posix)
- Uses user level pico processes and corresponding kernel drivers
*** V1
#+DOWNLOADED: screenshot @ 2021-03-09 11:13:00
[[file:Virtual_Machines/2021-03-09_11-13-00_screenshot.png]]
You start iwht a Win32 program (bash.exe) which communicated with the linux
session manager saying that it wants to create an environment to run the linux
program. The session manager creates a picoprocess (has little information
associated with the OS) and inside we set up a standard linux process
(init). When the picoprocess creates a syscall it calls out of the LXCore which
then is processed to the NTCore 
*** V2
There were two main problems with the V1 WSL:
1. I/O was slow (all file access had to be translated from linux calls into
   correstponding windows calls)
2. The linux kernel wasn't there (so we couldn't add Linux kernel modules, eg
   FUSE)
The solution was to revert to a more traditional Linux running in a VM ontop of
Hyper-V (which is part of most installations of Windows)
** C and OS implementations
C has been used for OS implemenations because it is low level meaning that you
can access memory. It maps easily to ASM making it easier to debug and also easy
to inline ASM which makes it easy to change to processor state with
efficiency. Has small requirements for runtime support.
** Accessing registers
- You can use the "register" storage class specifier to say that a veriable
  should be kept in a processor register: /register long number = 1234/
- However this does not guarantee that a value is stored in a register, it
  depends on the number of availiable registers
- Also compilers do a really good job of optimising register usage and it is not
  usually a good idea for a programmer to worry about this level of optimisation
- Memory mapped registers can be accessed directly using pointer manipulation
*** Volatile
type qualifier /eg volatile unsigned char *reg;/
- This means the variable may change in a non local way, in other words there is
  no way the compiler could possibly know whether the value has changed or not
  between references
- So the compiler is not allowed to optimise accesses. Every single read must go
  back to the register and retrieve the current value
  - Memory mapped device registers
  - Values modified in interrupt routines
  - Values modified in another thread
#+BEGIN_SRC c
unsigned char *reg = (unsigned char*)0x1000;
while (*reg) {}
#+END_SRC
In the above example it would loop forever only ever doing one read on *reg,
while if this was volatile there is no guarantee that this value is the same
(could have been modified in another thread). Thus it would have to read
everytime
** Memory Mangement
- All local variables disappear when functions are returned from
  - Stack frame for every function call
  - There is a limit to the size of the stack (especially for threads as each
    thread needs its own stack
- Areas of static memory
  - Global variables
  - Static variables
  - If in a function it maintains its value even when the function is returned
    from (area of memory that the header of the binary allocated before runtime)
  - The advantage
#+DOWNLOADED: screenshot @ 2021-03-09 11:47:00
[[file:images/Virtual_Machines/2021-03-09_11-47-00_screenshot.png]]
Above is how the layout of a runtime for a compiled program:
- Stack grows downwards
- Heap grows upwards
*** Dynamic Memory
- C requires explicit control of dynamic memory because there is no GC
  avaliable. Also you can't really have GC on a embedded device, it's pretty
  great that this is bare bones
- We use /malloc/ or /calloc/ to allocate memory on the heap and free it with
  /free/
- When we call /malloc/ we are actually allocating a little bit more memory to
  store the size of the struct that you are storing. This means that you can
  just call /free(x)/ which will *detect* the size of the struct such that it
  can be freed without having to pass in /sizeof(structname)/
*** Running commands from a C program
/system("ps aux")/
This creates a shell and gets it to execute the command
* Tutorial 2
I'd like to get some more details about programs (fork and pipe)
** Processes vs Threads
Threads are more lightweight because they share memory (because they are within
the same process), but they do have their own stack.
Multithreading is great! We can process more information and we can do async
things and have completely seperate IO
- You should be careful with multiple threads accessing the same resource. We
  can use mutex locks to prevent this
Processes are created with =fork=
- Child process will recieve a copy of the code and variables
- A process ID value of 0 incidcates its the child and any non zero value means
  its the parent
- Processes unlike threads do not share memory
- Everything after the =fork= call will happen twice
- =wait= unless the child process is finished with execution
- Now you have a way to do things in parallel but how do I share data between
  forks?
- Two ways
  - Pipes
  - Shared memory
** Pipes
- It's like a stream of data. When the parent calls =read()= it will wait until
  there is something to read in the pipe (stalling until then)
read, write, pipe, fork
** Shared memory
- Shraed memory areas created using mmap
- Memory that can be accessed by multiple processes
* Processes
- The thing which represents our work to the system
- Sometimes refereed to as a heavyweight process. Sometimes called /An instance
  of a program in execution/
- *An instance*
  - May be more than one version of the same program running at the same time
  - Each instance has resource limitations, security information and rights etc
- *Of a program*
  - So it includes code, data, connections (to files, networks, other processes)
    and access to devices
- *In execution*
  - It needs the processor to run. But it doesn't run all the time. So it needs
    information about what it is up to stored somewhere
** Two parts to a process
1. Resources, the things the process owns (may be shared). Also information
   about the process
2. What the process is doing (the streams of execution)
- Traditional processes had /resources/ and a /single current location/ (trad
   UNIX). The resources part is called a /task/ or a /job/. The location part is
   commonly called a /thread/
- Most operating system now provide support to keep these parts separate.
** Threads
- Sometimes referred to as lightweight processes (*make sure that you don't get
  these concepts confused*)
- *A sequence of instructions being executed when there is no external
  intervention*
- Sometimes we want to share data as well as code. (Could just shre files or
  memory and not use threads)
- Easier to create than a process. They provide a nice encapsulation of a
  problem within a process rather than multiple processes
- Easier to switch between threads than between processes
** Thread implementation
1. *User level (green threads)
   - The OS only sees one thread per process
   - The process constructs other threads by user level library calls or by hand
     (ie: no syscalls are made)
   - User level control over starting and stopping threads
   - Usually a request is made to the OS to interrupt the process regularly (an
     alarm clock) so that the process can schedule another thread
   - The state of threads in the library code does not correspond to the state
     of the process
2. *System level*
   - The OS knows about multiple threads per process
   - Threads are constructed and controlled by sys calls
   - The system knows the state of each thread

** User level threads
 Two was to support user level and operating level
 - User level therads work even if the OS doesn't support threads
 - Easier to create (no sys calls) because it's just a norm library call meansing
   that there is no switch into kernel mode
 - Control can be application specific
   - Sometimes the OS does give the type of contorl an application need
   - eg: precise priority levels, scheduling decisions according to state changes
 - Easier to switch between - saves two processor mode changes
   - Can be as simple as saving and loading registers
 - So why woudl anyone want to use system level threads

** System level threads
 - Each thread can be treated separately
   - Rather than using the time slice of one process over many threads
   - Should a process with 100 threads get 100 times the CPU time of a process
     with 1 thread?
   - *A thread blocking in the kernel doesn't stop all other threads in the same process*
     - With user levels threads if one thread blocks for Io the OS ses the
       process as blocked for IO
   - *On a multiprocessor (including multi-core) different threads can be
     scheduled on different processors*
     - The above can only be done if the OS knows about the threads
     - Even then sometimes doesn't work - standard python has system level
       threads but the Global Interpreter Lock (GIL) means that only one runs at
       a time ven on a multicore machine

*** Some further information as to why user level threads block on syscalls
So when a single thread calls a blocking system call, that single thread has
to block until that system call returns, and while it is blocked it can't do
anything. One thing in particular that it can't do while blocked is switch to
another user-thread-context and run some more code, because there is no way to
call the switch-to-the-other-user-thread-context routine. Who would call it? The
only "real" thread available is the one that is blocked inside the system call,
and it can't do it because it is blocked inside the system call.

Kernel threads handle blocking system calls differently

With kernel threads, the kernel is aware of all of the threads inside the
process, because the kernel created them (on behalf of the application) and
manages them directly, so the kernel can schedule any of them directly. Because
of that, when thread A blocks inside a system call, the kernel/scheduler can go
ahead and run thread B for a while instead, because the kernel knows that thread
B exists. (Contrast this with the user-threads case, where the kernel can't
schedule thread B to run, because the kernel doesn't know that thread B exists;
only the user-application itself knows about the existence of the user-level
threads)

[[https://stackoverflow.com/questions/40877998/why-blocking-system-calls-blocks-entire-procedure-with-user-level-threads][Source]]

** Jacketing
 One major problem with user-level threads is the blocking of all threads within
 a process when one blocks.  One possible solutions is known as jacketing:
 - A blocking syscall has a user-level jacket
 - The jacket checks to see if the resource is available (eg when the device is
   free)
 - If not another thread is started
 - When the calling thread is scheduled again (by the thread library) it once
   again checks the state of the device
 - So there has to be some way of determining if resources are available to
   accept requests immediately

** The best of both worlds
Ok so both user level and kernel level threads have their own advantages and
disadvantages but what if we can combine them? Solaris is an example of this
with both system level and user level threads. It has a number of definitions
before we get started
- LWP: A light weight process (What we ahve been calling system level threads)
- Kernel threads - active within the kernel where each LWP is associated with one
  kernel thread
- One or more user threads could be mulitplexed on each LWP
- A process could have several user and several LWPs
- The number of LWPs per process was adjusted automatically to keep threads
  running

#+DOWNLOADED: screenshot @ 2021-03-15 13:50:16
[[file:images/Processes/2021-03-15_13-50-16_screenshot.png]]

** Original Linux threads
- Clone: makes a new process with particular shared things that you can
  configure what is shared (memory, open files (descriptors), signal handlers
- From one point of view original threads are processes - but they share all
  resources and hence the advantages of threads
- This was not POSIX compliant
*** Original Linux Threads and PSIX
- Can't be set to schedule threads according to priority within a process
  - each thread is scheduled independently across all threads/processes in the
    sytem
  - Can't send a signal to the whole process because each thread is considered
    to be a process thus if you send a sigint to kill then you're just killing a
    thread (You should never kill threads!)
    - The reason you should never kill threads is that they could be in the
      middle of doing work and leave something half mutated which is bad for
      program state
    - Or it locks a mutex, is killed and that mutex is never unlocked. Now you
      can never access the data associated with that mutex again
  - Ordinary system calls (eg read) are not cancellation points
  - Starting a new program in one thread doesn't kill the other threads in the
    same process
  - When an original Linux thread blocks doing IO do all the other threads in
    the same process stop?
** Reading <2021-03-16 Tue>
*** Zombies and orphans
A process that has terminated but whose parent has not yet called =wait()= is
known as a *zombie* process. All processes transition to this state when they
terminate but generally they exist as zombies only briefly. Once the parent
calls =wait()=, the process identifier of the zombie process and its entry in
the process table are released
Now consider what would happen if a parent did not invoke =wait()= and instead
terminated, thereby leaving its child process as *orphans*. Traditional UNIX
systems addressed this scenario by assigning the /init/ process as the new
parent to the orphan process (init is the root process given by systemd). The
init process periodically invokes =wait()= thereby allowing the exit status of
any orphaned process to be collected and releasing the orphans pid and process
table entry
*** Process management
A small prelude note on when a process has multiple threads and then that
process forks. The fork operation copies over the stack of the previous
process. Each thread has its own stack therefore when you call fork on a process
with multiple therads you're only going to ever copy over the stack/memory of
the thread *from which you forked the process from*

- A new process is created by running =fork()=
- A new program is run after the call to =exec()=
I think that exec loads a new binary into the process.

**** Process
- Process ID (PID): Each process has a unique identifier
- Credentials: Each process must have an associated user ID and one or more
  group IDs that determine the rights of a process to access sytem resources and
  files
- Personality:
* Process Control Blocks
PCBs. They are where the OS can find all the information it needs to know about
a process. Some of these things that the OS would like to know about processes
are:
- memory
- open streams/files
- devices, including abstracts ones like windows
- links to condition handlers (signals)
- processors registers (single thread)
- process identification
- process state (including waiting information
- priority
- owner
- which processor
- links to other processes (parent, children)
- process group
- resource limits/usage
- access rights
- process result which may be wanted by another process
- time that the process has been running 
All of the above doesn't have to be kept together at (loaded into memory at the
same time or kept around the same locations of memory) because different
information is needed at different times
- UNIX for example has two separate places in memory with this information. One
  of them is in the kernel the other is in the user space. Windows does the same
** UNIX process parts
- The PCB is the box labelled *process structure* but the *user structure*
  maintains some of the information as well (only required when the process is
  resident)

#+DOWNLOADED: screenshot @ 2021-03-16 13:19:40
[[file:images/Process_Control_Blocks/2021-03-16_13-19-40_screenshot.png]]
You may be wondering about the two stacks here in the swappable process
image. It's because there are actually two stacks per program:
- User stack:
  - The normal stack, can overflow
- Kernel stacks
  - Copy parameters & check them. This is done because kernel calls have to be
    safe!
  - Typically doesn't overflow

** Winows NT PCBs
Let's not forget about windows here. Windows does something very similar to UNIX
in the case of tracking process information. Information is scattered in a
variety of objects
- Executive Process Block (EPROCESS) includes
  - KPROCESS and PEB (Process Environment Block)
  - pid and ppid (the ppid is not visible to Wind32)
  - file name of the program
  - window station (the screen or remote terminal
  - exit status
  - create and exit times
  - links to next process
  - memory quotas
  - memory mangement info
  - Ports for exceptions and debugging
  - Security information
Kernel process block (KPROCESS) includes info the kernel needs to /schedule/
threads
- Kernel and user times
- Pointers to threads
- Priority information
- Process state
- Processor affinity
Process Environment Block (*PEB*) includes infor which needs to be writable in
user mode
- image info: base address, version numbers, module list
- heaps (blocks of one or more pages)
** Process table and Thread strictures
- Process Table is a collection of PCBs (which was originally just an
  array). Now it's more a dynamic collection of pointers to PCBs
They have the following things inside of them
- Thread structures (like PCBs)
  - private memory (runtime stack) and static storage for local variables
  - processor registers
  - thread identification
  - thread state (including waiting informatin)
  - priority
  - processor
  - associated process
  - thread group
  - thead result (maybe waited for by another thread)
** Process/Thread states

#+DOWNLOADED: screenshot @ 2021-03-16 13:30:49
[[file:images/Process_Control_Blocks/2021-03-16_13-30-49_screenshot.png]]

#+DOWNLOADED: screenshot @ 2021-03-16 13:31:13
[[file:images/Process_Control_Blocks/2021-03-16_13-31-12_screenshot.png]]

** Being created
- Different methods of creating processes
  - Create process system call (takes a program name or a stream with the
    program data)
  - Copy process system call (a strange way of doing it but is now very
    widespread thanks to UNIX
  - Create a new terminal session
Whichever way
- Find a spare (or create a new) PCB (what if there isn't one?)
- Mark it as "being created"
- Generate a unique identifier
- Get some memory (what if there isn't any?) or
- Alteast fill in the page table entries
- Set up PCB fields with initial values
- Set priority, resource limits
- When all set up, change the state to "runnable"
- This could be done by inserting into a queue of runnable processes
Some things to note:
- What about other resources?
- Some OSs carefully allocate resources before a process runes (this prevents
  deadlock later)
- Others leave these to the process to collect as it runs
** Fork
- The UNIX fork call duplicates the currently running process
  - Parent process (the one which made the call
  - Child process (the new one)
- Traditionally memory was duplicated (the code was shared even from earliest
  days)
  - Share open files as well
  - Open file information blocks will have the count of processes using them
    increased by one
  - And shared memory regions
  - Fork returns 0 in the child process and the child's pid in the parent

#+BEGIN_SRC c
if (fork() == 0)
  execl("nextprog", ...);
#+END_SRC

#+DOWNLOADED: screenshot @ 2021-03-17 09:54:29
[[file:images/Process_Control_Blocks/2021-03-17_09-54-29_screenshot.png]]
This image shows that the only thing that isn't copied is the code. When you're
calling =execl= you're first copying the original program then reloading a new
binary into that process. It's a waste copying the original program then. The
next solution explores different solutions to this
** exec
- Checks to see if the file is executable
- saves any paramters in some system memory
- releases currently held memory
- loads the program
- moves the saved parameters into the stack space of the new program
- ready to run agian
- Fork used to cipy the data memory of the process. If the child is going to do
  an exec this is a waste of effort. Particularly bad with virtual memory

*** Two solutions
1. Copy on wite
   - No copy is made at first
   - The data pages of the parent process are set to read only
   - If a write occurs the resulting exception makes a copy of the page for the
     process (both copies are then marked writable)
2. vfork
   - Trust the programmers to know what they are doing
   - *With vfork parent process blocks until child finished or calls exec*
   - *Copy on write is the predominant strategy*
     - It is used in many situations to improve throughput

** NT Process creation
- Open .exe file and create a section object (actually quite complex because of
  the different subsystems)
- Create NT process object
- Set up EPROCESS block
- create initial address space
- Create KPROCERSS block
- Finish setting up address space (including mapping the section object)
- Adds process block to the end of the list of active processes
- Set up PEB
- Creates initital thread (initially suspended)
- Win32 substesm is notified about the new process (includes the arrow and
  hourglass cursor)
- initial thread starts
- Goes through more startup in the context of the new process (includes loading
  and initializing DLLs)
* Some notes for A1
- Condition variables
  - A thead sometimes needs to wait vefore continuing
  - You need to wait until another thread gives you more work to do
  - Rather than continually running checking on global variables we can check
    and if the state says you should wait then wait until the condition changes
  - That is what condition variables are used for
 
* Runnable
- On a single core only one process/thread can run at a time (Not actually true:
  Simultaneous Multithreading SMT)
- Many may however be runnable: either /running/ or /ready to run/

#+DOWNLOADED: screenshot @ 2021-03-22 14:22:20
[[file:images/Runnable/2021-03-22_14-22-20_screenshot.png]]

** Preemptive multitasking
- A clock interrupt causes the OS to check to see if the current thread should
  continue
  - Each thread has a time slice
  - How is the time slice allocated?
- What advantages/disadvantages does preemptive multistasking have over
  cooperative mulittasking?
- Advantages:
  - Control
  - Predictability
- Disadvantages
  - Critical sections
  - Efficiency

*** Cooperative mulititasking
- Two main approaches
  1. a process yeilds its right to run
  2. system sopts a process when it makes a system call
- This does *NOT* mean a task will work to completion without allowing another
  process ro run. eg: Macintosh before OSX and early versions of Windows

**** A mixture
- Older versions of UNIX (including versions of LINux before 2.6) did not allow
  preemptive multitasking when a process made a system call
  
** Context switch
- The change from one process running to another one running on the same
  processor is usually referred to as a "context switch".
- What is the context?
  - registers
  - memory: including dynamic elements such as the call stack
  - files, resources.
  - But also things like caches, TLB values: these are normally lost
- The context changes as the process executes
- But *normally* a "context switch" means the change from one process running to
  another, or from a process running to handling an interrup. Whenever the
  process state has to be stored and restored.

#+DOWNLOADED: screenshot @ 2021-03-22 14:34:24
[[file:images/Runnable/2021-03-22_14-34-24_screenshot.png]]

** Returning to running
*State transition*
- Must store process properties so that it can restart where it was
- If changing processe the page table needs altering
- Rest of environment must be restored
- If changing threads within the same process simply restoring registers might
  be enough
- Some systems have multiple sets of registers which means that a therad change
  can be done with a single instruction

** Waiting
- Processes seldom have all the resources they need when they state
  - memory
  - data from files or devices: eg keyboard input
- Waiting processes must not be allowed to unnecessarily consume resources, in
  particular the processor
  - state is changed to waiting
    - my be more than one type of waiting state
    - short wait eg for memory
    - long wait eg for an archived file (see suspend below)
  - removed from the ready queue
  - probably entered on a queue for whatever it is waiting for
- When the resource becomes available
  - state is changed to runnable
  - removed from the waiting queue
  - put back on the runnable queue

** Suspended
- Another type of waiting (remember C-z)
- operators or OS temporarily stopping a process (ie it is not (usually) caused
  by the process itself)
  - allows others to run to completion more rapidly
  - or to preserve the work done if there is a system problem
  - or to allow the user to restart the process in the background etc
- s Suspended processes are commonly swapped out of real memory
  - This is a state which affects the process not individual threads

** Why we don't use Java =suspend()=
- If dealing with threads in Java we don't use these deprecated methods:
- =suspend()= freezes a thread for a while. This can be really useful
- =resume()= releases the thread and it can start running again
- But we can /easily(?)/ get deadlock
  - =suspend()= keeps hold of all locks gathered by the thread
  - If the thread which was going to call =resume()= needs one of those locks
    before it can proceed we get stuck

** Java threads and "stop"
- Why we don't use =stop()=
- =stop()= kills a thread forcing it to release any locks it might have
  - We will see where those locks come from later lectures
- The idea of using locks is to protect some shared data being manipulated
  simultaneously
- If we use =stop()= the data may be left in an inconsistent state when a new
  thread accesses it

** Waiting in some UNIXes
- A process waiting is placed on a queue
- The queue is associated with the hash value of a kernel address
  - (waiting or suspended processes my be swapped out)
  - When the resources becomes available
    - originally used to scan the whole process table
  - All things waiting for that resouce are woken up
  - (may need to swap the process back in)
    - Why would we do this? The reason why this would be bad is because you're
      waking them all up and then the rest go back to sleep (only one
      running). But the scheduler has a priority for each process
  - first one to run gets it
  - if not available when a process runs the process goes back to waiting
- a little like in java

#+BEGIN_SRC java
while (notAvailable)
  wait();
#+END_SEC

** Finishing
- All resources must be accounted for
  - may be found in the PCB or other tables (eg devices, memory, files)
- reduce usage count on shared resources
  - memory, libraries, file/buffers (can this shred library be released from
    memory now?)
- if the process doesn't tidy up (eg close files) then something else must
- accounting information is updated
  - This meaning that if you're session is in the cloud, it will track the
    amount of resources used so that they can bill you (money!) properly.
- remove any associated process
  - Was this session leader? If so then should all processes in the same session
    be removed?
- remove the user from the system
- notify the relatives?

** Two reasons to stop
- Stopping normally
  - must call an exit routine
  - this does all the required tidying up
  - What if it doesn't call exit and just doesn't have a next instruction?
- Forced stops
  - Only certain processes can stop others
    - parents
    - owned by the same person
    - same process group
      - Process group: The output of one process is the input to another, you
        can view these processes as a group. Stopping one doesn't make sense if
        the whole thing will die. Thus we need process groups
  - Why do they do it?
    - work no longer needed
    - somehow gone wrong
    - user got bored waiting for completion
  - OS also stops processes
    - usually when something has gone wrong
    - exceeded time
    - tried to access some prohibited resource
  - /Cascading termination/
    - Some systems don't allow child processes to continue when the parent stops

** UNIX stopping
- Usually call =exit= (=termination status=)
- open files are closed: including devices
- memory is freed
- accounting updated
- state becomes "zombie"
  - The zombie processes are finished but the parents haven't collected the
    return value. For a zombie all its resources are released but we are just
    keeping it around so that the parent can collect info 
- children get "init" as a step parent
- parent is signalled (in case it is waiting or will wait)
- after the parent retrieves the termination status the PCB is freed

** UNIX state diagrams

#+DOWNLOADED: screenshot @ 2021-03-22 15:02:57
[[file:images/Runnable/2021-03-22_15-02-57_screenshot.png]]

** Info form a Linux process table

#+DOWNLOADED: screenshot @ 2021-03-22 15:03:15
[[file:images/Runnable/2021-03-22_15-03-15_screenshot.png]]

* Scheduling processes/ threads
- We have lots of different processes, how do we order which ones we execute
  first
- Different systems of scheduling for different pruposes
  - Batch systems: The user isn't interacting with it, huge throughput
    - Keep the machine going
  - Time sharing systems
    - Keep the users going: Allows for user interaction
  - Real time ssytesm (including mulitimedia, VR, etc)
    - Always dealing with the important things first
- Burst time: A process is running. The burst time is the time from when the
  process starts running to the time it would take to hit the next wait or exit
** Levels of scheduling
Batch systems
- Very long-term scheduler
  - Before work can be submitted
  - can this user afford it?
  - administrative decisions: students can't enter jobs between 10pm and 6am
- Long term scheduler
  - May enforce administrative decisions
  - which jobs (currently spooled) should be accepted into the system
  - need to know about resource requirements
  - How many CPU seconds?
  - How many files, tapes, pages of output?
  - (need a way of encouraging users to try to be accurate in the estimation)
  - it is common for jobs with small resource requirements to run sooner (why?)
  - invoked when jobs leave the system
- Medium term scheduler
  - If things get out of balance suspend this process and swap it out
- Shot term scheduler (sometime called the dispatcher)
  - which of the runnable jobs should go next
- Dispatcher
  - The ocd ewhich performs the context switch form one process to another
** What about desktop or phone?
- Desktop OS
  - Most of the scheduling levels on the previous slide do not exist. Why?
  - Scheduler designed to optimise responsiveness for the user, this means
    input/output bound threads will be scheduled quickly when input becomes
    available
    - And CPU bound threads will be penalised (only if IO bound threads are
      ready to run)
- Phone OS
  - Usually one process runs in the foreground
  - We don't want too much work in the background (to reduce energy
    requirements)
** Scheduling algorithms
*FCFS*: First come first serve
- no time wasted to determine which process should run next
- little overhead as context switch only when required

#+DOWNLOADED: screenshot @ 2021-03-24 10:06:25
[[file:images/Scheduling_processes/_threads/2021-03-24_10-06-25_screenshot.png]]

*** Round-robin
- Round-robin scheduling
  - A pre-emptive version of FCFS
- Need to determine the size of the time slice or the time quantum
- What is wrong with treating every process equally?
  - No concept of priorities
  - Doesn't deal with different types of process: compute bound vs IO bound
- One way to tune this is to change the length of the time slice
  - What effect does a long time slice have?
  - What effect does a short time slice have?

- Once a process has been executed it goes to the back of the line!
- What is the average waiting time here?
eg: Time slice 10
#+DOWNLOADED: screenshot @ 2021-03-24 10:08:54
[[file:images/Scheduling_processes/_threads/2021-03-24_10-08-54_screenshot.png]]
What about 5?
| p1 | p2 | p3 | p1  |   |
|----+----+----+-----+---|
|  5 |  8 | 11 | ... |   |

*** Minimising average wait time
If we could choose the process which was going to use the CPU for the smallest
amount of time we woudl have an algorithm which minimised the average wait
*SJF*: Shortest job first
- Unfortunately we don't know which is the process with the shortest CPU burst
- Use the previous CPU bursts to estimate the next
- We may use a different method of pre-emption
  - If a process becomes ready with a shorter burst time than the remaining
    burst time of the running process then the process is pre-empted
  - This is simply a priority mechanism

**** Pre-emptive SJF
#+DOWNLOADED: screenshot @ 2021-03-24 10:12:41
[[file:images/Scheduling_processes/_threads/2021-03-24_10-12-41_screenshot.png]]

Run the shortest job that has arrived at the currently time.
p1 becomes 5 because it was running for 2ms. You need to update the values
during the runtime! Also remember that this is still using timeslices
*** Handling priorities
_Explicit priorities_
- Unchanging. When the priority is set you don't change it.
- Set before a process runs
- When a new process arrives it is placed in the position in the ready queue
  corresponding to its priority
- It is possibile to get *starvation*: Example: Processes that has poor or low
  priority. It's always pushed to the back of the queue! Because it's not going
  to be scheduled until it's priority is high enough. It's kinda like everyone
  just pushes in line
  - A way to tackle this is age: As your wait in the queue gets longer then the
    process gets higher a priority
_Variable priorities_
- Priorities can vary over the life of the process
- The system makes changes according to the process' behavious: CPU usage, IO
  usuage, memory requirements
- If a process is not running because it has a low priority we can increase the
  priority over time: This is one way of ageing the priority
  - Or a process of a worse pririty might be scheduled after five processses of
    a better priority
  - This prevents starvation, but better priority processes will still run more
    often
  - Can pre empty processes if a better choice arrives
*** Multiple queues
- Either a process stays on its original queue
- Or processes move from queue to queue
- Some are aboslute: worse priority queues only run a process if no better
  queue have any waiting
- Some have different selection strategies
  - Lower priority queues might occasionally be selection from
- Some allocate different time slices
- Processes can be moved from queue to queue because of their behaviour
  - CPU intensive processes are commonly put on worse priority queues
  - What behaviour does this encourage?
- Processes which haven't run for a long time can be moved to better priority
  queues
*** Moving between queues

#+DOWNLOADED: screenshot @ 2021-03-24 10:23:40
[[file:images/Scheduling_processes/_threads/2021-03-24_10-23-40_screenshot.png]]
Do you see that you get the same amount of computation time? Level 1 is chosen
most of the time and you chose at 1/10Th of the time at level 2 but the total
amount of time that they are running is the same
**** Mulitple processors
- We presume all processes can run on all processors (not always true)
  - Maintain a shared queue
  - Sharing the data in the shared queue means that there has to be locks, is
    slower. What a modern OS does it having seperate queues and then have a load
    balancer between them.
- Is this preferable?
  - Let each processor select the next process from the queue
  - Or let one processes determine which process goes to which processor
*** UNIX process scheduling
- Every process has a /scheduling priority/ associated with it; larger numbers
  indicate worse priority. Lower number is better!
- Priorities can be changed by the /nice/ system call
  - Ordinary usres can only /nice/ thier own processes upwards (ie: worse
    priorities). This means that you're being /nice/ to other processes
- Processes get wrose (higher) priorities by spending time running
  - There is a worst level which all CPU bound processes end up at
  - This means round robin scheduling for these processes
- Process ageing is employed to prevent starvation
- Priorities are recomputed every second
  - This means that there is a /blip/. For a moment every second there is
    nothing happening because recomputation is happening of these priorities!
    A small sutter
*** Old linux process scheduling
- Linux uses two process-scheduling algorithms:
  - A time sharing algorithm for most processes
  - A real time algorithm for processes where absolute priorities are more
    important than fairness
- A process's scheduling calss defined which algorithm to apply
- For time-sharing processes, Linux uses a prioritized credit based algorithm
  - The process with the most credits won
  - Every clock tick the running process lost a credit
  - When it reached 0 another processes was chosen
- The crediting rule was run when no runnable processes had any credits left

#+DOWNLOADED: screenshot @ 2021-03-24 10:29:48
[[file:images/Scheduling_processes/_threads/2021-03-24_10-29-48_screenshot.png]]
- This meant that waiting processes got extra credits and would run quickly when
  no longer waiting
*** Linux real-time scheduling
- Linux implements the FIFO and round-robin real-time scheduling classes
  (POXIS. 1b); in both cases, each process has a priority in additon to its
  scheduling class
  - The scheduler runs the process with the highest priority; for equal-priority
    processes, it runs the longest-waiting one
  - FIFO processes continue to run until they either exit of block
  - A round robin process will be preempted after a while and moved to the end
    of the scheduling queue, so that round robin processes of equal priority
    automatically time-share between themselves

*** Current Linux process scheduling
- Linux 2.6.23 updwards: Competely Fair Scheduler [[https://www.linuxjournal.com/node/10267][here!]]
  - If you have 4 different processes *over time* they will get a 1/4 of the CPU
    time
- Goal: Keep thread selection O(1) regardless of the number of threads
- Interactive processes get priority
- Uses *virtual run time (vruntime)* which depends on
  - A fair CPU share per thread. eg. If 4 threads each gets 1/4 of the CPU time
  - the wait time for the thread. If waiting your vruntime goes down
  - the priority (including nice value) of the thread, better priorities get
    smaller runtime values
  - and how long a thread has been running
- Uses a red-black tree of tasks (according to vruntime): task with smallest
  vruntime is selected to run
- Over time the vruntime of the running thread  will be greater than the left
  most node in the tree and it will be preempted
- Also completx load balancing is done (multi-core, NUMA)
- *The scheduler has been improved over the years*
  - To improved interactive response times 
- and for better multicore performance

* Real-time Scheduling
- /Hard real-time/ systems: Required to complete a critical task within a
  guaranteed amount of time
- /Soft real-time/ computing: requires that critical processe receive priority
  over less /important/ ones
- When processes are submitted they indicate their CPU requirements
- The scheduler may reject the process if the requirements cannot be met
- But very important processes can force other processes to relinquish their
  allocations


#+DOWNLOADED: screenshot @ 2021-03-24 11:39:46
[[file:images/Real-time_Scheduling/2021-03-24_11-39-46_screenshot.png]]

#+DOWNLOADED: screenshot @ 2021-03-24 11:39:57
[[file:images/Real-time_Scheduling/2021-03-24_11-39-57_screenshot.png]]

** Real-time scheduling
- Periodic and Sporadic processes
  - Periodic
    - activate regularly between fixed time intervals
    - used for polling, monitoring and sampling
    - predetermined amount of work every period
  - Sporadic
    - Even driven: can be used for fault detection: some external signal or
      change
  - (c, p, d)
    - c: computation time (worst case)
    - p: period time (at what period)
    - d: deadline you want it done by
    - c <= d <= p

#+DOWNLOADED: screenshot @ 2021-03-24 11:43:56
[[file:images/Real-time_Scheduling/2021-03-24_11-43-56_screenshot.png]]

*** Periodic processes
- Period and Deadline are determined by the system requirements (often the same)
- Computation time is found through analysis, measurement or simulation
- When the computation is complete the process is blocked until the nex tperiod
  starts
- Sometimes it doesn't matter if the deadline extends beyond the period or the
  period can change depending on system load
*** Sporadic processes
- (c, p, d) still applies
  - c and d have the obvious meaning
  - p is the minimum time between events
- aperiodic processes
  - p = 0
  - events can happen at any time, even simultaneously
  - timing can no longer be deterministic but there are ways of handling this
    - statistical methods, we design to satisfy average response times
    - if it is rare that the system has timing faults then special cases can be
      included in the handling code
*** Cyclic executives
- Handles periodic processes
- Sporadic processes can be converted into equivalent periodic processes or they
  can be ignored (if they take only a little time to handle)
- Pre scheduled: a feasible execution schedule is calculated before run time
- The cyclic executive carries out this schedule
- it is periodic
- Highly predicable: non preemptible
- Inflexible, difficult to maintain
*** CE schedule
- Major schedule: cyclic allocation of processor blocks which satisfies all
  deadlines and periods
- Minor cycle (or frame): major schedules are divided into equal size
  frames. CLock ticks only occur on the frame boundaries

#+DOWNLOADED: screenshot @ 2021-03-24 17:30:15
[[file:images/Real-time_Scheduling/2021-03-24_17-30-15_screenshot.png]]

**** CE example
- Periodic processes:
- A = (1,10,10), B = (3,10,10), C = (2,20,20), D = (8, 20, 20)
- Major cycle time is 20 (smallest possible value we can use in this case). LCM
  of periods
- Frame time: can be 10, GCD of periods
- A feasible schedule

#+DOWNLOADED: screenshot @ 2021-03-24 17:31:39
[[file:images/Real-time_Scheduling/2021-03-24_17-31-39_screenshot.png]]

*** Scheduling with priorities
- Scheduling decisions are made:
  - When a process becomes ready
  - When a process completes its execution
  - When there is an interrupt
- Priorities can cause schedules to not be feasible
A = (1,2,2) better priority
B = (2,5,5) worse priority
- This is feasible (without preemption), wut if the priorities are reversed it
  is not
- Still priorities are almost always used
  - Fixed: determined before execution
  - Dynamic: change during execution

*** Priority allocation
- Fixed
  - Rate monotonic (RM): The shorter the period the higher the priority
  - Least compute time (LCT): The shorter the execution time the higher the
    priority (shortest job first)
- Dynamic
  - Shortest completion time (SCT): shortest job first with preemption. But this
    time we have /good/ information on the execution time requirement
  - Earliest deadline first (EDF): the process with the closest deadline has the
    highest priority
  - Least slack time (LST): the process with the least slack time ahs the
    highest priority
    - Slack time is the amount of time to the process's deadline minus the
      amount of time the process still needs to complete
*** Calculation schedules
- Calculate a schedule for the following two processe using EDF and SCT
A = (2,4,4,) B = (5, 10, 10)
- EDF 
#+DOWNLOADED: screenshot @ 2021-03-24 17:37:06
[[file:images/Real-time_Scheduling/2021-03-24_17-37-06_screenshot.png]]
- SCT: same as above
- What about LST? Same as above until time 17

*** Theory
- For static priorities
  - RM is an optimal scheduling policy
  - If the CPU usage is < ln 2 ~= 0.69 RM will always find a schedule
- For dynamic priorities
  - EDF and LST are optimal
- But these are only true for single processors
  - "The most practical policy for multiprocessors is to pre-assign processes to
    CPUs using some heuristic, and then to schedule each one independently
  - Also theory assumes complete knowledge: non preemptible resources,
    precedence constraints, interrupt and context switching times all need to be
    taken into account

* Test
These are a compilation of things that I should know for the test
- Interrupts: When the computer wants to do something like get some keyboard
  input it calls it sets up registers in the device driver which then calls the
  device controller, the device controller then reads the registers in the
  device and does what it needs to do. Then to inform the process that it's done
  it then sends in interrupt via the main bus to the CPU. The reason why we
  use interrupts is because we don't know how long that they are going to take
  so we need it not to be blocking (eg getting user input). The interrupt works
  by sending a signal to the CPU which stops immediately what it's doing and
  instantly jumps to a fixed location

#+DOWNLOADED: screenshot @ 2021-03-25 19:34:24
[[file:images/Test/2021-03-25_19-34-24_screenshot.png]]

- Interrupts are really fast and they need to be!

#+DOWNLOADED: screenshot @ 2021-03-25 19:35:18
[[file:images/Test/2021-03-25_19-35-18_screenshot.png]]

I/O is when you need to access something in hardware or on the screen, something
that produces sideffects. So like the mouse/keyboard but it can also be the
networking card. Point being that it is outside of the CPU and not ready, you
must read from these devices

** Computer system architecture
Core is the component that executes insrtuctions and registers for storing data
locally
A multicore processor is a single computing component comprised of two or more
CPUs that read and execute the actual program instructions. The individual cores
can execute multiple instructions in parallel, increasing the performance of
software which is written to take advantage of the unique architecture. Each CPU
core has its own set of registers that are manipulated by programs, The way that
Hyperthreading works is that when a CPU core is waiting/idle a single CPU core
will communicate with the idle/waiting core and make it do some work so that the
program the original core is running runs faster which is good!


#+DOWNLOADED: screenshot @ 2021-03-25 19:54:36
[[file:images/Test/2021-03-25_19-54-36_screenshot.png]]

#+DOWNLOADED: screenshot @ 2021-03-25 19:54:42
[[file:images/Test/2021-03-25_19-54-42_screenshot.png]]


Why do we need kernel mode?
If there were only one mode then the user would have access to all the CPU
instructions and would have full control over the system, this would allow you
to be generally mallicious to other users on the system such as closing their
processes and then making your own process a higher priority, or allocating your
process more RAM/ CPU time. Clearly this cannot work. There has to be some sort
of authentication layering which is called kernel mode, this gives access to all
of the privileged instructions. Remember about ring0 process is kernel
mode. Many CPUs come with another mode called a VMM (virtual machine manager) in
this mode the VMM has access to more instructions than the user level but less
than the kernel

A trap is an exception in a user process. It's caused by division by zero or
invalid memory access. It's also the usual way to invoke a kernel routine (a
system call) because those run with a higher priority than user code. Handling
is synchronous (so the user code is suspended and continues afterwards). In a
sense they are "active" - most of the time, the code expects the trap to happen
and relies on this fact.

An interrupt is something generated by the hardware (devices like the hard disk,
graphics card, I/O ports, etc). These are asynchronous (i.e. they don't happen
at predictable places in the user code) or "passive" since the interrupt handler
has to wait for them to happen eventually.

You can also see a trap as a kind of CPU-internal interrupt since the handler
for trap handler looks like an interrupt handler (registers and stack pointers
are saved, there is a context switch, execution can resume in some cases where
it left off).

- For the test I should know something about the history of computing. I'm just
  going to write something significant about each stage and what advatanges that
  it had over the thing that came before it

Off-lining
- Nothing really changed but the procedures were made more formal
- offlining allowed mulitple programs to be submitted to the computer so that
  there was less down time between each program. (originally after a program was
  run then the operator would have to put in their punch cards and then run the
  program, this instead allowed multiple people to submit programs and then they
  would be run one after the other. This is way better in terms of time
  efficiency

** Resident monitors
I think that this would be something that they could ask in the test
- The computer operators had formal procedures
- Get the computer to help
Thus a program was always in memory (hence a resident) that would control how
other programs that were submitted to the system were handled. Each program
started by the monitor and each program ended by jumping back to the resident
monitor. The resident monitor helped doing several things such as
- clear memory
- load the next program
- find the data for the program
- jump to the start address of the new program, returning to the res monitor
  when finished
- it also maintained the standard IO routines in memory

A big change
Interruptible processors. Initally IO was polled which is really slow and takes
a lot of time. Eventually interruptable processors were discovered such that
devices raising interrupts and processeors responding to them substantially
changed the way IO was performed. Development from single location return
address to the use of a stack. This is HUGE! and was the introduction to the
stack and different registers and are basically how computers still work today,
furthermore moving away from polling means that the CPU could do other things
while the IO routine happened. 

The next large step was having multiple programs in memory at once
Memory protection can be done by sotware but it way better done by hardware (in
terms of saftey)


#+DOWNLOADED: screenshot @ 2021-03-25 20:40:08
[[file:images/Test/2021-03-25_20-40-08_screenshot.png]]
syscall -> change processing mode -> kernel does the thing -> returns back to
subroutine

Kernel module are also a big deal. The kernel has a set of core components and
can link in additional services via modules either at boot time or durig
runtime. The idea of the design is for the kernel to provide core services while
other services are implemented dynamically as the kernel is runnig. Linking
services dynamically is prefereable to adding new features directly to the
kernel, which would require recompiling the kernel every time a change was
made. So EG we might add in the kernel core  CPU scheduling and memory
amnagement algs directly but then add as module support for differnt file
systems by way of loadable modules. Kernel modules are mainly done for device
drivers now

Why use monolithic kernel? (meaning that the whole kernel is loaded at once
instead of sharding it into several different programs. Because having
everything in a single address space (process) means that we get better
performance!)

Scheduling!
Man knowing shit about processes is good, like if I were a processor I'd like a
set of information about each process that would allow me to schedule it for my
single CPU. So we gotta know a lot!
- what devices a processes uses
- how many files
- how much output
- how long it's expected to take
- All this information was submitted on control cards. Different queues meant
  different expected resourcer requirements. Jobs on different queues were
  scheduled differently

Big change!
- Computer sytesms could now be mass manufactered
- People were now the expensive bitt, we wanted to make them more
  productive. Thus lets give everyone a console!
This is the introduction to time share systems. BIG CHANGE!
This is a multiuser environment where people can submit jobs. Now we can no
longer have a single source of truth. Initially there was a team of people that
would run the computer, that was the security. Now people are accessing the
computer.
We need a way to schedule all the tasks that people are submitting. Thus the
interesting bit is security and scheduling tasks.

Devices and the CPU can't be sued to capacity otherwise response time is too
long. So there has to be slack in the system. High level scheduling decisions
are made by the users

loosely and tightly coupled systems
#+DOWNLOADED: screenshot @ 2021-03-25 21:28:26
[[file:images/Test/2021-03-25_21-28-26_screenshot.png]]


#+DOWNLOADED: screenshot @ 2021-03-25 21:32:29
[[file:images/Test/2021-03-25_21-32-29_screenshot.png]]

Some ideas on Virtual machines
- VMM AKA hypervisor creates and runs virtual machiens by providing an interface
  that is /identical/ to the host

Virtualization: The actual program is being executed with minimal translation of
commands (such as syscalls) but all calls to otehr parts are trapped and
emulated on the host OS
Emulation: translating the ISA to another ISA that provides the same effect. EG
gameboy ISA on x86

Virtualisation gives you a fuck tonne of power! You can suspend, snapchost and
resume where you left off. This is really powerful because it allows you to
create a very stateful machine which is powerful because you can access any
prior state or even mutate the current state of the OS without loosing what you
have at the moment. For example this can be a really powerful tool in OS
development. A kernel call could ruin the system but we now have state!

Some VMMs include live migration in which you can move a /OS/ without
downtime!!! This is good for resource management. EG one computer cluster
becomes overloaded and thus you must be moved to another. But downtime is really
dangerous! 

Virtualizing is really difficult. There are a number of things that we have to
do like trap and emulate and binary translation which I'll cover below! But
before that we should look at virtual CPU VCPU which represents the state of the
CPU on the guest OS. This is important because when the VMM traps and emulates
on the host os then the host must update the VCPU of the guest to reflect the
new state after the operation

The guest OS runs in user mode (as a process). So it has to do the trap/emulate
#+DOWNLOADED: screenshot @ 2021-03-25 21:43:59
[[file:images/Test/2021-03-25_21-43-59_screenshot.png]]

Binary translation
This is just a fancy thing for explaining that there are some cpus such as
intels that do not have a clean seperation of what is a user level and what is a
kernel level instruction. Some instructions are shadowed. There is a good reason
for doing this because you don't want developers to have to look up the
privalleged version of each instruction also it would bloat the ISA spec
needlessly. So here we are. For example: popf pops the head of whatever the
stack pointer is looking at, but they behave differntly in kernel mode and user
mode because we need to know if we have permission to read that piece of
memory. So this is how we deal with ti


#+DOWNLOADED: screenshot @ 2021-03-25 21:49:01
[[file:images/Test/2021-03-25_21-49-01_screenshot.png]]

This translation works surprisingly well!

#+DOWNLOADED: screenshot @ 2021-03-25 21:49:48
[[file:images/Test/2021-03-25_21-49-48_screenshot.png]]

mentioning of page tables
A page table is the data structure used by a virtual memory system in a computer
operating system to store the mapping between virtual addresses and physical
addresses.

The benefits of virtual machines:
- obviously firstmost is that we have the abliity to share the same hardware yet
  run several different execution environments (operating system)
  concurrently. It also provides a layer of protection by acting as a sandbox,
  meaning that if one of the guest OSes gets a virus then the host OS doesn't
  get it (or is very unlikely to get it unless it has some sort of sandbox
  escape exploit which are rare and valuable)

Virtual memory
Because ram is a precious resource the OS might opt into using virutal memory,
which is a memory address space that combines ram and disk space, addresses that
are looked up in virtual memory often will be used in ram because the lookup
will be faster, and addresses that are not used often will be on the slower disk
#+DOWNLOADED: screenshot @ 2021-03-27 10:12:22
[[file:images/Test/2021-03-27_10-12-22_screenshot.png]]

Page table: 
- The thing that keeps track of VA to PA mapping

#+DOWNLOADED: screenshot @ 2021-03-27 10:16:42
[[file:images/Test/2021-03-27_10-16-42_screenshot.png]]

The way that an OS handles virtual page tables is called nested page tables
(NPTs) each guest operationg system maintains one or more page tables ot
translate from virtual to physical memory. The VMM maintians NPTs to represent
the guest's page table state, just as it creates a VCPU to represent the guests
CPU state. The VMM knows when the guest tires to change its page table and it
makes the equivalent change in the NPT

** Processes
Processes have a lot of states

#+DOWNLOADED: screenshot @ 2021-03-27 10:23:27
[[file:images/Test/2021-03-27_10-23-27_screenshot.png]]

Each process is represetnted in the operating system by a process control block
(PCB). It includes
- Process state: The state may by new, ready, runing, waiting, halted, and so on
- Program counter: Is the address of the nex tinstruction to be executed for the
  process

#+DOWNLOADED: screenshot @ 2021-03-27 10:24:52
[[file:images/Test/2021-03-27_10-24-52_screenshot.png]]

** Threads
Each thread has its own stack, program counter (PC) and register set, thread
ID. It shares a lot with other threads in the same process such as part of the
binary (code and data section) and other resources such as open files and
signals. If a process has multiple threads of control it can perform more than
one task at a time

#+DOWNLOADED: screenshot @ 2021-03-27 10:28:16
[[file:images/Test/2021-03-27_10-28-16_screenshot.png]]



#+DOWNLOADED: screenshot @ 2021-03-27 10:29:19
[[file:images/Test/2021-03-27_10-29-19_screenshot.png]]

There is something called green threads, these are threads that are done by the
process where the process creates several threads, but the bad thing about these
is that if one green thread blocks then so do all of the other green threads
because they are actually all using a single underlying kernel thread

#+DOWNLOADED: screenshot @ 2021-03-27 10:33:45
[[file:images/Test/2021-03-27_10-33-45_screenshot.png]]

Also green threads have the disadvantage that they cannot work with mulitple
processor cores which are now standard

** One to one model
This maps each user thread to a kernel thread. This is great because the OS now
can distinguish threads and see when a single one is blocking and then change
the context but the only downside is that createing kenerl thredas can burden
the perforamce of a system as they are expensive to create and maintain (as
apposed to many-to-one model)


#+DOWNLOADED: screenshot @ 2021-03-27 10:37:32
[[file:images/Test/2021-03-27_10-37-32_screenshot.png]]

** Many-to-many
Many to many suffers none of the above conseqences and developers can create as
many user threads as necessary and the coreesponding kernel threads can run in
parlallel on a mulitporcessor. Although the many-to-man model appears to be the
most flexible of the models discussed, in practice it is difficult to
implement. In addition with an increasing number of processing cores appearing
on most system, limiting the nubmer of kernel threda has become less
important. This mnodel also can put more overhead on the developer which is the
expensive part of the system

process; an instance of a program in execution
two parts to a process: 1. resources 2. what the process is doing

Jacketing
- Ok we don't want user level thread to block other threads
A solution is jacketing:
- A blocking system call has a user-lvel jacket
- The jacket checks to see if the resources is available
- If not another threda is started
- When the calling thread is scheduled again (by the thread library) it once
  again checks the state of the device

** Processes!
#+DOWNLOADED: screenshot @ 2021-03-27 11:16:39
[[file:images/Test/2021-03-27_11-16-39_screenshot.png]]

The processes are a tree! Assuming that you're running systemd (some OSes don't
use this such as void linux) the first process that's created is /init/

fork creates a child process from the current parent process
exec system call to replace the processes memory space iwth a new program. The
exec system call loads a binary file into memory  (destoying the memory image of
the program cointaint the exec system call) and starts its execution .

The the parent has nothing else to do it can issue a wait system call to move
itself off the ready queue  until the termination of the child.

Some sytes do not allow a child to exist if its parent has terminated. In such
system is the parent terminates then so does the child. This is known as
cascading termination

When a process terminates, its resoruces are deallocated by the operating
stesm. However its entry in the process table must ramin there until the parent
calls -wait()= because the process table contains the process's exit status. A
process that has terminated by whose paretn has not yet called wait is known as
a zombie process. All process trainsitiont othis state when they terminate, by
genreally they exist as zombies only breifly

Now when a parent spawns children and then exits without calling wait the child
process become orphans. In UNIX systems the children will have a new parent
which is the systemd init


#+DOWNLOADED: screenshot @ 2021-03-27 11:26:02
[[file:images/Test/2021-03-27_11-26-02_screenshot.png]]

** Process scheduling
Th eobjecting of mulitprogramming si to have some process running at all times
so as to maximuze CPU utilissations. The objective of time sharing is to switch
a CPU core among processes so frequently that users can interact with each
program while it is running. To meet these objectives the process scheudler is
born

So a process scheduler has two queues

#+DOWNLOADED: screenshot @ 2021-03-27 11:32:15
[[file:images/Test/2021-03-27_11-32-15_screenshot.png]]
the ready queue 
Remeber that there are two types of processors. Ther eis the IO bound process
which spends most of its time doing IO than spending doing computation. This
type of process is expected to be waiting a lot. The CPU bound process in
contrast does IO infrequently and spends most of its time doing computations

When a process is ready to be run it is put into the ready queue! from there
heaps of things that can happen:

#+DOWNLOADED: screenshot @ 2021-03-27 11:36:05
[[file:images/Test/2021-03-27_11-36-05_screenshot.png]]

Swapping: the key idea being that sometimes it can be advantageous to remove a
process from memory, (and from active contention for the CPU) and thus reduce
the ddegree of multirogramming. Later the process can be reintroduced into
memory, and its execution can be continued where it left off. This scheme is
known as /swapping/ because a process can be "swapped out" from memory to disk,
where its current status is saved and later "swapped in" from disk back to
memory where its status is restored. Swapping is typically only necessary when
memory has been overcommitted and must be freed up. 

Context switching

#+DOWNLOADED: screenshot @ 2021-03-27 11:48:23
[[file:images/Test/2021-03-27_11-48-23_screenshot.png]]

Context switching is pure overhead because the CPU does no useful work while
switching
Interrups cause the operating systesm to change a CPU core from its current task
to run a kernel routine. This is basically by writing the pointer counter
register straight away with the jump to the new function. This is really quick
but theres some stuff to do before we swtich

preemptive vs cooperative multitasking
preemptive
- A clock interrupt causes the OS to check to see if the current thread should
  continue
- Each thread has a time slice
- Advantages
  - control predicatability
- Disadvtanges
  - Critical sections
  - efficiency

Cooperative multitasking
- Two main approaches
  - A process yeilds its right to run
  - system stops a process when it makes a system call
- This does NOT mean a task will work to completion it will run until the next
  wait block


Suspending processes
whicih is another type of waiting
- Operators or OS temporarily stopping a process (it is not usually caused by
  the process itself)
allows others to run to completion more rapidly.
suspended processes are commonly swapped out of real memory

why we don't use suspend
- suspend freezes a thread for a while. This can be really useful
- resume releases the thread and it can start running again
- but we can easily get deadlock
  - suspend keeps hold of all locks gathered by the thread
  - If the thread which was going to call resume needs one of those locks before
    it can procee dwe et stack

We don't use stop either
Stop kills a thread forcing it to release any locks it might have
- The idea of using locks is to protect some shared data being manipulated
  simultaneously
- if we use stop the data may be left in an inconsistent state when a new stread
  access it

Basically never kill a thread! it may leave things in a bad state. Instead you
should wait for the thread to finish its work and call join

** Critical section problem
consider a system consisting of n processes p0, p1, p2, p3, .... Each process
has a segmetn of code called a critical section in which the process my be
accessing and updating data that is shared with atleast of other process. The
improtant feature of the system si that when one process is executing in its
ciritical seciton, no tother process is allowed to execute in its ciritical
section, ie no two processes are executing their criitcal section at the same
time. This is called the critical section problem where the problem is to design
a protocol such that theycan synchronize their acticiyt so as to cooperatively
share data


#+DOWNLOADED: screenshot @ 2021-03-27 12:38:17
[[file:images/Test/2021-03-27_12-38-17_screenshot.png]]

#+DOWNLOADED: screenshot @ 2021-03-27 12:39:48
[[file:images/Test/2021-03-27_12-39-48_screenshot.png]]


#+DOWNLOADED: screenshot @ 2021-03-27 12:42:33
[[file:images/Test/2021-03-27_12-42-33_screenshot.png]]

Soft real time systems: porvide no guarantee as to when a ciritcal real time
process willl be scheduled. They guarantee only that the process will be given
prefernce over non critical processes
Hard real time systems have stricter requirements. A taks must be servied by its
deadline. Service after a deadline has expired is the same as no service at all

There's two types of latency here
1. interrupt latency: period of time from the arrival of an interrupt at the CPU
   to the start of the touritn that services that interrupt. When an interrupt
   occurs there is a context switch (because the current process/thread state
   has to be stored). 

#+DOWNLOADED: screenshot @ 2021-03-27 12:52:33
[[file:images/Test/2021-03-27_12-52-33_screenshot.png]]

2. Dispatch latency: The amount of time required for the scheduling dispatcher
   to stop one process and start another


#+DOWNLOADED: screenshot @ 2021-03-27 12:53:04
[[file:images/Test/2021-03-27_12-53-04_screenshot.png]]


#+DOWNLOADED: screenshot @ 2021-03-27 12:53:31
[[file:images/Test/2021-03-27_12-53-31_screenshot.png]]


#+DOWNLOADED: screenshot @ 2021-03-27 13:09:18
[[file:images/Test/2021-03-27_13-09-18_screenshot.png]]
** Interrupt latency and dispatch latency
Interrupt latency is the period of time from the arrival of an interrup at the
CPU to the start of the routine that srvices the interrupt
Whereas
dispatch latency is the amount of time required for the scheduling dispatcher to
stop one process and start another

#+DOWNLOADED: screenshot @ 2021-03-28 09:49:10
[[file:images/Test/2021-03-28_09-49-10_screenshot.png]]


#+DOWNLOADED: screenshot @ 2021-03-28 09:49:17
[[file:images/Test/2021-03-28_09-49-17_screenshot.png]]

conflict phas of the following:
1. Preemption of any process running in the kernel
2. Release by low priority process of resources needed by a high priority
   resource

Scheduling with priorities
Scheduling decisions are made:
- When a process becomes ready
- When a process completes its execution
- When there is an interrupt


#+DOWNLOADED: screenshot @ 2021-03-28 10:05:10
[[file:images/Test/2021-03-28_10-05-10_screenshot.png]]
* Readings
A situation like this, where several processes access and manipulate the same
data concurrently and the
outcome of the execution depends on the particular order in which the access
takes place, is called a race condition. To guard against the race condition
above, we need to ensure that only one process at a time can be manipulating
the variable count . To make such a guarantee, we require that the processes be
synchronized in some way.

A critical section is a part of a subroutine where the process may be accessing
and updating datat that is shared with at least one other process. Th eimportant
feature of the system is that when on e proces sis executing in its ciritical
section, no other process is allowed to execute in its critical sections at the
same time. 

#+DOWNLOADED: screenshot @ 2021-03-29 10:02:13
[[file:images/Readings/2021-03-29_10-02-13_screenshot.png]]

race conditions

#+DOWNLOADED: screenshot @ 2021-03-29 10:03:14
[[file:images/Readings/2021-03-29_10-03-14_screenshot.png]]

Why, then, would anyone favor a preemptive kernel over a nonpreemp-
tive one? A preemptive kernel may be more responsive, since there is less risk
that a kernel-mode process will run for an arbitrarily long period before relin-
quishing the processor to waiting processes. (Of course, this risk can also be
minimized by designing kernel code that does not behave in this way.) Fur-
thermore, a preemptive kernel is more suitable for real-time programming, as
it will allow a real-time process to preempt a process currently running in the
kernel.
* The Problem of Concurrency
- The problem is simply sharing resources
  - Several threads/processes running at the same time
  - Using the same resources: accessing the same data structures/objects/devices
  - Some resources can only be safely used by one thread at a time
    - eg: readers accessing shread data while a wrter is changing it
    - or writers changina resource simultaneously
  - Race condition
    - Any situation where the order of execution of threads can cause different
      results
Our programs must control the /non-deterministics/ nature of thread scheduling


#+DOWNLOADED: screenshot @ 2021-03-29 10:17:03
[[file:images/The_Problem_of_Concurrency/2021-03-29_10-17-03_screenshot.png]]

** Critical sections
- An area of code in which we only want one thread to be active at a time
- Providing this is known as /mutual exclusion/
- We need:
  - a way of locking threads out of critical sections
  - to guaranteee threads are not keps waiting forever (starvation)
- Starvation can be cause in different ways
  - deadlock
  - indefinite postponement: priority too low or just unlucky
** Software solutions
- We want something like this

#+DOWNLOADED: screenshot @ 2021-03-29 10:34:35
[[file:images/The_Problem_of_Concurrency/2021-03-29_10-34-35_screenshot.png]]
- we have a boolean variable locked which is true if the critical section is
  being used by a thread, initially locked is false
- Attempt 1

#+DOWNLOADED: screenshot @ 2021-03-29 10:35:48
[[file:images/The_Problem_of_Concurrency/2021-03-29_10-35-48_screenshot.png]]
- Locks like this are known as /spin-locks/ or /busy waits/
- What is wrong with this lock? At least 3 different things
  - Busy wait, its eating resources that it shouldn't use
  - It doesn't provide mutual exclusion over the lock, they can both read the
    lock in the false state and then lock. Even with atomic it could read the
    lock as false and then after the other thread also reads and false, no
    mutual exclusion
  - No control over which thread goes first, some threads get lucky, some don't.
** Another attempt: Petersons Solution
- This only works on shared memory mulitprocessors if instruction reordering can
  be turned off. Otherwise we need hardware help with memory locks
- Two writes don't get interleaved at some minimum write size, ther hardware
  allows only one processor acess at a time
- Jave note: all primitiees except double and longa re guaranteed to be written
  atomically
- Software solutions to locking critical regiions require this level of harware
  assistance
- A two thread solution

#+DOWNLOADED: screenshot @ 2021-03-29 10:40:31
[[file:images/The_Problem_of_Concurrency/2021-03-29_10-40-31_screenshot.png]]

** Bakery algorithm and hardware help
- The previous method works bu tdoesn not solve the general case
- The bakery algorithm:
  - Each thread is given a number indicating when it requires the lock
  - These are not unique so some other method of ordering: eg: pid is necessary
    as well
*Interrupt priority level*
- We could just raise the interrupt priority level to stop any other process
  (which might affect the area) from running while the lock is being tested
- Disadvtanges
  - heavy handed: not all process art the current interrupt priority level need
    to be stopped
  - Doesn't work efficiently on mulitprocessors
  - a message requiresting the IPL change must be sent to all processors, in
    some circumstances all other processors must wait
** Test and Set
- Or equivalent /atomic/ or indivisible instructions
- They appear uninterruptible: once started no other process can interfere until
  completed

#+DOWNLOADED: screenshot @ 2021-03-29 10:45:56
[[file:images/The_Problem_of_Concurrency/2021-03-29_10-45-56_screenshot.png]]
- returned the current value of the locVariable and sets the lockVariable to
  true
- With this our lock can become

#+DOWNLOADED: screenshot @ 2021-03-29 10:46:28
[[file:images/The_Problem_of_Concurrency/2021-03-29_10-46-28_screenshot.png]]
- unlock

#+DOWNLOADED: screenshot @ 2021-03-29 10:46:42
[[file:images/The_Problem_of_Concurrency/2021-03-29_10-46-42_screenshot.png]]

** Getting out of the spin
- Our lock is a spin lock of busy wait. A waiting thread keeps running trying to
  get the resource even though it is not available. It is also not fair (see the
  textbook's progress and bounded waiting requirements
*** Fairness
- Without priorities:
  - Each thread shouldn't have to wait while another thread gets access to the
    resource more than once
  - Each thread should get access ebefore any other thread which requirests it
    later
  - Otherwise indefinite postponement is possible
  - ie: a queue would help
- But with priorities
  - Threads with higher priorities: Should they get prior access to resources?
  - Makes the priority mechanism more effective
  - Increases the chance of indefinite postponement
  - Priority mechanism can still work when selecting next runnable thread
** Priority inversion
- When a low priority process blocks a high process from running. The low
  priority process gets a hold of the lock and the high priority process can't
  get a hold of that lock. But that's not the problem. Lets say that there are
  multiple processes, the low priority processes releases the lock and then it
  is snatched up by a higher priority process but the middle priority process
  never gets the lock
- When you have priorities on processes and a locking mechanism you can get
  priority inversion
- Lower priority processes with a lock can force higher priority processes to
  wait. But because they are low priority they man not run very frequently
- Particularly improtant in real-time systems
- Solved with priority inheritance: when a higher prirority process blocks
  waiting for a resouce the process with the resource is temporarily given the
  priority of the blocked process. The high priority process will now only wait
  during the critical section

** Placing in a queue
- When a thread must wait we put it on a queue and stop it running. This solves
  two problems
- Fairness
- Wasting processor cycles

- Other advantages:
  - Possibly frees pages for other processes
  - we know how many threads are waiting for this resource

- It is suble, however. What could go wrong iwth the following?


#+DOWNLOADED: screenshot @ 2021-03-29 11:40:06
[[file:images/The_Problem_of_Concurrency/2021-03-29_11-40-06_screenshot.png]]

- suspend: put the thread on the queue and start another thread
- awaken: take the thread at the head of the queue and then make it runnable.

#+DOWNLOADED: screenshot @ 2021-03-29 11:40:13
[[file:images/The_Problem_of_Concurrency/2021-03-29_11-40-13_screenshot.png]]

Problem with this: conditional:
- nothing in the queue
- - testAndSet is then set to true
- Another thread comes along and sees that the thing it locked, and just hasn't
  put itself on the queue yet
- The thread then tries to unlock and sees the empty queue (as the other thread
  hasn't put itself on the queue yet and then says locked=false. Now the other
  thread is put on the queue.
- Invalid state! Suspended queue and unlocked state. Another thread could come
  along and then correct the state

** Semaphores
- A semaphore is an integer count, two indivisible (atomic) operations and an
  intalization
- S a semaphore: The indivisible operations are:
- Semaphore is how many resoruces are free! >0 means there's atleast 1 of that
  resource that is free. Otherwise we wait.

#+DOWNLOADED: screenshot @ 2021-03-29 11:41:02
[[file:images/The_Problem_of_Concurrency/2021-03-29_11-41-02_screenshot.png]]

- The count tells how many of a certain resource are available

*** Binary semaphores
- The semaphore is initialised to 1
- To get a resource the threadcalls P on the semaphore and to return the
  resource the thread calls V

** Impletementing semaphores
- Rather than calling the operations P and V we will call them wait and signal
  (Posix wait and post). This is the correct name! 

#+DOWNLOADED: screenshot @ 2021-03-29 11:42:49
[[file:images/The_Problem_of_Concurrency/2021-03-29_11-42-49_screenshot.png]]
- Another common alternative is

#+DOWNLOADED: screenshot @ 2021-03-29 11:43:36
[[file:images/The_Problem_of_Concurrency/2021-03-29_11-43-36_screenshot.png]]
- Usually used for multiple resources (like 5) where you have mulitple resources
  that you can shrae across a number of threads

** Producer/Consumer problem

#+DOWNLOADED: screenshot @ 2021-03-29 11:45:23
[[file:images/The_Problem_of_Concurrency/2021-03-29_11-45-23_screenshot.png]]

- A thread producing data, a thread consuming the data
- We don't want to lose any data
- We don't want to use any data more than once

- number deposited = 0
- number received = 1
* File Systems
A file system needs to satisfy these general requirements
- We need some way of storing information
  - independently from a running program
  - so it can be used at a later time
  - permanently (or an approximation to it)
  - Non-volatile storage
  - so it can be shared with other programs or users
- An infinite variety of data is to be stored
  - The more information the OS knows about the data the more it can facilitate
    use of the data
  - Eg: executable files
- Naming the data
  - The data needs to be stored and retrieved easily. We need a way to name the
    data.
  - We must then be able to locate the data using its name
** What is a file?
- Textbook: A named collection of related information that is recorded on
  secondary storage
- The informatoin is usually related in the sense that it is used by a
  particular program (or is a particular program)
- Secondary storage is usually a disk drive (including SSDs) but it could be a
  tape drive, or any other non-volatile device
- In some systems "files" are not always files eg: in UNIX devices are "files"
  and so are some operating system information structures (eg /proc in linux)
** File system operations 1
On most systems these commands need security authorisation to perform and they
work on the file as a whole
*** Create
- Need to specify information about the file:
  - The name
  - The file type (or some representation of the program associated with this file)
- Do we need t ospecify the size of the file? (certainly helps with keeping
  storage contiguous but is usually regarded as an unnessary restriction)
- Creation needs to do something to the associated device (at least write to
  some structure (sometimes a directory))
- Some systems allow transitory files to not be recorded permanently in
  secondary storage
*** Delete
- Remove the file. Return the space used by the file. Zero the space? On SSDs
  the space does not need to be copied when the block is rewritten
- What if the deletion was a mistake?
  - Deletion might keep the file in a "going away" area from which it can be retrieved
  - Versioning file systems maintain multiple versions over time. They can also
    be used to track down intruders (self-securing  storage)
  - When the data is deleted on an SSD with TRIM, it is lost
*** Move
- Moving a file can be performed in different ways depending on the before and
  after locations
- If both locatoins are on the same device the data doesn't have to be copied
  and then the original deleted. Instead change information about the file
*** The need for TRIM

#+DOWNLOADED: screenshot @ 2021-04-20 11:16:56
[[file:images/File_Systems/2021-04-20_11-16-56_screenshot.png]]
IF there is data in a block (in the pages) you have to clear the entire block
(and all the pages) where it's all zeroed If there are free pages you can
write. Stale page is deprecated and the new page is the new thing. When the
green comes in you want to kill the stale page and then write the green (this
happens in a SSD (top)) while in a spinning disk the whole block is cleared and
then rewritten. The operating system can give extra information for what pages
to kill so that it doesn't have to kill all of them and then rewrite. Means that
there is less work to do (more efficient). YOU HAVE TO DO THE WHOLE THING WITH
SSDS BUT THE QUICKER WRITE ONE IS THE SPINNING DISK

** File system operations 3
*** Copy
- Most file systems preserve attributes (including last modified times_ when a
  copy is made. This way a file can be last modified before it was created
- In some situations we can use copy on write rather than making complete copies
  the file information needs to point to the original data
*** Change attributes
- We will see the different sorts of attributes shortly. Some of these should be
  changeable, others should be secured
** File system operations - read
- These operations work on the files contents
- Must specify what data to read, how much and where to put it
- Sequential access
  - data is retrieved in the same order it is stored
  - There is a current position pointer somewhere
  - May be stored within the using program
  - May be stored within the file system (but spearately for each process)
  - Has ramification for distributed systems
- Directed (or random) access
  - Easy on a disk device. Even easier on a solid state device
  - The read specifies exactly where it wants to get the data from \
  - it could be a byte offset
  - or a record number
  - Some file systems let you specify record length when you create a
    file. Others leave all such control up to individual programs
** File system operations - write
- Very similar to read but commonly requires the allocation of extra space
- Direct (random) access writes can create holes
- The program "seeks" to the new position
- If this is outside the bounds of the file then we have two choices
  - Allocate all of the intervening space and fill it with some null value
  - Mark the intervening space in the directory as not allocated (this is known
    as a /sparse/ file
- If a process tries to read from the empty space of a sparse file it gets the
  null value for the record
- If a process writes to the empty space then real blocks are allocated and
  written to
** Design decisions
- Files need to contain vastly different types of information
- Some of this information is tightly structured with lines, records etc
- Should the file system allow flexibility in how it deals with differently
  structured files
  - At the bottom level the file system is working with discrete structures
    (sectores, pages, and blocks) of a definite size
  - The most common solution is to treat files as a stream of bytes and any
    other structure is enforced by the programs using the files
  - The work has to be done somewhere
  - Some operation systems provide more facilities than others for dealing with
    a variety of file types
** File attributes
- Information about the files
- These vary widely because of the previous design decisions
- Standard ones
  - file name: the full name includes the directories to traverse to find this
    file. How much space should the system allocate for a name? Many ssytems use
    a byte to indicate the file name length and so are limited to 255
    characters. There are usually limitations on the characters you can use in
    filenames
  - Location: Where is the file stored, some pointer to the device (or server)
    and the positions on the device
  - Size of the file: either in bytes, blocks, number of records
  - Owner information: usually the owner can do anything to a file
  - Other access information: What should be allowed to do what
  - dates an times; of creation, access, modification
  - and file types...
** File name limits
- NTFS
  - Extended paths (start with "//:) up to 32767 chars, but ordinary paths up to
    256 + "C:\" and null
  - in Windows 10 can opt-in to avoid 256 char limit
  - Each directory or final filename commonly limited to 255 chars
- Linux (many different file systems)
  - path limit 4096
  - commonly 255 bytes per path component
- APFS
  - 255 characters per path component
** File type
- The more the system knows about file types the more it can perform
  appropriate tasks
- eg:
  - Executabel binaries can be loaded and executed
  - Text file can be indexed
  - Pictures can have thumbnails generated from them
  - File can automatically be opened by corresponding programs
  - Also the system can stop the user doing something stupid like printing an mp3
    file
- All operating systems "know" about executable binary files
- They have an OS specific structure: information for the loader about necessary
  libraries and where different parts should be loaded and where the first
  instruction is
** Dealing with file types
- Windows deals with different file types using a simple extension on the file
  name
- The extensions are connected to the programs and commands in the system
  registry
- But there is nothing to stop a user changing an extension (except a warning message)
- UNIX uses magic numbers on the front of the file data
- If the file is executed the magic number can be used to invoke an interpreter
  for example
- For "universal" file types many OSs can extract information from files

#+DOWNLOADED: screenshot @ 2021-04-21 11:10:43
[[file:images/File_Systems/2021-04-21_11-10-43_screenshot.png]]

- where is the magic number information stored? =/usr/share/file/magic/= which
  has a standard format for the magic numbers
** (old) Macintosh solution
There are some other ways that files can be handled:
- Resource fork is a part of the file that is used for preferences
- The Macintosh used 8 bytes to identify file types (4 for the creator and 4 for
  the type)
  - Not normally visible to the user. Therefore harder to change by accident
- Also more *structure*: each file has two components (one can be empty)
- Resource fork (/rsrc)
  - Program code (originally),
  - Icons, menu items
  - Window information
  - Preferences
- Data fork
  - Holds the (possibly unstructed) data, program code
  - eg. the text of a word processing document

#+DOWNLOADED: screenshot @ 2021-04-22 09:59:49
[[file:images/File_Systems/2021-04-22_09-59-49_screenshot.png]]
- Each program includes in its resource form a list of all the types of files it
  can work with
- In MacOS X if using the Unix File System the resource fork has merged back
  into the data fork
  - Or into bundles (directories which look like single files)
- Under HFS+ it is possible to have multiple forks
** NTFS
- NTFS takes a very general approach to file attributes
- NTFS views each file (or folder) as a set of file attributes
- The most important one is usually the data attribute
- New attributes can be added

#+DOWNLOADED: screenshot @ 2021-04-22 10:01:51
[[file:images/File_Systems/2021-04-22_10-01-51_screenshot.png]]


#+DOWNLOADED: screenshot @ 2021-04-22 10:02:12
[[file:images/File_Systems/2021-04-22_10-02-12_screenshot.png]]

** Alternate Data Streams
- Have you own data streamed onto a file/folder, the problem with this is that
  with the tools that we use we don't see a change in size which is a security
  risk

#+DOWNLOADED: screenshot @ 2021-04-22 10:02:50
[[file:images/File_Systems/2021-04-22_10-02-50_screenshot.png]]

** Representing files on disk
- We know that disks deal with constant size blocks. All files and information
  about files must be stored in these blocks (usually accessible via a logical
  block number)
- We need to know what we want our file system to look like to the users in
  terms of its structure
- We also need to know how we can place the required information on the disk
  devices
** Structure
- Data about files and other information about storage on a device is called
  metadata
- Usually a disk device has one or more directories to store metadata
- There directories can be arranged in different ways
- *single level*: simple small systems did this, espically with small disk
  devices (floppies)
- Disadvantages in finding files as the number of files grows (some
  implementations use a B-tree)
- To be workable it requires very long filenames

#+DOWNLOADED: screenshot @ 2021-04-22 10:05:59
[[file:images/File_Systems/2021-04-22_10-05-59_screenshot.png]]

** Multiple levels
- *two level* the top level (Master file directory) is one entry per user on a
  multi-user system, the next level (user file directory) looks like a single
  level system to each user
- Creating a user file directory is usually only allowed for administrators
- Use file directories can be allocated like other files. What about the master
  file directory?
  - When people log in they are placed within their own directories. And files
    mentioned are in that directory
  - Can use full pathnames to refer to other user's files (if permissions allow
    it)

#+DOWNLOADED: screenshot @ 2021-04-22 10:08:14
[[file:images/File_Systems/2021-04-22_10-08-14_screenshot.png]]

** Normal file hierarchy
- *tree*: as many directories as required. This facilitates the organisation of
  collections of files
- Directories are special files. Should users be able to write directly to their
  own directories? 

#+DOWNLOADED: screenshot @ 2021-04-22 10:13:13
[[file:images/File_Systems/2021-04-22_10-13-13_screenshot.png]]

** Sharing files and directories
- We commonly wsame data to be accessible from different places in the
  file hierarchy, eg: Two people might be working on a project and they both
  want the project to be in their local directories
- The can be accomplished in several different ways
- If the data is read only we could just make an extra copy
- We could make two copies of the file record information (not the data)
  - There is a problem with consistency
- There can be one /true/ file entry in one directory. Other entries have some
  reference to this entry
  - UNIX symbolic links and windows shortcuts
- There can be a sepatate table with file information. Then all directory
  entries for the same file just point to the corresponding information in the
  table
  - UNIX and NTFS hard links

** Hard links
** Windows shortcuts
- Make a file eg: test.txt
- Make a shortcut to the file
- Rename the original to "test
- By default windows uses attributes such as file type, size and time of
  modification to find renamed shortcuts. Except when I just tried this in
  Windows 10 I get an errors message
- Much better under NTFS with the Distributed Link Tracking Client service
  running
  - This keeps a log of changes made to files that have shortcuts. SO the file
    can almost always be found
  - It even works in distributed environments (sometimes)


#+DOWNLOADED: screenshot @ 2021-04-29 06:00:38
[[file:images/File_Systems/2021-04-29_06-00-38_screenshot.png]]

** Mac aliases
HFS+ has a unique, persistent identifier for each file or folder. An alias
stores this identifier with the pathname.
Originally the identifier was first used to find the file. Only if this failed
was the pathname used
Now in order to work more like symbolic links the pathname is used first. This
means that if a file is renamed and a new file with the old name is created both
and alias and a symbolic link will find the same file (the new one)
If the pathname does not find the file but the identifier does then the pathname
is updated to the current correct value

** Cycles in the directory graph
- If we allow directories to appear in multiple places in the file system we can
  get cycles
- We don't don't want to fall into infinite loops if we traverse the file system
  eg: to search for a file

#+DOWNLOADED: screenshot @ 2021-04-29 06:04:59
[[file:images/File_Systems/2021-04-29_06-04-59_screenshot.png]]

We need some way of uniquely identifying directories. In UNIX directories can
only be linked with symbolic links and algorithms count symbolic links on
directories to stop infinite recursion

** Deletion of linked files
- With hard links we maintain a count of the number of links. THis gets
  decremented each time one of the links gets deleted. When this finally
  reaches zero the actual file is deleted
- With symbolic links if a link is deleted we do nothing. If the real file is
  deleted...
  - We could have dangling pointers
  - Or we could maintain a list of all linked files and go around and delete all
    of them
* Methods of allocation
** Terminology
- The things on the disk that hold information about files: The textbook calls
  the file control blocks FCBs.
- There are several file tables held in different types of memory and accessed
  by different entities (as we will see)
- In this case the on-disk representation that holds the file information,
  attributes and other pointers
- In UNIX this is the inode
- In NTFS this is the MFT file record
- In MS-DOS this is the directory entry

** So what is in a directory?
- The file name (actually the last part of the pathname): We can scan the
  directory to see if the file exists within it
- We can have the file attributes stored in the directory entry. (MS-DOS)
- At a minimum we need a pointer to the file attributes and location information
- UNIX keeps the file attributes and location information file in a separate
  structure: the *inode* (information node or index node). The inode also keeps a
  count of the number of /hard/ links to the file
- The inode table is an array of inodes stored in one or more places on the disk
- So a UNIX directory isn't much more than a table of names and corresponding
  inode numbers

** NTFS directory entries
- All file and folder information is stored in the MFS (Master File table)
- Each file has at least on file record consisting of the attributes from
  before, see the picture below
- Folders have an indexed table of file information
- Each folder entry includes the file name, a pointer to the file's MFT entry,
  plus the most commonly referenced file attributes: eg created and modified
  dates and length
- So much of the information in the files MFT record is duplicated in the
  directory entry. Why?
  - Perhaps it's so that we don't have to go out and read every block of the
    file that is in the folder
*** TODO  You need to check the recording here
- This explains the hardlink behaviour discussed elsewhere
- An MFT entry for a file or directory:

#+DOWNLOADED: screenshot @ 2021-04-29 06:19:22
[[file:images/Methods_of_allocation/2021-04-29_06-19-22_screenshot.png]]

** Finding the file blocks
- Whether it is in the directory entry or in some other data structure there
  must be a place on the disk which points to the blocks allocated to each file
- This can be done in many different ways. It depends on how blocks are
  allocated to files
- Some early file systems always allocated files in contiguous blocks
  - So only the start block and the number of blocks needed to be stored
- Otherwise blocks could be allocated from all over the device
  - So a table of block usage needs to be held for each file

#+DOWNLOADED: screenshot @ 2021-04-29 06:21:26
[[file:images/Methods_of_allocation/2021-04-29_06-21-26_screenshot.png]]

** Contiguous allocation
- Have to find a large enough hole
  - First fit: always starts at the beginning of the device
  - Next fit: like first fit but next time starts from most recent position
  - Best fit
  - Worst fit
  - Buddy algorithm
- First fit is commonly used
- External fragmentation: Lots of holdes outside files (and too small to hold
  bugger files)
- How much space should be allocated for each file?
  - If we allocate too little it is a pain to increase the size
  - If we allocate too much we are wasting space (internal fragmentation)
- Extents: collections of contiguous allocation (NTFS)

** Linked allocation
- The second major method to keep track of the blocks associated iwth each file
  is linked allocation
- A small amount of space is set aside in each block to point to the next (and
  sometimes previous as well) block in the file
- The directory entry holds a pointer to the first block of the file and
  probably the last as well

#+DOWNLOADED: screenshot @ 2021-04-29 06:26:20
[[file:images/Methods_of_allocation/2021-04-29_06-26-20_screenshot.png]]

** Pros and Cons of linked allocation 
- Pros
  - Simple
  - No external fragmentation
- Cons
  - To directory access a particular block, all blocks leading to it have to be
    read into memory. So direct (random) access is slow (at least until pionter
    data is stored in memory)
  - Damage to only block can cause the loss of a large section of the file (or
    damage to two blocks if doubly linked)
    - We could store a file id and relative block number in every block
  - Because of the pointers each data block holds a little less data than it
    could. Only a problem with small blocks

** MS-DOS & OS2 FAT
- The MS-DOS File Allocation Table (FAT) is a collection of linked lists
  maintained separately from the files. One entry for each block on the disk
- A section of disk holds the table
- Each directory entry holds the block number of the fie's first block
- This number is also an index into the FAT
- At that index is the block number of the second block
- This number is also an index into the FAT
- etc...
- Over the years the number of bits used to index the FAT changed (9 to 12 to 16
  to 32) as disks got larger. So the size of the FAT and the maximum number of
  allocation units on the disk grew

** FAT advantages
- Accessing the FAT data for a file requires far fewer disk accesses than for
  normal linked access
- One block of FAT data might hold information for several blocks in the file
- If we have enough memory to cache the entire FAT we can determine the block
  numbers for any file with no extra disk access
- Unfortunately as the FAT gets larger it is more difficult to cache the whole
  thing and so it becomes more common for a single block access to require
  multiple FAT block reads

#+DOWNLOADED: screenshot @ 2021-04-29 06:35:21
[[file:images/Methods_of_allocation/2021-04-29_06-35-21_screenshot.png]]

** Indexed allocation
- A partial way around this to keep all block numbers in a contiguous table for
  each file

#+DOWNLOADED: screenshot @ 2021-04-29 06:36:21
[[file:images/Methods_of_allocation/2021-04-29_06-36-21_screenshot.png]]

** Pros and Cons of Indexed Allocation
- Pros
  - Good for direct access
  - Information for each file is kept in one area
  - No external fragmentation
- Cons
  - File size limited by the number of indices in the index table
    - But we can extend this in a number of ways
  - If we lose an index block we have lost access to a whole chunk of the file

** Extending index blocks
- If the file has more blocks than we can reference from the index block we need
  to have some way of connection to more index blocks
- We can link index blocks together (like the linked allocation of
  files). Similar pros and cons
- We can have multiple levels of index blocks
  - The first level points to index blocks
  - The second level points to actual blocks
  - eg: If we have blocks of size 8K, a block address of 4 bytes and we have a
    two level system we can address files of up to 32 gigabytes
  - The indirect index block points to the 2048 index blocks
  - Each index block points to 2048 actual file blocks
  - So we can have 4194304 actual blocks in a file
  - How do we find the address of a particular block?

** UNIX index block scheme
- In order to minimise the number of blocks read to find the actual block
  (especially with small files) several versions of UNIX don't use extra
  indirect blocks until necessary
- NTFS goes one step further and stores small files < 1K in the MFT file record
  itself

#+DOWNLOADED: screenshot @ 2021-04-29 06:41:49
[[file:images/Methods_of_allocation/2021-04-29_06-41-49_screenshot.png]]

** NTFS Extents
- NTFS keeps all information in the MFT. Rather than storing all block numbers
  it stores *extents*
- A cluster is a number of blocks (2,4,8,etc). An NTFS volume is seen as an
  array of clusters
- An extent is a start cluster number and a length(number of clusters)

#+DOWNLOADED: screenshot @ 2021-04-29 06:44:28
[[file:images/Methods_of_allocation/2021-04-29_06-44-28_screenshot.png]]

** Keeping track of free blocks
- Whenever a new block is requested for a file we need to check to see if there
  is an empty block. How do we keep track of all the empty blocks?
- There are lots of them - a 6Tb disk with 4Kb blocks has about 1.6 billion
  blocks
- We could link the free ones together like one large empty file using the linked
  allocation method
- With a linked list it is trivial to find the first free block
- But it not efficient if we want several blocks at a time (espcially if we want
  them to be contiguous)
- We can maintain a bitmap (or a bit vector). Where each bit represents a used
  or free block (1 represents free)
  - For the above disk this takes up about 200 Megabytes of space
- We can keep a list of start points and lengths (like NTFS extents)

** Eg: UNIX 4.3BSD file system
- Early versions of UNIX maintained a device wide free list of blocks in a
  stack, whereby the next allocated block is the most recently returned one
  - What are the consequences of this? Wasted space I'm assuming
- Also all inodes were stored in one place
  - What are the conseqences of this? If there's corruption in your inode table
    that means that you're losing your table to a whole lot of inodes
- *Cylinder groups* were introduced in order to improve file access and
  reliability
- Cylinder groups: one or more consecutive cylinders (disk head seek time is
  minimised so access to all blocks in the same group is fast). inodes and free
  lists stored with each cylinder group. Each cylinder group has a copy of the
  superblock for the "file system" (UNIX terminology for a partition of disk
  device). Tries to keep blocks from the same file within the same cylinder
  group. But larger files are split over different cylinder groups

** Runtime file data structures
- Now that we know how the information can be represented on the disk we need to
  nkow about the data structures the OS maintains when we use a file
- *System wid open file table*
  - THe system must keep track of all open files
  - Information from the on-disk FCB (these must be kept consistent)
  - Which processes are accessing the file?
  - How is the file being accessed? Is is read/write only?
- *Process open file table*
  - A pointer to the system open file table. As we don't want to replicate
    information about the file if it's just in the MFT or inode
  - Current file position (for sequential reading or writing). May be in a
    system wide table
  - A pointer to the buffer being used for this file by the process
- *The file buffer*
  - Data is read in block (or cluster) amounts

- If you have a file in memory and then modify it. You then need to write that
  file back to the disk

** UNIX runtime file structures
- Here we see
  - The on disk inodes and data blocks
  - the incore copy of the inode, this includes the reference count
  - the system wide open file table (file structure table), this actually stores
    one entry for every time the file was opened
  - the process open file table, just an array of pointers to the file structure
    table
  - What would a fork do?
    - When calling fork it's not just data in the user space that is copied,
      it's also some of the data in the system space as well which causes some
      interesting effects
    - Upon calling fork the child will get a copy of the open file table as it
      was when the parent called fork
    - The position where you are in a file is stored in the system wide file
      table. Which means that if you are reading from position 1000 in a file on
      a parent process and then call fork. If the child then reads to the same
      file it will *start from position 1000*. And the location pointer will
      then move on to where ever the child reads to. The parent will then read
      from whereever the child left off.

#+DOWNLOADED: screenshot @ 2021-04-29 07:16:26
[[file:images/Methods_of_allocation/2021-04-29_07-16-26_screenshot.png]]

** UNIX fork interaction

#+DOWNLOADED: screenshot @ 2021-04-29 07:16:45
[[file:images/Methods_of_allocation/2021-04-29_07-16-45_screenshot.png]]

=File.tell()= tells you where you are in the file.
- You can see that the output is different after running it multiple times. How
  is the data there twice? You can tell it's being written twice because 30
  bytes is twice the payloads length.
- When we call file.write we have not actually written to a file! We have
  written to the file buffer, we have not flushed it. This file buffer is
  duplicated when calling =fork()= and then the OS gets around to writing it
  which it is then written for each process. Thus we get a file pointer index of
  30 (where both buffers are written to disk). If you then call =file.flush()=
  before forking then you get the expected result.
- Never write to a file and then fork. Make sure that you are flushing
  first. Technically flush doesn't write to disk, it just moves that information
  from the user space that is forked to some system space that isn't copied on
  fork.

** Opening and closing files
- Most systems require some call to make the connection between a process and a
  file
- The open call does several things (not all OSs do all of these):
  - Searches for the file with that name
  - Verifies that the process has access rights to use the file in the way
    specified
    - This means we don't check after this
    - This can be a security problem, sometimes referred to as the TOCTTOU (time
      of check to time of use) problem
  - Records the fact that the file is open (in the system-wise open file table)
    and which process is using it
  - Constructs an entry in the process open file table
  - Allocates a buffer for file data
  - returns a pointer (file handle or file descriptor) to be used for future access

** Opening a file in UNIX
- =fd = open(filename, type of open)=
- convert filename to an inode: this also copies the on-disk inode into memory
  (if not already there) and *locks the inode* for exclusive access
- If file does not exist or not permitted access
  - return error (ensure inode is not locked)
- allocate system wide file table entry, points to incore inode, increment the
  count of open references to the file
- fill per-process file table entry with pointer to system-wide file table entry
- *unlock the inode*
- return the index into the per-process file table entry (known as the file
  descriptor)

so therefore there is a short period of time where the inode information for the
file is locked so that other people can't play around with it. We then unlock
the inode

** UNIX write system call

#+DOWNLOADED: screenshot @ 2021-04-29 07:41:56
[[file:images/Methods_of_allocation/2021-04-29_07-41-56_screenshot.png]]

If this is the code for writing then nobody can possibly access the file
(because you have locked the inode) which makes this atomic. You then release
the inode lock then the other things can access the file. When does it really
get to the disk (the data)? delay write just says at some point please just
write this to disk in the future

** Delay write
- Buffers are shared by the system
  - The write doesn't occur until another process is to use the buffer for a
    different block (LRU replacement) or a daemon process flushed it
- _Advantage_
  - if a process wants to access this information it is already/still in
    memory. This is in memory so any process can access it
  - eg: process writes some more and it fits in the same block
- _Disadvantage_
  - Information is not written immediately. Usually a daemon process writes data
    buffers after 30 secs, metadata buffers after 5 secs
  - =sync= command forces buffers to write

** UNIX append
- If the file has been opened in append mode O_APPEND then there is a possible
  race condition
  - This is where multiple appends happen at the same time then will overwrite
    each other because the file pointer index is the same (where you are in the file)
- Before each write the file position pointer is moved to the length of the file
- What if another write changes the length of the file before this write
  completes?
- The file system must guarantee atomicity for the append write operation
- This is why there is an append mode for opening a file
* File versioning systems
- It can be very useful to keep earlier versions of files
- We can recover from mistakes
- We can restore damaged files
- We can compare versions to see the changes made
  - Useful for security purposes (self securing storage)
  - Also useful for other purpose: eg working on a project with others and you
    want to see the changes they made
- Sometimes we want to use earlier versions and still hold on  to the recent
  versions
- Similar to code management services like Git
- Can be done in a variety of ways but all require extra disk space
- This sort of thing can be done at different levels (eg, software or disk
  level)
** Methods of versioning
- A new version could be created when the file is closed of saved
- A new version could be created after every modification: *comprehensive*
  versioning. Obviously a lot more versions
- Either way we can
  - Keep Complete copies of all previous versions
    - Very space intensive
    - But fast to retrieve/recover
  - Keep a journal (or log) of changes
    - The journal keeps a record of the changes between two versions
  - keep a tree with all data
    - Finding any version takes the same amount of time
    - Can be slow for current version if the tree is big
*** Example

#+DOWNLOADED: screenshot @ 2021-04-29 11:11:20
[[file:images/File_versioning_systems/2021-04-29_11-11-20_screenshot.png]]

** Log version
- original version was
  - /Dear Mum, I hope you are well./
- *changes to get to version 2*
  - 11i54 (54 chars inserted at position 11)
- *changes to get to version 3*
- 13d (a deletion at position 13)
- /am/ (the deleted data, this must be kept)
- 13i13
- 74i21
- The current version is always stored
  - Dear Mum, I /thought I was/ doing really well in my Operating System course
    /until I sat the test./ I hope you are well
- To get previous versions have to go backwards through the log
- If we want to be able to roll forward from a checkpoint we need the new data
  in lines like 13i13
** Multiversion B-tree

#+DOWNLOADED: screenshot @ 2021-05-03 08:37:01
[[file:images/File_versioning_systems/2021-05-03_08-37-01_screenshot.png]]

To get the current version of the file you have to iterate through all versions
to get the current. Whereas in the log version it was quick to create the
current version. Btree quick to revert to previous but log slow to revert
** Advantages and disadvantages
- Log system
  - Very compact
  - Access to the current version is the same as without versioning
  - slow to revert to previous versions especially if there are meny versions
  - Can use checkpoints to improve this, but this adds considerably to the space
    requirement
- Tree system
  - very compact
  - quick to revert to any previous version
  - if there are lots of versions the tree can be big and then access to the
    current version will be a little slow
  - Can keep a separate copy of the current version, this also adds to the space
    requirement
- No method works well if the data between versions is completely different. We
  are stuck with having to keep complete versions
- It's common to have a complete version stored in the tree system to mitigate
  its issues

** VMS versions
- When a file is closed VMS checks the number of versions. If the number is
  greater than the maximum number then the oldest version is discarded
*** Windows, XP onwards
It takes a checkpoint (restore points) of *important system files* on a regular
basis
- daily
- On installation of new drivers and applications
NTFS maintains a log of all changes to metadata, along with redo, undo
information and whether the change was committed. SO it can recover all metadata
to a consistent state after a crash (but not all data).
*** Windows and Mac
- Windows: Volume Shadow Copy
  - Keeps copies of files on volumes which have the service turned on
  - Also used to create restore points
  - Works at the block level: only modified blocks are copied
  - Trypically made once a day
  - Users cannot trigger new versions of individual files
  - Users can access versions from Previous Versions tab of file properties
  - Not available at GUI level in Windows 8 but still in Windows Sever and came
    back in Windows 10 (based on File History)
- OS X Lion onwards: Versions
  - Auto saves individual files (if enabled by the application)
  - Works on chunks (intelligently determined by content): only modified chunks
    are copied
  - Typically made every hour (autosave works every 5 minutes or during pauses)
  - Users can trigger new versions for individual files
** ZFS snapshots

#+DOWNLOADED: screenshot @ 2021-05-03 08:48:48
[[file:images/File_versioning_systems/2021-05-03_08-48-48_screenshot.png]]

- "snapshots"
- COW: Copy on write come blocks when they are modified. You're then changing
  which blocks the original is kept as a backup while the user sees the new
  greenblocks. Instead of throwing away the blue block you can keep it like on
  Bonus: Constant-time snapshots for a way to keep backups

** Pruning
- All conventional versioning system use pruning to keep the amount of data
  stored under control
- Different heuristics
  - a fixed number of versions
  - treat some changes as more important than others
  - "observe" user behaviour, eg, most often accessed
  - the user has to explicitly request a version be held
- Snapshot systems: keep versions of files at particular times
- only keep version for a small number of files

** Self-securing storage
- All metadata, directories and critical files (OS files) are kept on a
  versioning system
- Any intrusion (that uses files) can be tracked because the intruded cannot
  erase changes they have made to the system
- We need to maintain all versions between checks for intrusion
- This is referred to as the detection window
- If the system is unable to keep enough versions we signal an alarm
  - Either something has gone wrong, in the sense of not enough space allocated
    for a normal amount of usage
  - Or someone is trying to force a pruning to hide their tracks

* Distributed File Systems
A Distributed File System (DFS) is a file system which has data stored in
several different sites of hosts (computers and associated devices) on a network
- Advantages
  - Greater amount of storage
  - Greater flexibility for administration and sharing purposes
    - Users can log on to any machine and have access to all of their files
    - Files can be stored close to where they are normally used but are still
      available elsewhere
  - Can replicate information for greater reliability
    - If a site goes down the files may still be accessible from another
      location
** Accessing Remote Files
Different approaches to providing access to files over networks
- Remote file transfer approach
  - No requirement for the machines to even be running the same operating system
  - A user can explicitly connect ot another machine on the network and download
    files. eg: FTP
  - Can't directly use a file on the remote site
  - We end up with multiple copies and no method of maintaining consistency
  - The user must know exactly where a file is (including the host)
- Direct access using explicit site names
  - Each file is prefixed with a site identifier eg: cs26.auckland.ac.nz:/home/robert-s/310exam
    - But it is usually automatically copied to and fro
  - The user still must know exactly where a file is located
  - No replication possible
** Better methods
It is better if we don't have to specify the host name when accessing remote
files
- _Approach 1_
  - Keep information on each machines pointing to the machine where the files are
    actually stored
    - files can be used directly on the remote host
    - the user sees no difference between accessing remote and local files
    - remote files may not be visible from all machines on the network, even if
      they are, they may have different pathnames
    - moving remote directories entails changes on all local machines (location
      information has to be changed on all machines)
- _Approach 2_
  - Keep a server(s) with location information and associate a standard
    directory base with all remote files
    - Direct use and the same view of the file system regardless of where you
      are logged on from
    - Can move files without any information needing to be sent to local
      machines, you only need to update the location server
    - can replicate files
  - If it's not a local file then you go to the location database server and
    then it will tell you where it is then you connect to the correct server

** Transparency
An ideal: The distribute system should look like a single machine and associate
files. It is called transparency because the user should not be able to see the
differences (and complications)
- *location transparency*
  - No obvious connection between the name of a resource (file) and its positoin
    on the system
- *migration transparency*
  - Resources (files) can move around the system without programs needing to be
    changed (or stopped and restarted). This is similar to what the textbook
    calls /Location independence/: The name doesn't have to be changed with the
    location changes
  - It needs location transparency to implement

** Collections of files
- All common distributed file system group files into collections for simplicity
  and administration pruposes
- The collections (compoent units in the textbook) are commonly subtrees
  stemming from particular directories
- So the subtrees are shared and moved and replicated together
- If we are going to transparently migrate component units we need to have a
  structure something like this

#+DOWNLOADED: screenshot @ 2021-05-03 21:09:28
[[file:images/Distributed_File_Systems/2021-05-03_21-09-28_screenshot.png]]

** Using remote files
- Once we have discovered where a file is we have to move all data accessed (not
  necessarily all data) from the file over the network
- This is expensive and has problems with consistency
- We could:
  - send only required blocks back and forth between the server and the client
    (*remote service*), every file access results in messages across the
    network. What happens if there are two write commands sent to the server at
    the same time? They will overwrite.
  - Keep copying large chunks when required and *cache* them at the client. When
    opening file you ask the remote to send a chunk and then you cache that.
  - Copy complete files when accessed and copy back when done (only if written to)

** Caching
- Blocks of files are cached locally
- All accesses come from the cache
- If a block is not in the cache it is requested from the server and then cached
- Old blocks can be replaced using Least Recently Used algorithm

If we modify data in the cache
- We do we send the information back to the sever?
  - Write-through: every write requires the block to be sent back to the
    sever. People will see your file change in real time
  - Delayed-write: send the block to the server at a later time (check every 30
    secs, or when the file is closed or when the cached block is needed for
    another block)
- How do we cope with writing to the cache when other processes (on other sites)
  also have the file open?

** Pros and Cons
- *Caching*: usually faster, more efficient, scales better than remote service
- *remote service*: simpler to implement because there is no consistency
  problem, uses less local memory (primary or secondary), matches local file
  access
- Some system use both schemes, basically providing a remote service solution
  but with some caching for efficiency reasons

** Consistency semantics
- When you are sending changes to blocks back when does the remote then write
  those changes so that those changes are visible to other processes on the
  network
- The way changes in data get distributed between processes accessing the same
  files is known as consistency semantics. There are two major types
- *UNIX semantics*: any change made by any process is immediately visible to any
  other process
- *Session semantics*: the process gets a copy of the file when it is opened and
  changes are not visible to other processes until a file is closed
- Both can be worked with but programmers need to known which is used on the
  system they are programming

** Remote Service: Stateful
Server knows
- Who has the file open
- For what type of access
- And where it is in the file etc
- When the client calls open it receives and identifier to be used to access the
  file
- Looks very similar to traditional local file access to the client process
- Efficient, the needed data may be read ahead by the server
- Information about the file is held in memory
- If the server crashes
  - If is difficult to start again since all the state information is lost
- Server has problems with processes which die
  - Needs to occasionally check

** Remote service: Stateless
- Server does not maintain information on the state of the system. It merely
  responds to requests
- Open and close calls don't send messages to the server. Handled
  locally. (Except for access privileges.)
- Requesting processor has to pass all the extra information with each
  read/write
  - eg. the current file location the process is reading from,
  - accessibility information
- Server doesn't have to worry about processes stopping
  - It doesn't keep any records and is not taking up any memory space on the
    server
- No complicated recovery process if the server goes down
  - A new server (or the recovered old one) just starts handling requests again

* NFS
- Was Sun's Network File System: a stateless system
- Based on the UNIX method of /mounting/ disk devices within a file directory
  tree
- Another disk can be mounted at /usr/local/ for example. The inode contains a
  bit indicating a device is mounted there (a table holds the required
  information)_
- In NFS remote file directories can be mounted on a local directory structure


#+DOWNLOADED: screenshot @ 2021-05-03 21:31:27
[[file:images/NFS/2021-05-03_21-31-27_screenshot.png]]


#+DOWNLOADED: screenshot @ 2021-05-03 21:31:31
[[file:images/NFS/2021-05-03_21-31-31_screenshot.png]]

** NFS
- No need for dedicated servers (meaning that every server could be both a
  server and a client)
- Remote directories (or entire devices) can be mounted anywhere in the local
  directory tree
- Works with heterogeneous environment (doesn't matter what arch/OS that you are
  using. It works as a protocol for multiple arch/OS)
  - RPC and XDR: external data representation
*Mount protocol*
- Mount servers on each machine
  - export table /etc/exports
    - This keeps information about which directories will be exported and which
      machines on the system will have access to the files that you put in the
      file
- Requests come in from the clients and the exports are checked and then if you
  have access permissions you get a handle
- the full pathname of the directory to be exported
- The client machines that will have access to the exported directory
- Any access restrictions
- Request comes to mount a directory from this machine
  - Returns a file handle to this directory (file system: inode)
- Server maintains a list of which machines have mounted one of its directories

** Automounter
- Client maintains a list of the directories which are mounted from other
  systems. It will go around and flag all the directories that might have
  information at a remote and then mounts them automatically
- /Automounter/ mounts and unmounts remote directories on demand
- Uses maps (files containing links between the mount point and the actual
  directory)
- eg: Setting up a shared namespace for /home
  - First an entry is made in auto_master (master configuration file) which
    associates the mount point /home to a map called auto_home:*/home:
    auto_home. What this means is that whenever you access a file in */home you
    then have to read another map called auto_home to actually find out where
    the real files are stored on what particular server
- auto_home is a map that associates user names to home directories on their
  respective servers:

#+DOWNLOADED: screenshot @ 2021-05-05 11:06:11
[[file:images/NFS/2021-05-05_11-06-11_screenshot.png]]

so You can see that /sally/ has her files on server1 and that includes the
directory where her files are located on that server

- When the /automount/ command is invoked at system start up time it looks in
  auto_master and then auto_home and knows to set up /home as a directory of
  mount points. These mount points will become mounted file systems at the time
  they are referenced by users. Users can be added or deleted from the namespace
  by adding or subtracting them from the auto_home map. Any changes will be
  automatically implemented the next time the file system is mounted
- the automounter uses a timeout (usually 5 minutes) to unmount a directory when
  unused.
- Also when you have a folder that include remotes they will have a special flag
  that indicates that you should be using NFS

** NFS protocol
So how is this all tied into our normal filesystem? You can see that everything
has to go through the VFS interface which then routes the system call to the
particular client (in this case the NFS client). This request is then packed
into an RPC call, goes over the network and then is decoded. On the remote
machine you're then going through its VFS interface which routes

- normal system call
- VFS (virtual file system) determines local or remote
- NFS service layer makes the RPC to the remote machine
- request gets pumped into NFS on the remote machine
- carried out locally
- then back goes the result

#+DOWNLOADED: screenshot @ 2021-05-05 11:09:10
[[file:images/NFS/2021-05-05_11-09-10_screenshot.png]]

** Looking for a particular file
Accessing /usr/local/bin/dir1/filename on machine 1
/usr/local might be a directory on machine 2
and /usr/local/bin is within that directory on machine 2
but /usr/local/bin/dir1 might be a directory on machine 3.
Once a mount point to a remote file is crossed then accessing the rest of the
path is expensive. Because you have to send lots of information backwards and
forwards to see what's in the directory
Each directory has to be read remotely
Actually it must be checked locally first in case there is another mount point
to a different directory on another server.
Each machines maintains a directory name cache to speed up the lookup

** Problems with NFS
- Administration is difficult
- As all sites can mount exported subtrees anywhere it can be difficult to
  maintain a uniform view of the directory structure
- Moving a collection of files is complicated
- All sites need to be notified as they all have location information stored in
  their maps
- Can provide replicas of read-only file collections. But these suffer from
  difficult administration as well
- Because of these problems is doesn't scale well. Only used on medium size
  networks
- Some of these problems have been addressed in the latest versions of NFS

** AFS
Originally called the Andrew File System
 - now known as AFS
- Local name space (like NFS)
Some files only appear on the local host. Usually machine specific files.
- Shared name space
/afs is the root of all shared files
- Identical on each client (not like NFS) and location transparent even over a
  WAN (SSI - single systems image)
- Files can be relocated without removing access (except for a brief time)
  - Copy them across
  - Update the location servers
  - The original location still handles old requests: shipping them to the new
    location. So if you don't shut down then the original server A will reroute
    to the new server that actually has the files to server B
  - Remove the originals
- Scales easily (to thousands of machines)
- Uses kerberos for authentication as a SSO
login and be authenticated once for all network access.
Mutual authentication: clients *and* servers authenticate themselves when you
log in

** AFS implementation
- The Volume Location Database (VLDB) contains the location information and is
  usually held by several servers. This means that you have some redundancy in
  the lookup so that if one server goes down there are backup servers that you
  can tap into
- Servers are dedicated; they are *not* also clients
- Files are grouped into volumes
  - A volume is commonly the files of a particular user or groups of users
  - The volumes are the things that are referenced in the location database
  - Volumes can be transparently migrated
- Files identified by volume: vnode_number: uniquifier
  - Vnode numbers can be reused, therefore to keep uniquness there is the
    uniquifier (extra bits added on until unique)
- How does it find/access files? There's a cache manager
- Client machines run a Cache Manager process
  - Finds where files are (from the VLDB)
  - Retrieves files from the host (the entire file)
  - Uses caching rather than remote service
  - files are cached in large chunks (64K): hopfully the whole file
  - This minimises network traffic and is usually more efficient at both client
    and server ends
  - Because we are caching we are just accessing local files from that point
    onwards. Question: When do we update things at the other end?

** AFS shared access to files
- Files and directories are protected by access control lists not the simple
  unix bit protection scheme
  - This means that a directory may look as though it is not protected (eg. it
    is writable) when it is actually protected
- Session semantics are used. When you make changes it makes them locally and
  then when the file is closed then you're sending back all the changes to the
  server
- Changes in shared files are not seen until the file is closed (or synced)
- Callbacks
  - A callback in AFS is a promise from the server that the cached version of a
    file is up to date
  - Before a file is changed the server breaks the callbacks. Thus it notifies
    all the other hosts. They make requests back to the server to get the new
    version
  - Then when another process uses its cached version the Cache Manager detects
    the broken callback and refreshes it cached data from the server
  - It's not as stateless because it has to keep track of all the clients using
    the files
- So AFS is not a stateless as early versions of NFS

* NAS and SAN
- Many distributed file systems are provided by Network-Attached Storage or
  Storage Area Networks
- Network attached storage
  - Basically file storage available over a network. Can be as simple as a
    device which deals with a protocol such as NFS over the network. NAS devices
    just act as servers but they are designed to do the job efficiently and
    safely. The actual storage may be in a RAID setup
  - They are controlled and configured over the network
  - You can think of them as servers


#+DOWNLOADED: screenshot @ 2021-05-05 19:50:13
[[file:images/NAS_and_SAN/2021-05-05_19-50-13_screenshot.png]]

** NAS vs SAN
- Use a different type of protocol, such as Fibre Channel. These protocls
  provide a specialised high speed netowrk specifically for accessing
  storage. The main difference is that SANs deal with blocks rather than file
  systems. The client side deals with the file system, the SAN provides the
  block storage.
- You can thnk of them as attached to servers, sort of like a really really
  large, very flexible disk devices
- There are also SAN-NAS hybrids

** Question?
- Which of the following would be slower in a DFS?
  - Opening files
  - reading files
  - writing files
The answer: None of them. They are likely to be all equally as fast in the
local version than the distributed version
Opening files:
- In the open call you could design a system where the call only uses local
  information and the server just sends back a file handle. All the open call
  does is create a file handle for the client to use.
Reading files:
Everything is cached. All your reads have been cached therefore it's already
local. And usually it's in memory so you don't even have to go to disk
Writing:
You're going to be used a delayed write which is fast

** Distributed Systems
A distributed system is...
"One on which I cannot get any work done because some machine I have never heard
of has crashed"
- Loosely coupled
- Network connection
- Could be different Oss, or different parts of the OS
- Processes must communicate via messages
_What advantages does a network of sites offer?_
- More work can get done
- Ability to share devices, programs and data
- Greater reliability
- Easier to expand

** Two Phase Commit Protocol
With distributed systems we want to ensure that if someting goes wrong at one
site we don't end up with inconsistent data
A "transaction" is some event that has to completely successful or not done at
all (atomic)
We need stable storage (usually replicated on server devices) can be done with
two copies
 - Make the change to one (check for sucess)
 - Make the change to the other (check for sucess)
 - If ever the copes disagree copy the original data from the second back to the
   first
2PC - Transaction coordinator and all sites involved in a trasaction have stable
storage logs.
All transactions can be undone and redone safely
_Log entries and messages_
*Commit request phase* (started at the end of the transaction)
- <prepare> - started the protocol, sent to all sites (query)
- <ready> - recorded and returned if ok, <abort> if not ok (veto voting)
*Commit phase*
<commit> - if all reply in time, sent to all sites
<abort> - sent be coordinator to all sites if something went wrong (they must
rollback)
- eg: Setting up 
* Two phase commit
** Questions
- What happenes if one of the sites goes down part way through the commit phase?
  - What affect does this have on the other sites?
  - And when it later becomes available?
    - Answer: It sent the ready message, detects it crashes, then it says that
      it doesn't have a commit with that ready. Then it can request information
      from the TC
- What happens if the transaction coordinator goes down part way through the
  commit phase?
  - What affect does this have on the other sites?
    - They wont recieve the commit message.
  - And when it later becomes available?
    - TC will send the commit

* Network & Distributed Operating Systems
- *Network OS*
  - Communications layer on top of a normal OS
  - Possibly different OS
  - User is aware of different machines
  - Some can copy files across the network but not share them. eg: ftp
    - In this case the file location is explicitly known
  - Others can share files but the locatoin is still part of the name
- *Distributed OS*
  - Aim to have the system look like one machine
  - There is no difference (except speed) between accessing local and remote
    resources (location transparency)
  - The distributed SO can move resources and processes (migration transparency)
* Remote Procedure Calls (RPC)
- *Birrell & Nelson 1984*
  - Hide the message passing system so that it looks like a series of procedure
    calls
  - Most requests for service wait until the request is fulfilled: semantically
    just like a procedure call
- Programmer doesn't hvae to package and unpackage data in the messages

#+DOWNLOADED: screenshot @ 2021-05-06 10:29:04
[[file:images/Remote_Procedure_Calls_(RPC)/2021-05-06_10-29-04_screenshot.png]]

** Client and Server stubs
- Client makes ordinary procedure call to the local stub
- Stub marshals parameters (may need to locate server as well)
- Stub sends request via local kernel
- Remote kernel passes request to Server stub
- Server stub unpacks message request and parameters
- Makes ordinary procedure call to Server
- And then vice-versa
- The stubs at both ends need to be constructed from the same interface
  specification: to unsure consistency
- Care has to be taken about different versions of the service. A different
  version may take slightly different parameters or return different types etc.

** RPC messages
- *messages are highly structured*
  - What procedure to execute
  - parameters
  - version number (service may servive a long time and code may be written to
    different versions)
  - timestamp (could be used for synchronization purposes)
  - source address
  - where to send the results
  - possibly the type of machine the rquest comes from
- *Finding the server*
  - include port numbers at compile time or
  - Have a binder or rendezvous/matchmaker service
  - Server usually registers with a binder or name server
  - Client end sends the request to find the server
  - Multiple servers: the binder can spread the load
  - Binder can periodically check on servers - deregistering those that don't
    respond

** Zero configuration
- There are many different zero configuration servicese which don't require a
  binder
- Zeroconf (Apple's Bonjour)
  - Multicast DNS - is "service" here?
  - Flexible: protocols can be decided at access time
- UPnP - Universal plug and play
  - A UPnP device can join a network and get an IP address
  - Announce its name to control points
  - Provide a list of its functions
  - Interrogate other devices
  - Protocols are strighctly defined
- Both are designed for small LANS

** Marshalling
- Heterogeneous networks. Data formats in the different machines may be
  different
- ASCII, Unicode, EBCDIC for strings
- Big or little endian for integers
- Different floating point formats
- Stubs need to know about this
- _Different solutions_
  - Client stub converts to the server format
  - Server stub converts from the client format
  - Convert to and from a canonical format
    - No one needs to know other machines formats
    - eg: NFSXDR - external data representation

** Reference parameter problem
The references no longer make sense
- Either disallow reference parameters
- Or copy the parameter to and fro
- Either in one chunk or whenever the server makes a change to it (this is just
  like distributed shared memory)
- Copy/restore semantics aren't quite the same
  - Pointer as a reference parameter
  - The same parameter passed twice in the parameter list
    - we can check for this. If you're passing through two of the same. It might
      make changes to both of the same pointer instead of the one that you're
      wanting to edit
- Sometimes we don't need to copy the data both ways
  - Eg: input buffer (only needs to be copied from the server to the client)
  - Out interface specification should be able to express this

** RMI & COBRA & DRb
- *Remote Method Invocation*
  - JAVA technology
  - RPC with objects
  - Solves object reference problem by sending serialized versions of the
    object. If object passed as a parameter is itself actually on the remote
    site it is just sent by reference
- *Common Object Request Broker Architecture*
  - Similar to RMI
  - Worked with a wide variety of languages (not just java)
  - Used a common description language to specify interfaces (IDL interface
    Definition language)
- *Distributed Ruby*
  - Very similar but simpler than Java RMI. Normally objects are passed by
    copying. If you
    - =include DRbUndumped=
    - In a class then all references to such objects are sent back to the
      original object

** Linda Tuplespace
- Another technique to share services over a network. Rather then explicitly
  sending messages we can share data in a tuplespace
- The tuplespace is a logically shared (sometimes distributed) memory consisting
  of tuples. A tuple is a list of parameters, some of which are empty, eg <"hi",
  15, 12.5>
- Tuples are put into the tuplespace by the "out" or "write" primitive and
  retrieved from the tuplespace by matching contents and tupes with the "in" or
  "take" primitive. The example above could be matches with <"hi", _, _>
- JavaSpaces are an implemenation of Linda tuplespaces. Rinda is the Ruby
  version of Linda. Linda originally worked with C and Fortran.

** Python example

#+DOWNLOADED: screenshot @ 2021-05-10 10:49:38
[[file:images/Remote_Procedure_Calls_(RPC)/2021-05-10_10-49-38_screenshot.png]]

** Tuplespace advantages
- Processes don't communicate directly with each other but instead access the
  tuplespace. It doesn't matter if one process dies, as long as another that
  deals with the same tuple is availiable (and the tuple hasn't been removed yet)
- Tuplespace problems scale naturally. Adding more processes which handle
  particular requests is trivial
- (The tuplespace itself doesn't scale well. It is usually stored on one
  server. It can become a bottlenack. There are techniques to replicate
  tuplespaces.)
- The space itself deals with synchronization .Each tuple operation is atomic

** Process Migration
- Moving a process from one site to another while it is running
- Why would we like to do process migration?
  - To enable us to do proper load balancing
  - If the process can be subdivided we can increase performance by having
    different parts running simultaneously on different machines
  - To move the process close to the resources it is currently accessing
  - To move the process closer to the user
  - To enable us to keep a process going when the site it is executing on has to
    be taken down
** What do we need?
We need location and migration transparency of
- processes
- resources used by the processes
What defines a process?
 - PCB
 - Resources
   - Files
   - Communication channels
   - memory
   - Devices: including windows, keyboard, mouse
   - Threads
 - Need some compatible machine (or virtual machine) architecture
 - Internal and external reference problems
 - References to resources within the program
 - References to the process from outside. Eg: other processes communicating
   with it
** How Can that be done?
- Need a way of referring to all resources indirectly via a global tables (like
  we did with out distributed file systems)
- We can extend the ideas of a distributed file system to refer to other
  objects, including processes
- All process identifiers have no host information in the identifier
eg:
- A process table keeps track of which site each process is running on
- When the process is moved the table is updated
- Caching of information can be used for efficiency but we need ways to recover
  when the cache data is out of date
- not all processes need to be stored in this table
  - Processes specific to a site which are not visible away from the site

** Doing the migration
- *Minimise the amount of down time*
- Stop, copy, notify
- How much do we copy?
  - Only the working set
    - Get the remaining pages by demand paging
    - Can't be used if the host is going down
  - Everything but don't stop the process
    - Then copy pages which were dirtied during the copy
- Both approaches only stop the process while the working set is moving

** Current uses
- In reality process migration is not used for load balancing
- it is too expensive 
  - Most processes only run for a few seconds
  - Transferring a process can easily take a few seconds
- It is still useful when a machine needs to be closed down for maintenance and
  it has running processes which we don't want to kill
- *Another use - idle workstations*
  - Move processes when no longer idle
- Generally load balancing is only done when a process starts
  - Or when it has to move
- Where is the best place to run this process?
- The textbook also talks about Computation migration - This means sending
  messages (or RPCs) to get work done on another site

* Lock free algorithms
- Another approach is to code so that we don't need locks
- This is difficult: so we use standard lock free libraries of stacks, queues,
  sets, etc
- They usually rely on compare and swap type instructions (similar to test and set)
  - =cas(address, old, new)=
- If the current value at the address is the same as the old value then it
  replaces it with the new value and returns true
- It returns false if the current value is not the old value

** Lock free modification

#+DOWNLOADED: screenshot @ 2021-05-12 16:22:20
[[file:images/Lock_free_algorithms/2021-05-12_16-22-20_screenshot.png]]

- No matter how many threads are accessing this code they will never block
- This is not a wait free algorithm as it is possible that a thread may stay
  looping indefinitely
- There are ways of making sure the wait is bounded. Then it is wait free as
  well as lock free
- Many wait free algorithms increase in memory size as the number of threads
  increases
- There is also the hope of Transactional Memory

** Deadlock
- Multiple resources + locks introduct the danger of deadlock
- A circle of processes each holding a resource wanted by another process in the
  circle
- It is a local phenomena
  - But it can easily spread
- Can it be cured?
  - Not without hurting some process
- At least one process must be forced to give up a resouce it currently owns
  - (or provide a resource eg: a message, which another process requires)

** Conditions for deadlock
- There is a circular list of processes each wanting a resource owned by another
  in the list
- Resources cannot be shared
- Only the owner can release the resource
- A process can hold a resource while requesting another
- One reason deadlock is tricky is that testing may not discover it. It depends
  on the order of requests and allocations

** Deadlock detection
- All resources and processes are vertices in the graph
- Allocation and requests are edges
- Cycles in the graph indicate deadlock (if each square holds one copy of the
  resource). It gets more complciated with multiple resources of the same type
- When should the deadlock detection algorithms run?
  - How often do we expect deadlocks?
  - How many processes are usually affected?


#+DOWNLOADED: screenshot @ 2021-05-12 16:35:09
[[file:images/Lock_free_algorithms/2021-05-12_16-35-09_screenshot.png]]

** Someone has to suffer
- What do we do when deadlock is detected?
  - One of the processes in the circle can be selected and removed. Its
    resources are returned and the deadlock is broken
  - We could use priority or age to select the process
- It may not solve the problem (deadlock may occur again immediately)
  - Remove all processes involved
  - Overkill but certainly solves the problem
- Force a process to restart (or rollback to some safe state)
- If we want to rollback, the system needs to maintain checkpoints where the
  processes can be restarted from
- We must ensure that the same process is not selected for restarting repeatedly

** Deadlock prevention
- We make sure atleast one of the conditions will not be met. ie. it is impossible
for deadlock to occur if we sue prevention
- All resources must be issued in a specific order if you have one of these you
  can't go back and reqest one of those. A then B then C
- an alternative
  - allow reqests from ealier in the ordering if all resources later than this
    are returned first
  - /Resources cannot be shared/
    - Make them sharable?
    - Virtual devices ( printer spooling)

** Deadlock prevention
- /Only the owner can release the resource/
- Forcibly remove: but that causes damage
- OR
- if a new resource is currently busy release all currently held resources and
  try to get them back with the new one as well
- OR
- If removing the resource temporarily does no harm, eg: a page of memory or use
  of the CPU (state is saved and restored)
- *A process can hold a resource while requesting another*
- Only allow one resource at a time?
- OR
- Return a group of resources before reqesting another group
- OR
- Allocate all resources at once, can't ask for more as the process runs

** Deadlock avoidance
- Before granting requests we check if deadlock could occur if we allocate this
  resource to this process. "I can see deadlock might happen if I allow that. So
  I won't allow that"
- This may stop a process from getting a resource ven though the resource is
  available. But dong so leads to a situation which could cause deadlock
  later. So avaoidsance prevents deadlock too, but dynamically as the process
  runs
- System know who has what
  - But doesn't usually know who wants what: has to use a very conservative
    strategy
  - Worst case scenarios of future resource requests
- eg: Two processes P and Q and two units of the same resource R
  - Trouble only develops if both P and Q both require 2 Rs
- Obviously no problem if they each only want one R
  - If P want two Rs and Q has one (and doesn't want anymore), then P has to
    wait until Q releases its R
  - Deadlock only occurs when they both have one and both want one more
- In this case the avoidance algorithm should not allow an allocation to the
  other process when one process already has an R

#+DOWNLOADED: screenshot @ 2021-05-12 17:22:22
[[file:images/Lock_free_algorithms/2021-05-12_17-22-22_screenshot.png]]

** The Banker's algorithm
- Dijkstra invented a deadlock avoidance algorithm known as the Banker's
  algorithm
- Suppose that the reqest has been granted
- Repeat until no more processes can be finished
  - Search for a process which can be given all its resources
  - Return all that process's resources to the system
- If all the process can be removed then the state is safe and the allocation
  can go ahead

** Banker's example

#+DOWNLOADED: screenshot @ 2021-05-12 17:24:56
[[file:images/Lock_free_algorithms/2021-05-12_17-24-56_screenshot.png]]

* Distributed deadlock
- Everything gets just a bit more complicated when dealing with distributed
  systems
- We can still prevent deadlock by resource ordering
- Or we can prevent deadlock by process ordering, only allowing processes with
  higher priorities to wait for resources. Processes with lower priorities get
  rolled back
- But this quickly loads to starvation
- Or we can avoid deadlock by the Banker's algorithm, use one process as the
  banker
- Even more expensive
- More processes, more resources, all requests have to be checked by a Banker
  processes

** Time-stamp prevention methods
- We prevent a cycle by only allowing older processes to wait for resources held
  by younger ones or vice versa. Rather than resource ordering this is process
  ordering
- *wait-die*
  - Process A requests a resource held by process B
    - If process A is older than process B it waits for the resource
    - Otherwise process A restarts, process B (the older) continues
  - Older processes hang around in the system (they have done more work). Age
    has its privileges
  - Younger processes may have to restart multiple times, the resource might
    still be busy (but they eventually get old too, they retain their original
    timestamps)
** Another time-stamp method
- *wound-wait*
  - Once again A wants a resource held by B
  - If A is older than B it takes the resource and B restarts
  - Otherwise A (the younger) waits for B to release the resource
  - Old processes never wait for anything. Age really has its privileges
  - Less restarts
  - In both cases processes keep their timestamps even when
    restarted. Eventually they are really old and will not have to restart
  - Either way lots of unnecessary restarts

** Distributed deadlock detection
- Each processor keeps track of the resource allocation grph to do with its
  local resources. May include remote processes. Ccycles don't just occur
  locally. Need to check the union of resource allocation allocation graphs

#+DOWNLOADED: screenshot @ 2021-05-12 17:41:16
[[file:images/Distributed_deadlock/2021-05-12_17-41-16_screenshot.png]]

** DDD (cont.)
- *Centralized deadlock detection process*
  - Information may have changed by the time the data from the last machine is
    gathered, the data from the first machine is probably out of date
  - Graph is only an approximation of the real allocation of resources and
    requests. If there is deadlock it will be detected, but it is prossible to
    detect deadlock when it doesn't exist
  - Timestamps can be used to avoid false deadlock detection

** Avoiding false cycle detection
- When process A at site 1, requests a resource from process B, at site 2, a
  request message with a timestamp is sent
- The edge A -> B with the timestamp is inserted in the graph of 2 only if 2 has
  received the request message and cannot immediately grant the requested
  resource
- The deadlock detection controller asks all sites for their wait-for graphs
- For requests between sites, the edge is inserted in the global graph if and
  only if it appears in more than one local graph (with the same timestamp)
** Dsibributed approach
- There is an extra node (P_ex) in each local wait-for graph
- All local processes waiting on any external processes point to P_ex
- Any local processes waited on by an external process are pointed to by P_ex
- If a cycle with P_ex invovled is found we have a possible deadlock and
  information is sent to the site waited on
- If a deadlock is then found then it is handled
- If another possible deadlock is found involving P_ex
  - Another message is sent to another site etc
- Until either deadlock is detected or there is no cycle

** Messages
- Passing messages can also be used to control concurrency
- Two (main) ways to send information from one process to another
  1. Shared resource
  2. Message passing
- _Message Passing_
  - Needs:
    - Some way of addressing the message
    - Some way of transporting the message
    - Some way of notifying the receiver that a message has arrived
    - Some way of accessing or depositing the message in the receiver
  - May look like:
    - =write(destination, message)=
    - =receive(source, message)=
  - OR
    - =write(message)=
    - =read(message)=
  - In this case we also need some way of making a connection between the
    process, like an open call

** Design decisions
- Should the sender block until the message is received?
- Should the receiver block until the message is received?
- What are advantages and disadvantages of blocking or not blocking?
- Blocking reader seems natural
- Blocking writer slows writer thread
  - But doesn't require message buffering
- Non-blocking writers might have to be blocked in some cases
- If both block we have synchronous communication: rendezvous
- Should communication by one way or two way?
- Client/server requires two way

** Design decisions
- Should the system have message "types". ie, the sender can specify the type of
  message it is sending and the receiver can specify the type of message it
  wants to receive
- =send(destination, type, data)=
- Should  the receiver be able to wait for more than one type simultaneously
- =message = receive(type1, type2, ...)=
- Some systems include extra conditions on message reception as well, normally
  known as well, normally known as "guards"

** Storing the message
- We want to minimise the amount of copying
- Move it straight from the sender to the reeivers's address space
- Pass a pointer (sender cannot alter until it is received)
- Buffer the message in the system
- If a fixed size - reject or block senders

#+DOWNLOADED: screenshot @ 2021-05-12 18:30:24
[[file:images/Distributed_deadlock/2021-05-12_18-30-24_screenshot.png]]
- *Message size*
  - Should there be a fixed size? (packet or page size)

** Direct communication
Process to process
- address: name or id of the other process
- one link between each pair of processes
- receiver doesn't have to know the id of the sender (it can receive it with the message)
- So a server can receiv from a group of processes
- _Disadvantages_
  - Can't easily change the names of processes. Could lead to mulitple programs
    needing to be changed

** Indirect communication
- *Mailbox or Ports*
  - Mailbox ownership
  - Owned by the system
    - Servives even without processes
  - Owned by a process
    - The one which created it - usually the process which can receive from it
    - The creator can pass on the ability to receive
    - mailbox is removed when the process finishes

#+DOWNLOADED: screenshot @ 2021-05-12 18:33:47
[[file:images/Distributed_deadlock/2021-05-12_18-33-47_screenshot.png]]

** UNIX process communication
- *Signals*
- =kill(pid, signalNumber)=
- Originally for sending events to the process because it had to stop

#+DOWNLOADED: screenshot @ 2021-05-12 18:42:00
[[file:images/Distributed_deadlock/2021-05-12_18-42-00_screenshot.png]]
- But processes can catch and handle signals with signal
  handlers. =signal(signalNumber, handler)=
- Can also ignore or do nothing. If you don't ignore or set a handler then
  getting a signal stops the process
- One signal can't be handled: 9 SIGKILL

** UNIX Pipes
- Data get put into the pipe and taken out the other end
  - Implies buffering mechanism
  - What size pipe?
  - What about concurrent use: can writes interleave? etc
- In UNIX it strats as a way for a process to talk to iteself
- 

* Memory
Question: What do virtual memory systems help us with? 
** Memory
If we define memory as a place where data/code is stored there are many levels
of memory
- Processor registers
- Primary (or main) memory
  - RAM
- Secondary memory
  - Slower and more permanent
  - Disks
- Tertiary memory
  - Archival
  - Removable
- Cache memory
  - One level of memory pretending to be another
    - Different levels of cache
      - At the processor
      - At devices
      - And at hosts in distributed systems
** Main memory
- All processes need main memory
- Instructions and data being worked on are brought into the processor from
  memory and sent back to memory
- Traditional view
  - Addresses are numbers: 0-maxMem
  - An address goes out of the processor on the address bus to the memory store
  - The value comes back on the data bus (read)

#+DOWNLOADED: screenshot @ 2021-05-17 10:17:05
[[file:images/Memory/2021-05-17_10-17-05_screenshot.png]]

Each segment is a byte (byte addressable)

** Address binding
- We can make the connection between code and data and their addresses at
  different time:
- Compile time
  - Need to know exactly where the avlue will be stored
  - Fixed addresses
  - Not very flexible
- Load time
  - Object (not to be confused with OO) modules need tables of offsets
  - eg: variable x is 24 bytes from the start of the model
  - Can also reference other modules - linking
  - More flexible but can't be changed after loading
  - Dynamic loading (don't load unless called)
  - Dynamic linking similar (useful with shared libraries) - may already be in
    memory if shared between processes needs OS help
- Run time
  - Mapping to final address maintained on the go by hardware
  - *Commonly this is used in conjunction with the above techniques*

#+DOWNLOADED: screenshot @ 2021-05-17 10:19:50
[[file:images/Memory/2021-05-17_10-19-50_screenshot.png]]

** Memory spaces
Even in simple systems we would like a way of protecting OS memory from our
program and enabling programs larger than memory to run
- _Split memory_
- Can protect with a single fence register
- Alternatively if the OS is in ROM its code and constant data is safe from
  overwriting

#+DOWNLOADED: screenshot @ 2021-05-17 10:20:57
[[file:images/Memory/2021-05-17_10-20-57_screenshot.png]]

** Dividing memory
- With relocating loaders we can have mulitple programs in memory simultaneously
 - We also want protection
- In the history section we saw sperate partitions and base and limit registers
- Both require contiguous memory
- The algorithm for base and limit registers is simple
- If the address is less than the base or greater than the base + limit -1 then
  we have an access violation
- The base and limit registers are loaded for each process
- We need a lot more memory, maybe more than we have.
  - We we could use overlays in each allocated area

#+DOWNLOADED: screenshot @ 2021-05-17 10:22:49
[[file:images/Memory/2021-05-17_10-22-49_screenshot.png]]

** Two different addresses
- If we change our base and limit system to produce the address by adding the
  base register (now called a relocation register) to each address we can make
  all processes start at address 0. So no need to translate addresses at load
  time
- *This is a huge conceptual change*
- We now have two types of addresses
  - The logical (virtual) address coming out of the process, produced by the CPU
  - And the physical (real) address used on the address bus to memory
  - We still have contiguous memory for each process but a process can now be
    positioned anywhere in physical memory without its addresses needing to be
    changed
  - We can even move a process around, just copy the memory to the new place and
    change the relation register
  - This is useful if we want to defragment memory to give on large free area
  - Have to be careful if moving data (eg from a device) into the process'
    memory space

** Split memory into smaller chunks
- This is another approach to the above slide
- Rather than moving memory around to make big enough spaces for processes we
  could have more than one section of physical memory accessed by the same
  process
- We need either a number of base limit registers or a table of the information

#+DOWNLOADED: screenshot @ 2021-05-17 10:26:30
[[file:images/Memory/2021-05-17_10-26-30_screenshot.png]]
where base is the starting address and has a limit (eg 1k) of bytes that are
allocated to the program, these are chunks. Your program are split across
different parts (called chunks)
- This is a pain because for example find address 2000, you have to go through
  the chunks

** Two approaches
This technique evolved in two directions
1. Same sized chunks (pages)
2. Variable sized chunks (segments)

- Both have advantages and disadvantages
- Both use memory management units (MMUs) in hardware to do the transation
  between the logical and physical address
- Rather than doing a tedious calculation (where is logical address 2000 on the
  previous slide?) to find what chunk and address is in, we just split the
  address into two parts
- Then the translation is much simpler and looks very similar to both pages and
  segmented systems
- Same size chunks are called pages, while variable sized chunks are called
  segments
** Paged system address translation
Logical address is divided into
- /Page number/: index into a /page table/ which contains base addresses of each
  page in physical memory
- /Page offset (displacement)/: added to base address to get the physical
  address

For example load address 1000. You're going to divide this by p where the
remainder will be the offset
- p is 20bits
- d is the displacement 20 bits
- f is the frame
#+DOWNLOADED: screenshot @ 2021-05-17 10:34:13
[[file:images/Memory/2021-05-17_10-34-13_screenshot.png]]

** x86 variable sized pages

#+DOWNLOADED: screenshot @ 2021-05-17 10:34:37
[[file:images/Memory/2021-05-17_10-34-37_screenshot.png]]



** Frames and pages
- The textbook makes a distinction between pages and frames
- A frame is a page sized chunk of physical memory that can be allocated to a
  page from a process
- A page is a frame sized chunk of logical memory that fits in a frame
- It is common to refer to both simply as pages (physical and logical)
- _Fragmentation_
  - No external fragmentation in any pages that are not cimpletely used
  - Could be an average of 1/2 a page per process area (or 1/2 a page per
    thread, 1/2 a page for heap cape, 1/2 a page for code, 1/2 a page for
    initalized data, etc)
  - Small pages save space but require larger page tables
  - In x64 machines it is possible to use page sizes of 1GB (but normall we work
    in 4KB pages)

** Tables
- Each process has its own page table
- And commonly the OS has a grame table with one entry for each fram showing
  whether it is free or not and which process is currently using it

#+DOWNLOADED: screenshot @ 2021-05-17 10:37:51
[[file:images/Memory/2021-05-17_10-37-51_screenshot.png]]

** Linux kernel memory
- Kernal memory is not paged out (more on paging soon)
- Kernel addresses are virtual at the top of memory but are always mapped to
  real memory starting at address zero
- There are special zones which need to be handled differently
  - In particular a DMA zone where devices can transfer data to and from emmory
    without going through the CPU
  - And a normal zone for most kernel memory

** Allocating kernel memory
- Some kernel memory must be in particuler real memory location: because of
  device access
  - Possibly multiple frames contiguous in real memory
- Other times we can use virtual memory
- We want it to be fast to allocate memory in the kernel
- There are two main allocators
  - The page allocator
  - The slab allocator

** Page allocator
- Ises the biddy algorithm

#+DOWNLOADED: screenshot @ 2021-05-17 10:40:39
[[file:images/Memory/2021-05-17_10-40-39_screenshot.png]]

** Buddy and Slab
- Disadvantages
  - If you want 65K you get 128K
  - End up with lots of little allocations
- The slab allocator is used for common data structures eg: file information,
  task information (pcb)
  - A slab allocator allocates data structures of the same size
  - Slabs are empty, partially full, or full
  - Request a file data structure from an empty of partially full slab
  - Return it when no longer needed: available for future allocation
  - Extra slabs are allocated when needed using the buddy algorithm

** Different sized chunks
- Rather than constant sized pages we could design our hardware to work with
  variable sized chunks: these are know as segments. (not to be confused with
  variable sized pages)
- _Memory model_
  - How memory appears to be organised to the program (and programmer) is
    sometimes referred to as the memory model
  - A segmented memory model is when we look at memory as a group of logical
    units all with addresses starting at zero
  - processes can have lots of segments
    - Functions
    - Global variables
    - Activation records
    - Large data structures
    - Objects

** Segments
- These logical units fit nicely into hardware specified segments where
  different amounts of memory can be allocated into one chunk
- Segments are contiguous blocks of memory
- We will get problems of external fragmentation
  - Our memory addresses become <segment name, displacement>
  - And the translation proces looks very much like paging, except there is a
    length associated with each segment. We have a segment descriptor table
    rather than a page table
  - The physical address is the rsult of an addition rather than a concatenation
    as it is in a paged system

** Example

#+DOWNLOADED: screenshot @ 2021-05-17 10:46:48
[[file:images/Memory/2021-05-17_10-46-48_screenshot.png]]

** Allocation strategies
- Segments have no internal fragmentation: we only allocate the amount of space
  we want
- But we get external fragmentation
- We have seen the allocation strategies before
  - First fit
  - Next fit (first fit but starting from where we were up to
  - Best fit
  - Worst fit
  - We can defragment memory if we want to find large enough chunks. Faster than
    defragmenting disks

** How much spaces in the holes
- Knuth's 50% rule
- If there are n segments there are n/2 holes
- Each segment is either below another segment or below a hold
- We always combine adjacent holes

#+DOWNLOADED: screenshot @ 2021-05-17 10:49:00
[[file:images/Memory/2021-05-17_10-49-00_screenshot.png]]
** Half speed memory
- In both paged and segmented memories every logical memory access requires (at
  least) two memory accesses. One for the page/segment table and one for the
  actual data
- Actually the number of segments may be quite small and there may be registers
  for them
- So the MMUs cache recent page table informatoin in a speical fast-lookup
  hardware cach called /associative registers/ or /translation look-aside
  buffers/ (TLBs)

#+DOWNLOADED: screenshot @ 2021-05-17 15:09:05
[[file:images/Memory/2021-05-17_15-09-05_screenshot.png]]

** TLB use

#+DOWNLOADED: screenshot @ 2021-05-17 15:09:18
[[file:images/Memory/2021-05-17_15-09-18_screenshot.png]]

** Average access times
- TLB Lookup = us
=======
When you have segments you could get gaps.

** Half speed memory
- In both pages and segmented memories every logical memory access requires (at
  least) two memory accesses. One for page/segment tables and one for the actual
  data (when you are reading at the offset)
  - What this means is that our computers will become half speed
  - What we do to mitigate this half speed thing is when we are doing a lookup
    we then store this recent page table information in a special fast lookup
    hardware cach called the translation lookasid buffer (TLB) what this does is
    it means taht we no longer have to go out to memory when doing a lookup on
    the same memory address
- Actually the number of segments may be quite small and there may be registers
  for them
- So the MMus cache recent page table information in a special fast-lookup
  hardware cache called /associative registers/ or /translation look-aside
  buffers/ (TLBs)

** TLB use

#+DOWNLOADED: screenshot @ 2021-05-18 09:24:46
[[file:images/Memory/2021-05-18_09-24-46_screenshot.png]]
We should also add another arrow if we TLB miss then when we do that lookup in
the normal page table then store that in the TLB (put the (p,f) values in the
cache).
- We should also be mindful that every time we are putting something in the TLB
  we have to evict something from the TLB. This is important because we have to
  decide what size we make our TLB

** Average access times

#+DOWNLOADED: screenshot @ 2021-05-18 09:26:45
[[file:images/Memory/2021-05-18_09-26-45_screenshot.png]]

- epsilon: the time to do a lookup in the TLB
- beta: lets say that you have the frame and the offset, memory cycle time is
  the amount of time going from this frame and offset to an actual value of
  memory
- hit ratio: the percentage of times that a page number is found in the TLB
- EAT = (looking in the TLB + accessing that address)*hit ratio + (looking in
  the TLB, missing and then having to do a lookup in the page table and then
  another in the actual real data) * (not hitting the TLB)

** TLB coverage
- TLB coverage (or reach) is the amount of the address space included in the TLB
  entries
  - def: Reach: the amount of memory (proportion) that is covered by TLB
- Typical TLB caches hold about 128 entries
- With 4K pages this is only half a megabytes of memory
- As working sets (more on those later) increase this means lots of processes
  have a real performance hit, memory wise
n- The solution is large page sizes. This means more internal fragmentation. More
  IO (in virtual memory systems)
  - This is because with larger page sizes this means that a process would need
    less pages, thus less page numbers would be required than if you were having
    smaller page sizes (which would be more page entries required) which would
    bloat the TLB
- Variable page sizes can be used but they need good allocation algorithms to be
  worthwhile

** Page table size
- Another program with page tables is their potential size
  - eg: 32bit address space and 4Kbyte pages (offset of 12 bits)
    - to be able to uniquely offset into all 4096 spaces you need 12 bits (2^12)
  - So 20 bits to index into the page table ~= a million entries (at least
    4Mbytes for each process and then 4 bits per entry?, depends on the PTE size)
  - Each process has a page table (in theory). So 4MB per process, lets say 100
    processes. This would mean that we would need 400MB just to keep the page
    tables!
- You can work out the equivalent for 64bit address spaces
- Most processes do not use all memory in the CPUs logical address space
  - because of this we could possibly shrink the size of our pages
- We would like to limit the page table to values that are valid
- Can do this with a page table length register
  - allocate enough of a page table for a process
- Can flag page table entries with a valid bit
- Only allocate the parts of the page table we actually need
- Page the page table (see virtual memory)

** Multi-level page tables

#+DOWNLOADED: screenshot @ 2021-05-19 19:11:14
[[file:images/Memory/2021-05-19_19-11-14_screenshot.png]]
- If we have a process that only uses 1 page. The page itself would have to be
  executable and writing (bad idea). Well anyways. In that case the minimum
  amount of space that you have to give it one page. If you have a size of 1024
  for page tables then if you have a process with 2048 pages then you would need
  two levels. You need a good TLB hit ratio because otherwise you're going to be
  doing a whole lot of lookups

** Inverted page tables
- As the number of address bits increase to 64 we need even more levels of page
  tables
- Another approach is to keep information about the physical pages (or frames)
  rather than all of the logical pages. This is known as inverted page
  tables. We share one page table across several processes!
  - Lets not talk about the pages and instead talk about the frames.
  - The problem is is 2 processes try to access virtual address 1000 then there
    would be 2 entries in the original version. but with this one we are tagging
    along the pid so we know which frame belongs to which pid
  - The address coming out of the CPU has extra bits for the PID. We have to
    search in the table for a pid and the page number (scan through the entire
    page table which is going to be slow)
- Only need one page table for all processes. Each entry needs to refer to the
  process that is using it and the logical address in that process
- A logical address is
  - <pid, page number, displacement>
- Have to search the page table for <pid, page number>. Use hashtable for the
  page table and rely on TLBs. This is faster than having to use an array which
  is slower
  - What happens if there is a conflict? We are going to have to use conflict
    resolution


#+DOWNLOADED: screenshot @ 2021-05-19 19:16:01
[[file:images/Memory/2021-05-19_19-16-01_screenshot.png]]

** Paging and Segmentation
- The MULTICS system solved problems of external fragmentation (where you would
  have space between segments) and lengthy
  search times by paging the segments
- You can kinda get the best of both worlds with this
- Different from pure segmentation in that the segment-table entry contains not
  the base address of the segment but rather the base address of a /page table/
  for this segment

#+DOWNLOADED: screenshot @ 2021-05-19 19:21:04
[[file:images/Memory/2021-05-19_19-21-04_screenshot.png]]
- Top bits are a segment number. Check if the displacement of the segment if
  greater than the size of the segment, if it isn't (good) then you look into
  the segment table which gives you the base address of the page table (page
  table for every segment). Take the displacement into the segment (displacement
  of the segment into the page)

** Programs larger than memory
- It has always been the case that no matter how much memory a computer system
  has there are programs that need more
  - Not because an individual program is too big but rather the combination of
    programs makes too much memory use
- THis was handled early on by overlays
  - Need memory A in part A of program, when going to part B we would unload
    memory A because we no longer need it in part B
- But these required care on the part of the programmer to split the program up
  into distinct sections
- Also any connection between the sections had to be carefully worked through
- It became even more of a problem with multiprogramming and several programs
  occupying memory
- Interestingly for person computers at home there is almost no problem anymore:
  memory is so cheap
- With multiprogramming we can swap entire process out of disk to provide space
  for others (and swap back in to run)
- The disk is known as backing store
- Must be able to hold all memory for all processes
- Swapping is slow: especially if done at every process context switch
- Does the process have to be swapped back in to the same memory space?
- Early UNIX used to swap. We still use variants of swapping

** Does it all have to be there?
- Overlays provide the hint
- We can execute programs without the entire program being in memory at once
- Can keep either pages or segments on disk when not needed
- The logical address space can be larger than the physical. We call these
  virtual and real address spaces when we have virtual memory
- This has many advantages
  - unused code doesn't waste physical memory
  - We have more memory for multiple processes
  - We don't need to load the whole program into memory at once
    - Hence speeding up responsiveness to commands
- Why does it work? because of this thing:

** Locality of reference
- In almost all programs if we look at their memory access over a short period
  of time (a window) we see that only a small amount of the programms address
  space is being sued
- Each memory access is very probably going to be near another recent memory
  reference
- True for both code and data
- But it is possible to write programs that don't do this. eg: arrays stored as
  rows accessed by columns
  - because you're accessing data that isn't "nearby"
- This is known as the principle of /locality of reference/
- Programs do not reference memory with a random distribution

** Paging

#+DOWNLOADED: screenshot @ 2021-05-19 19:47:42
[[file:images/Memory/2021-05-19_19-47-42_screenshot.png]]
- Remember how we had extra space in the address space? 21 bits used 11 bits
  unused. We should fill the rest of the address space with this
** Work for the OS
- So when a page is accessed the page table entry indicates whether the page is
  currently in real memory or whether it is in a paging file (or swap space) on
  disk
- The MMU happily takes care of the translation between logical addresses and
  physical addresses when all pages are in real memory
- If a page is not in real memory it is up to the memory management system to
  - Allocate real memory for the page
  - Move pages from disk into memory
  - Indicate when the page is now ready
- To do this several design decisions need to be made

** Moving pages into swap space
Different systems move pages into swap space at different times
- Allocate space for the entire process in swap space (this is usually allocated
  contiguously)
  - This slows down the startup time for processes
  - But it can speed up later operations
  - Possible complications as processes grow
- Allocate when the page is accessed the first time
  - Quick startup
  - All accessed pages have a copy in swap space (even if in real memory as well)
  - New accesses are slowed down
- Only allocate when a page is swapped out
  - Dont' use swap spaces at all unless necessary
- Only allocate space for changed data
  - Code, libraries and read-only data can have their virtual memory in the
    normal files (requires cooperation between paging system and file system
    uniform storage structures

** Demand paging
- Demand paging is concerned with when a page gets loaded into real memory
- When a process starts all of its memory can be allocated (and loaded)
  - If there is not enough real memory available it has to be taken away from
    pages currently used
  - If there is still not enough some has to go into swap space
  - Loaded a large program can have a severe penalty on other processes in the
    system (and the overall amount of work done)
- Demand paging only brings a page into real memory when thep age is used by the
  process
  - When a process runes it is allocated memory space but it all points to the
    swap space (or somewhere else)
  - Actually most demanding systems do load in the first few pages so that the
    program start without lots of page faults (not pure demand paging)

** Page faults
- If ever a memory access finds the valid bit of the page table entry not set we
  get a page fault
- The processor jumps to the page fault handling routine
- Checks if the page is allocated (if not we have a memory violation) (SEGFAULT)
- If allocated (but not in a frame)
  - Find a free frame (possibly create one)
  - Read the page from the swap space into the frame
  - Fix the page table entry to point to the frame
- If the page is shared then multiple entries must be mixed
- restart the instruction that caused the fault
  - Instructions must be restartable


#+DOWNLOADED: screenshot @ 2021-05-24 11:19:15
[[file:images/Memory/2021-05-24_11-19-15_screenshot.png]]
- The instruction was to load a particular memory address, if it's invalid then
  we go to the OS
- When it says reset page table I think that this means that you perform the
  eviction process of a frame


** The problems of one instruction
- Different architectures make the task of paging easier or harder
- Some architectures can access many pages with one instruction
- eg: add @x, @y -- you're going to the addresses x and y and getting their
  values
- This instruction could cause 6 memory accesses and in the worse case 10 pages
  faults
  - If EVERYTHING is stored at a page boundary and takes up two pages
- The requirement to restart is also a problem
- Autoincremement and autodecrememnt instruction operands need to be restored to
  their original values before restarting the instruction
  - This can be done with extra registers holding initial values or changes
- Block moves are also a problem: part way through the move there is a page
  fault
  - We don't want to restart the instruction
  - Can solve this by checking pages for validity beforehand or maintaining
    extra registers tracking progress

** Question

#+DOWNLOADED: screenshot @ 2021-05-24 11:26:56
[[file:images/Memory/2021-05-24_11-26-56_screenshot.png]]

* Effective Access Time

#+DOWNLOADED: screenshot @ 2021-05-24 11:34:43
[[file:images/Effective_Access_Time/2021-05-24_11-34-43_screenshot.png]]
- Our previous access time assumed that the frames where actually in memory but
  we didn't model for swap, where something isn't in a frame but is on disk and
  has to be loaded into memory
- Sometimes you dont' have to swap a page out when your get a page fault because
  there is space in the page table

** How often do we want page faults
With
- EAT ~= 20 + 6_000_000p nsecs
- If we want the EAT to be only half as slow again as real memory we need:
  - p = 10/6_000_000 = 0.0000016
- So one page fault every 600,000 memory accesses
- Even though the estimates were very approximate we see that we don't want page
  faults to happen very often
- With pages sizes of 4K-8K we need lots of frames or lots of repeated access
  to make the speed acceptable

** Reducing pages faults
- Different processes have different memory access patterns and therefore
  different numbers of pages they need to have in memory at one time
- There is a minimum number we must have. eg: with the add @x, @y instruction we
  saw earlier we need at least 10 frames for each process (otherwise the
  instruction may never complete).
- We can allocate frames equally or proportionally (depending on size or priority)
- We can set minimum and maximum numbers per process
- We really need the currently required pages in real memory

** Working sets
- We talked earlier of the notion of /locality of reference/
- The working set of a process is the collection of pages needed in real memory
  in order to keep the process running
- If we observe a process running over a short period of time (a window) we can
  record the page access the process amkes. This is a picture of the pages
  working set
- The trick is getting the window the right size
  - If it is too small not enough pages are included in the working set
  - If it is too big too many pages are included
- Approximate with interval time + a reference bit
- A reference bit is available in the page table entry in some architectures to
  indicate the page has been accessed (read or written) since it was cleared
- Example: window = 1000msec
- Time interrups after every 500msec
- Keep in memory 2 bits for each page
- Whenevery the time interrupts read and then set the values of all reference to
  0
- If one of the bits in memory = 1 -> put the page in the working set

** PFF
- We can also use the page fault frequence to control the number of frames
  allocated to a process
- As the number of frames increases the numberj of pages faults drops rapidly at
  first, then reaches a point where adding more frames hardly alters the rate at
  which paging occurs. We set upper and lower bounds and add or remove frames to
  stay within them

#+DOWNLOADED: screenshot @ 2021-05-24 11:46:25
[[file:images/Effective_Access_Time/2021-05-24_11-46-25_screenshot.png]]

** Choosing pages for replacement
- When there are no free frames to bring in a page the system has to pick one to
  replace
- There are two main ways of selecting frames for replacement
  - Global: any frame allocated to any process can be chosen
  - Local: chosen frames must come from the process own allocated frames
- There are different consequences for these
  - With the global method the number of frames for a process varies depending
    on its behaviour and the behaviour of the other process. (the same process
    can run with widely varying speed due to tother processes taking some of its
    frames)
  - With local scheme there are less frames to choose from
- Normally global replacement is chosen, but we might have a maximum number of
  frames for the process

** We still have to pick
- So of all currently occupied frames which one is chosen
- We have some preferences:
  - pages that are read-only or haven't been modfied don't have to be written
    back to disk (this aves on swapping time)
  - Page table entries commonly have a dirty-bit to indicate the frame has been
    changed since the page was loaded
  - pages that aren't going to be accessed again in the near future (so we don't
    end up with another page fault on the page we just moved out): unfortunately
    we can't see into the future so we rely on recent behaviour
- If we have a reference bit we might use this to get an approximation

** Selection algorithms
- Want lowest page fault rate
- Evaluate algorithm by running it on a particular string of memory references
  (reference string) and computing the number of page faults on that string
- In all our examples, the reference string is 
  1,2,3,4,1,2,5,1,2,3,4,5
- *Random*
  - It treats every process  fairly
  - Easy to implement
  - With  enough pages the method won't replace pages just about to be used too
    frequently

#+DOWNLOADED: screenshot @ 2021-05-24 17:32:02
[[file:images/Effective_Access_Time/2021-05-24_17-32-02_screenshot.png]]

** Selection algorithms
- FIFO: first in first out
  - Keep a list of pages in a queue
  - Remove the one at the head put new ones at the tail
  - simple
  - Very improtant pages (such as part of the operation system) which are
    referenced frequently will be paged out just as frequently as pages which
    are hardly ever referred to
  - Belady's anomaly: increasing the number of frames occasionally increases the
    number of page faults


#+DOWNLOADED: screenshot @ 2021-05-24 17:34:24
[[file:images/Effective_Access_Time/2021-05-24_17-34-24_screenshot.png]]

** Selection algorithms
- Least recently used: LRU
- Based on the assumption that a pgae not used recently will not be used in the
  near future
- Based on the assumption that a page not used recently will not be used in the
  near future
- In this example not as good as FIFO: generally better
- Why can't LRU suffer from Belady's anomaly?
  - Because what you're doing as you add extra frames means that you look back
    at the reference string now if you had an extra frame you would actually
    look further back into the past. Thus you're looking at a larger number of
    frames but your solution includes the decision of the fewer number of pages

#+DOWNLOADED: screenshot @ 2021-05-24 17:45:01
[[file:images/Effective_Access_Time/2021-05-24_17-45-01_screenshot.png]]
- Very expensive: need to have hardware that keeps track of last access tie for
  each page
- Or maintain a list of pages and move a page to the top of the list when
  accessed

** Approximations to LRU
- *Use the referenes bit: originally clear, set when the page is used*
- Keep regular track (additionally reference bits)
- Every 100 msecs (say) move the referenced bit into the high bit of a value
  (say 8 bits). shitfting all bits to the right and clear the referenced bit for
  every page. You're using this to check if a page has been used recently
- eg:
  - R: 1    referenced byte: 00011111
  - R: 0    referenced byte: 10001111
  - The pages with the lowest numbers have either been used the longest time ago
    (or not used as regularly)
  - Can select randomonly from lowest valued or use a FIFO strategy to choose
- *Second change (clock algorithm)*
  #+DOWNLOADED: screenshot @ 2021-05-25 14:45:17
  [[file:images/Effective_Access_Time/2021-05-25_14-45-17_screenshot.png]]
You keep a cycle of the pages, then your reference bit is checked. If it's 1
then you reset to 0 then continue, then if you come across a 0 then this is your
victim. It degenerates into a FIFO algorithm (what if they were all 0's). We are
giving you a second chance if it's been used recently

** Selection algorithms
- *Least frequently used: LFU*
  - Maintain a count of memory accesses for each page
  - Keep heavily used pages
  - Pages can only stay around after they are needed: can decrease the count
    over time
  - New pages get picked on
- You're only meant to count *FROM THE TIME THAT THE PAGE IS LOADED INTO MEMORY!!!*
  - So you're not counting from the start of the  thing, only from when it's
    been loaded in LAST
#+DOWNLOADED: screenshot @ 2021-05-24 17:58:25
[[file:images/Effective_Access_Time/2021-05-24_17-58-25_screenshot.png]]
- Also expensive adding to the count with every access
- If you have something that's been used a lot then is never used again this is
  bad because you would have something with a high count but then it's never
  used again so you're wasting a slot. In this case you can use aging. For every
  iteration that doesn't have an access you can age the count for that variable
  just a little bit
** More algorithms
- *Most Frequently Used: MFU*
  - Pages with very few accesses may have just been brought in to memory
  - Neither is commonly used
- *Death Row*
  - Put frames into a replacement pool according to FIFO selection but you're
    not actually paging them out. You're keeping them in memory. Then if you
    need something new then you would take from that page and page it out
  - Keep track of which page is in each frame
  - If a page is accessed while its frame is in the replacement pool then
    retrieve it. It's a soft page fault because it's in the replacement pool
  - There is no penalty for paging from disk in this situation

** Optimal
- If we can see the future, the optimal algorithm replaces a page which is going
  to be used furthest away in the future
- If a certain page isn't going to be used then you can chuck it out
- If there are multiple pages that are not going to be used then you do FIFO to
  select from those pages which one that you're going to chuck out
- If all pages are going to be used in the future then you would then throw out
  the one that is used furthest in the future
- If pages are not going to be used again the FIFO on those is suitable

#+DOWNLOADED: screenshot @ 2021-05-24 18:04:43
[[file:images/Effective_Access_Time/2021-05-24_18-04-43_screenshot.png]]
- Only problem is that we don't know the future very often...

** Windows VMM

#+DOWNLOADED: screenshot @ 2021-05-24 18:06:40
[[file:images/Effective_Access_Time/2021-05-24_18-06-40_screenshot.png]]
- UWP: universal windows
- Keeps a number of working sets for different archs
- Due to memory pressure from other applications then you have to trim your
  working sets a little bit. These are still kept in memory as a "standby
  list". If you have a fault you check the standby list then if you have the
  page in that standby list this is called a soft fault. This is quicker than a
  hard fault. If it's not in that standby list then you have to go to disk
  (which is slower)


** The VMM: virtual memory manager 
The VMM: virtual memory manager runs inthe background maintaining memory
policies
 - it keeps track track of the free list of frames and then zeroed list
   - The zerod list is tracked because if you're giving a frame to someone you
     want ot make sure it's been zeroed first
Processes have working set maximums and minimums
- The process is guaranteed its working-set minimum
- If the number of of frames allocated is bleow the maximum the systm will
  allocate it more frames (if it can)
- If there aren't enough free frames then working sets are trimmed to their
  minimum value
- Default working set size is 30: VMM occassionally steals pages to see if the
  Page is in the working set
Privileged processes can lock pages in real memory
- Useful fo real time processes and device drivers
  - Useful for device drivers because you want to lock pages in memory that are
    used by devices because otherwise you get page faults which is bad for a
    device because it makes them slow for something that you want to be fast
  - Real time: You have a time deadline. If you're busy page faulting then that
    would eat into your time and could make you lose your deadline (wasted compute)
Clustering: when a page is brought in the pages around it are also brought in
Windows prefetching
- When an application is started Windows observes the apges and files referenced
  in the first 10 seconds
- It keeps track of these and will load all such pages the next time the
  application is started
- It also defrags these files every few days

** Trashing
- Only occurs when you don't have enough RAM (frames) for your working sets
  (pages). If you get a page fault then you evict from the working set. This
  causes another page fault. You evict from working set again. Page fault. ETC
  to infinity. You're always getting page faults which makes your processes VERY
  slow
- If the sum of the nubmer of pages of the working sets of all processes in the
  system exceeds the number of frames we are in deep trouble
- This casuses thrashing
- Every pages fault causes a page from the working set of a process to be
  removed
- By definition the removed page is going to be accessed soon causing another
  page fault
- And so on
- It severely affects the amount of work that can be done
- Any process that falls below the number of pages in its working set should be
  suspended and swapped out (it is not going to get any work done anyway)
- *Solutions:*
  - If you detect thrashing then you can suspend some of the processes. Page
    them completely out and then allow the other processes to run
  - Or you get more memory
    
** Batch system thrashing 
- If a batch system is set up to increase the number of programs running in the
  system at a time if the CPU utilisation gets too small we can get thrashing
  very easily.

#+DOWNLOADED: screenshot @ 2021-05-24 18:43:33
[[file:images/Effective_Access_Time/2021-05-24_18-43-33_screenshot.png]]

** The location of a process' memory
- Addressable memory in a UNIX or Windows type process is scattered in different
  places

#+DOWNLOADED: screenshot @ 2021-05-24 18:44:13
[[file:images/Effective_Access_Time/2021-05-24_18-44-13_screenshot.png]]

* Security and Protection
- Protection: the *mechanism* for controlling access to resources for programs,
  processes and users
- Subjects: the active components in a system that can use resources (users,
  programes, processes). Also referred to as principals
- Objects: the resources being used (programs, processes, files, memory,
  communication channels, devices, databases, semaphores)
- Objects can also be subjects
- We will assume that we have authenticated the subjects, so what we are
  concerned with here is how to ensure subjects only access objects in permitted
  ways. *Authorization*
- We will look at *authentication* in the security section

** Goals
- Protect against
  - Malicious intent
  - stupidity
  - accident
  - errors
- Each object in the system has a number of operations that can be performed on
  it. Not only do we not want any other access than the permitted operations we
  would also like to limit access to the minimum required to achieve the allowed
  goals: the *need to know* principle
- All accesses to objects should be mediated by a *reference monitor*

#+DOWNLOADED: screenshot @ 2021-05-25 15:26:05
[[file:images/Security_and_Protection/2021-05-25_15-26-05_screenshot.png]]
- reference monitor: gives an answer to the question: are you allowed to do this
  on that?

** Examples of protection
- We have already seen several examples of protection in this course
  - Privileged instructions: the process must be executing in kernel mode in
    order to execute without causing an exception
  - Memory protection: the kernel address space is protected from user level
    instructions> Similarly one process' address space is protected from access
    by another
  - File system: one user's files are protected from access by another user
- What is the reference monitor in each of these cases, how could it work?
  - For privilleged instructions: This is done by hardware. Oh you're in
    usermode. You can't execute this!
  - For memory protection: hardware: There are hard limits on what you can write
    to. You can't write out of bounds! If you want more you have to do through a
    sys call

** Protection Domains
- Access rights are commonly associated with protection domains
- A process executes inside a protection domain. The process then has the rights
  and privileges of the domain
- Thus many process can have the same rights if they execute in the same domain
- There are too many subjects, objects and access rights in a normal system to
  explicity keep information about all of them
- So this combining is the first attempt to decrease the amount of protection
  information which the system needs to maintain
- A domain is a collection of ordered pairs
  - <object, rights>

** Intersection of domains
- Domains can overlap
- In this case the permission in the overlap is available to both

#+DOWNLOADED: screenshot @ 2021-05-25 15:35:16
[[file:images/Security_and_Protection/2021-05-25_15-35-16_screenshot.png]]

- Our programs frequently have to move from one domain to another
- This switching can only be allowed if the start domain has the permission to
  change to another domain
- Domains can be associated with users, locations (eg: URLS), programs,
  processes

** Crossing domains
- Crossing domains is dangerous and is commonly used to attack systems
- Why do we need it?
  - We want users to have controlled access to resources they don't have direct
    access to
  - eg: a database, particular hardware, networks
- So we give the user access to a program that does have access to the
  restricted resource
- The user's domain allows access to the program, the program's domain allows
  access to the resource
- UNIX
  - Domains are associated with users (and the groups they belong to)
  - When a program is run it take on the permissions of the user (both
    individual and group permissions)
  - We can set programs to take on the permissions of the group or owner of the
    program file instead. Because if jenny owns program A then bob accesses
    program A then we don't want bob to change program A because jenny owns it!
  - The program because a *setuid* or *setgid* program

** How to setuid
#+DOWNLOADED: screenshot @ 2021-05-25 15:44:11
[[file:images/Security_and_Protection/2021-05-25_15-44-11_screenshot.png]]
- After these changes anyone (not in my group) can run the /program/ file
- When they do so, the process uses my permissions. If it want' setuid then the
  process would have run with their permissions
- This is really dangerous, espeically if I am the superuser. If they can start
  another process from within the /program/ process that new process would have
  my permissions as well

** setuid precautions
- Restrict the uid
  - Don't use "root"; if necessary make a new user for the program
- Reset the uid before calling exec
  - Otherwise the new program will run with the privileges of the parent program
- Or any call that might call exec
- Close unnecessary files before calling exec
  - Because if a file was open in the parent then it will be open in the child
    when you're calling fork(), you have to remove those open files from the PCB
    of the child otherwise it does have access to them (unlikely that you would like access to them in the child). THEN call exec()
    - If a privileged file was open it would still be accessible
- If the program must be setuid "root" then use a restricted root directory eg: chroot("/usr/hi")
  - Then only files beneath /usr/hi can be reached
- Invoke subprograms using their full pathnames
  - If the path gets altered it may invoke another program (but the privileges remain)

** Multics ring structure

#+DOWNLOADED: screenshot @ 2021-05-26 10:17:09
[[file:images/Security_and_Protection/2021-05-26_10-17-09_screenshot.png]]

** Multics segments
- Each file is loaded as a segment. It has associated permissions: read, write,
  execute: and a ring number (the ring it runs in or is loaded into)
- Access to other segments depends on both the current ring number, the ring
  number of the other segment and the type of access required
  - If you have a lower ring number you can do things to higher ring number. You
    can't go the other way (ring 2 using a resource to ring 1. No you can't do
    this, you have to make a system call)
- The current ring number is maintained when a lower permission ring is entered
  by a process
  - Because you have to get back to your current ring
- Sometimes a lower permission segment needs to access a segment in a higher
  permission ring
- There are specified entry points which allow this: more access is allowed
  under controlled conditions (reference monitor checks this allowance)

** Other approaches to domain switches
- Special directories: programs in these directories run with the access
  privileges associated with the directory
  - This is safer than setuid programs because all privileged process must be in
    these directories rather than scattered all around the system

- Have server processes running with the necessary privileges the normal user
  processes send messages to the server process when they need the privileged
  access
  - server can grant/deny
  - Of course a system call is a change of domain and the hardware guarantees that
    when the call returns the domain reveres to its previous status
- All such techniques require great care

** Access Matrix
- Rows represent domains
- Columns represent objects
- =Access(i,j)= is the set of operations that a process executing in Domain_i
  can invoke on Object_j

#+DOWNLOADED: screenshot @ 2021-05-26 10:21:42
[[file:images/Security_and_Protection/2021-05-26_10-21-42_screenshot.png]]

** Changing permissions
- When an object is created a new column is added and the permissions are set
  (usually be the creator/owner), owners are allowed to do whatever the fuck
  they want to their own files
- The domains are objects as well
- This way we can control access to the domains
  - This gives us power to control the power that one domain has over another
    domain
- Transfer to another domain: switch eg: a process executing in D2 can switch to
  D3 or D4

#+DOWNLOADED: screenshot @ 2021-05-26 10:22:48
[[file:images/Security_and_Protection/2021-05-26_10-22-48_screenshot.png]]
- Eg D1 can switch to D2 then print on the laser printer

** Changing in a column
- Copy right "*" signifies the permission can be copied. Can only work on the
  same object/column. You can pass it on. Eg D2 with its  write* in F3, it could
  pass it on to another domain (for the same object) eg to D3. You don't give
  the other domain the permission to pass it on so you remove the asterisk (*)
- Owner right means any values on the object/column can be changed


#+DOWNLOADED: screenshot @ 2021-05-26 10:23:35
[[file:images/Security_and_Protection/2021-05-26_10-23-35_screenshot.png]]
- Before and after

** Changing in a row

#+DOWNLOADED: screenshot @ 2021-05-26 10:23:48
[[file:images/Security_and_Protection/2021-05-26_10-23-48_screenshot.png]]
If you're in D2 you can remove any permissions in D4 (you can't add them)

** Implementing and access matrix
- There is too much information required in an access matrix
- We have cells for every combination of domains and objects
- We could implement it as a sparse matrix but most OSs use one of two possible
  representations (and sometimes a mixture of both)
- Only hold information on the rows: each row corresponds to the access rights
  of a domain over all objects it can use. If the domain has no rights over and
  object no information is stored. This approach is known as /capability lists/
- Only hold information on the columns: each column represents the access rights
  held over this object. No information is stored about the domains that have no
  access. This approach is known as /access lists/
  - These are the access rights that some domains have over the object

** Confused Deputy Problem
- This is connected with the changing domains problem from last lecture
- A deputy is a process with special access rights to objects. eg: the Unix
  =passwd= program has access to the password files
- Another process has access to the deputy: eg: you can change your own password
- The process asks the deputy to do something for it: eg change someone elses
  password
- The deputy can do this but should it?
  - NO!!
- If the deputy doesn't check we have a security hole
  - Capabilities remove this hold

** Capabilities
- A capability is a permission to access an object in certain ways
- Capabilities are stored with domains. They always refer to the object and the
  access rights
  - Remember that these are the rows of our access matrix so the following makes
    sense
  - How do we keep people from forging their own capabilities? The creation of
    these capabilities should be out of your control. This should be protected
    for the OS to determine
- So a domain has a capability list
  - < f1, "read, write">
  - < f2, "execute">
  - < d2, "control">
- When a process needs to access a protected resource the capability is passed
  with the request. The capability is checked by the reference monitor to make
  sure the access is permitted
- we need to ensure that a process executing in a domain cannot freely change
  any of its capabilities
- The capability list is itself a protected object

** Keeping capabilities safe
- How do we stop a process altering its capabilities?
  - In particular we don't want a process making new capabilities without strict
    controls
  - So capabilities should not be forgeable
- We can use hardware or encryption
- On a single machine we can store all capabilities in protected kernel memory
  (so that the user level processes cannot directly access them or add to them)
- Tagged architectures: some machines have extra bits for every word in
  memory. Thus integers can be distinguished from strings from floats from
  capabilities. Only the OS is allowed to create or change capabilities
  - capabilities would be tagged in memory because of the tags. Then only the
    hardware would be allowed to change the addresses tagged with capabilities
- With distributed systems we need to encrypt capabilities. With a public key
  anyone can check them but the cannot be created

** Use of capabilities
- Regardless of how they are protected: capabilities must only be created by the
  OS
- When a new object is created the OS should construct an owner capability for
  the Creator process
- Usually the owner can pass the capabiltiy on to other processes (or domains)
  - Like a secure password: Secure because the capability cannot be forged or
    altered
- We must also ensure that capabilties can't be snooped off the network
- Capabilities are now widely used: many microkernel OSs (like Mach) use
  capabilities
- Kerberos tickets are capabilities

** Problems with capabilities
- It is difficult to determine which domains have access rights to a given
  object. From an object point of view it doesn't maintain a view of which
  domains have access over it. This information isn't tracked in this
  direction.
  - Not only are the capabilities associated with domains but they can also be
    passed on: keeping track of them is a problem
- Similarly because it is difficult to find references to all domains with
  particular capabilities it is difficult to revoke access permissions.
- Revoking capabilities
  - Keep track of all domains with the capability (made difficult if they have
    the capability to pass on capabilities)
  - indirection: the capability is not really to the object but to an
    intermediate object which points to the object (we change the pointer to
    null). All domains with the old capability no longer have access
  - reacquisition: have an expiry time and then request the capability
    again. (In this case we need another way of determining if the capability
    should be given)

** Access Control Lists
- Each object has a list of domains and their access rights. This list is an ACL
- An access control list (ACL) could look like:
  - <d1, "read, write">
  - <d2, "execute">
  - <d3, "read, write">
- When a requestet comes to the object form a specific domain for a partiular
  access the reference monitor checks the ACL to see if access is allowed
- ACLs don't have any revocation problems. THE ACL just gets changed


** ACLS vs NTFS ACLS
#+DOWNLOADED: screenshot @ 2021-05-31 11:08:40
[[file:images/Security_and_Protection/2021-05-31_11-08-40_screenshot.png]]

** Checking your ACLS

#+DOWNLOADED: screenshot @ 2021-05-31 11:09:22
[[file:images/Security_and_Protection/2021-05-31_11-09-22_screenshot.png]]
** Problems with ACLs
- They slow down file serach operations: all ACLs (which can be long) have to be
  checked ofr all directories
  - AFS imposes a limit,  commonly set to 20
- They are considreed by some as unnecessarily complex
- Ironically, for all the flexibility that ACLs offer, they have proven to be
  confusing and difficult to understand, and the extra functionality they
  provide is dwarfed by the feeling of dread which they instill in
  administrators and users alike
- Many prefer the simplicity of UNIX file protection mechanism

** Reducing information
- We hstill have the problem with lots of permission information in our systems
- Both ACL and capability systemscan have default access to objects to reduce
  the amount of necessary information
- ACLs (with heirarchial objects, such as files) can inherit permissions from
  objects above them
- A capability to a directory could give you access to all files in the
  directory
  - This is less flexible. What if we want to put a hidden file in the directory
    which we don't want accessed?

** UNix permissions
*** TODO

** Adding levels to Unix like systems
- The OS is divided into sepatate domains
  - Kernel
  - System and user applications
    - Applications are separate from each other
  - Each domain only has the minimum permissions (on files, sockets,
    directories) it needs to do its job
  - Each domain has limited access to system calls and file types (eg: httpd_t)
  - Use a struct file typing system
  - There is no global superuser
    - Each domain has its own administrator and the administrator of one domain
      has no power over another
  - So how is global administration done?
    - The system is restarted with a different administration kernel (without
      any connections to networks)

** Domains & Types
- The chcking of access is built in to the kernel and cannot be circumvented

#+DOWNLOADED: screenshot @ 2021-05-31 11:16:16
[[file:images/Security_and_Protection/2021-05-31_11-16-16_screenshot.png]]

** SELinux & AppArmor
- Security Enhanced Linux: AppArmor similar
- Mandatory Access Control (MAC) to all objects represented by the file system
  (including processes)
- Extended attributes associated with each file (device etc). Stored in the
  inode
- Policies are represented by policy files
- Policies are set of rules governing things such as the roles a user has access
  to; which roles can enter which domains and which domains can access which
  types
- Particular roles can be associated with servers. This way if one role gets
  compromised it only affects its domains and types
- Normal Unix permission bits are checked first and then SELinux checks

** Modern Processors and Oss
- Cache memory
- Checks on memory access: in particular kernel memory
- Partial execution and out of order execution
- Undo effects of execution if access is not allowed
- Most OSs map the same kernel space into all processes
- Lots of memory mapped into kernel space
- And everything works as specified so we are safe right?
  - Reace condition
  - Side channel timing attack

** The Exploit

#+DOWNLOADED: screenshot @ 2021-05-31 11:45:19
[[file:images/Security_and_Protection/2021-05-31_11-45-19_screenshot.png]]

** The timing attack

#+DOWNLOADED: screenshot @ 2021-05-31 11:45:36
[[file:images/Security_and_Protection/2021-05-31_11-45-36_screenshot.png]]
** That is bad enough

#+DOWNLOADED: screenshot @ 2021-05-31 11:45:48
[[file:images/Security_and_Protection/2021-05-31_11-45-48_screenshot.png]]

** Kernel Page Table Isolation

#+DOWNLOADED: screenshot @ 2021-05-31 11:46:03
[[file:images/Security_and_Protection/2021-05-31_11-46-03_screenshot.png]]

* Key pairs
- A little bit of information on methods of cryptography. Capabilities use
  cryptography
- Information is encoded using a key
- It is decoded using either the same or another key
- Symmetric algorithms: same key, must be kept secret
- Asymmetric algorithms use different keys for encrypting and decrypting (one
  key can be public). The idea is that algorithm the keys are mathematically
  related it is /impossible/ to produce one from the other

#+DOWNLOADED: screenshot @ 2021-05-31 16:15:04
[[file:images/Key_pairs/2021-05-31_16-15-04_screenshot.png]]

** Public key use
We can use public key algorithms in two ways
1. Make the encoding key public: then anyone can encrypt messages but only the
   holder of the private key can decrypt. Useful for talking to a site without
   interception
2. Make the decoding key public - then anyone can decrypt the messages but only
   the holder of the private key can encrypt them. Useful for proving the
   message came from the holder of the Private key. Capabilities can be of this
   form

** Digital Signing
- We need a way of proving that messages come from who they say they are and
  haven't been altered on the way
- The sender puts the message through a hash algorithm to produce a message
  digest (eg: SHA-256)
- The encrypts the digest with its secret key. This is the digital signature for
  this message
- Sends the message and the signature
- The recipient uses the sender's public key to decrypt the signature
- Also calculates the message digest on the message (SHA-256). We are no longer
  using SHA-1
- Checks the local digest value with the one sent
- If they match, the message was sent by the proper subject and way not modified
  on the way

** Sharing keys
There are problems with distributing keys
- If the key is public the receiver still needs a way of checking that they key
  is authentic
- If the key is private we need to ensure that only the right domains get the
  key
Solutions to checking key authenticity
- the key can be signed and we have the key to check the authenticity of the
  signature: a trust chain
- Trusted thrid parties: can certify that they key (or certificate) came from
  the correct source
- Contact the source and check some /fingerprint/

** Sharing secret keys
- There are methods to cooperatively form secret keys during sessions, eg: the
  diffie-Hellman protocol
- The parties then share a secret key but they don't know who the other party is
- This can be solved but requires knowing signature verification keys
- For OSs the usual approach is to use the server that holds secret keys for all
  domains. Then we can use the Needham-Schroeder Protocol

** Diffie-Hellman Protocol
- A and B are the parties that want to communicate securely. They need a shared
  scret key
- There are two public values (one is a large prime p and the other is related
  to the prime (primitive root module p))
- Both parties generate a random private key. A produces key 'a' and B produces key'b'
- From these keys and the two original public values, both can produce public
  values they transmit to each other
- A shared secret key can then be produced by combining the public key each gets
  from the other with their private keys
- This shared key cannot easily be broken, with the public values

** Diffie-Hellman Protocol

#+DOWNLOADED: screenshot @ 2021-05-31 16:25:38
[[file:images/Key_pairs/2021-05-31_16-25-38_screenshot.png]]

Example:
#+DOWNLOADED: screenshot @ 2021-05-31 16:25:53
[[file:images/Key_pairs/2021-05-31_16-25-53_screenshot.png]]

** Certification
- Diffie Hellman is susceptible to Man-in-the-middle attacks
- So identies need to be proved (MQV (Menezes-Qu_Vanstone) is based on
  Diffie-Hellman but uses pre-existing public keys of both parties to include
  authentication)
- _Certification Authorities_
- A want to prove its identity to B

#+DOWNLOADED: screenshot @ 2021-05-31 16:29:00
[[file:images/Key_pairs/2021-05-31_16-29-00_screenshot.png]]
1. A makes a request to the certification authority for a signed
   certificate. Includes its names and public key (A's name and public key)
2. CA signes the message with its private key to produce a signature (the
   message and signature now the certificate)
3. A sends the certificate to B
4. B checks certificate using CA's public key

** Needham-Schroeder Protocol
- But if we have a server with everyone's secret keys, we can use the
  Needham-Schroeder symmetric key protocol


#+DOWNLOADED: screenshot @ 2021-06-01 19:24:36
[[file:images/Key_pairs/2021-06-01_19-24-36_screenshot.png]]
So S contains A's private and B's private. This is a symmetric protocol
(symettric key pairs for A and B)

- A and B are the parties that want to communicate securely
- S is the server with secret keys for A and B
  - So A can communicate securely with S and B can communicate securely with S
- A tells S it wants to talk to B
- S gives A a key for A and B to talk K_AB. It also includes a verifying message
  for B using B's secret key. The message contains K_AB and A's identity
- A sends the verifying message to B. Remember that A can't change this
  message. It can only send it to B
- B checks the message (which includes A's identity and the key K_AB)
- This algorithm is extended by Kerberos (and hence AFS)

Downside: S contains all secret keys and it starts the conversation with
everyone. If anything ever goes wrong with S we are fucked!

** How things go wrong
The Three "c"s of security failure
- Change
- Complacency
- Convenience
*** Change
- Odd numbered versions of software (and Oss) commonly fix security errors
- The Mad Hacker (retrofitty security)
<<<<<<< HEAD
  - 
* Passwords (only partially)
** Single sign-on
- With distributed environments we don't want people to have to sign-on to every
  machine they use during a session
- enter a password at the computer
- Enter a password to use the network
- Enter a password to access a server
- Enter a password to use a database
- Enter a password to open a table in the database
- We can use a single sign-on service: which remembers our passwords and
  supplies them to the systems that need them
- We must ensure we don't store passwords in cleartext andt hat they aren't
  transmitted in cleartext

** Kerberos
- Uses tickets granted by central security server
- Principals: users and servers
- Kerberos Authentication Server (KAS) checks principals at login and issues
  tickets for ticket granting servers
- Ticket Granting Server (TGS): issues tickets to network services
- Tickets are time expiring capabilities
  - Much like capabilities
- We want two principles (A & B) to mutually authenticate themselves
- Based on the Needham_Schroeder protocol But A needs to authenticate itself to
  a TGS on the way to getting the service it wants from B

** Kerberos authentication

#+DOWNLOADED: screenshot @ 2021-06-02 11:08:00
[[file:images/Passwords_(only_partially)/2021-06-02_11-08-00_screenshot.png]]

1. A asks KA for a session key (for private communication between A & TGS and
   ticket to TGS.
2. KA returns Ticket_{a, tgs} (includes A's ID, network address, valid period
   and K_{a, tgs}) along with the key K_{a,tgs} for A and TGS to use. (All
   encrypted with A's secret key)
   - Ticket_{a,tgs} is encrypted with TGS's secret key, this stops A (or any one
     else) modifying it
=======
  - ICL's VME/B information on file was owned by :SD
  - Added security levels. So :SD didn't own classified file information
  - TO restor from backup a new user:SD/CLASS was added to handle this
  - To stop anyone logging in as as :SD/CLASS given an empty password by
    patching the password file
  - The wrong field was patched and caused the :SD/CLASS user to have unlimited
    access
  - Could only log on as :SD/CLASS from the master console, but if the master
    console was turned off the next device to open a connection became the
    master console
*** Complacency
- Bounds checking
- =fingerd=: process running to handle /finger/ requests
  - Used =gets= library routine to get input into a buffer
  - No length check
  - If you know the architecture and OS you can overflow the allocated stack
    space for the buffer and leave a return address to an exec method call on
    the stack
  - A buffer overflow attack
- The same osrt of thing is possible with many OSs and there are thousands of
  cases of unchecked buffers causig secruity problems under all varieties of
  Windows and Unixes
- VMS login: users could specify the machine they wanted to access
  - =username/DEVICE=<machine>=
  - The length of =<machine>= wasn't checked. The user's privilege mask was on
    the stack following the buffer. So you could overwrite the privilege mask to
    provide any desired privileges.
- So you're being complacent about user input here

#+DOWNLOADED: screenshot @ 2021-06-01 19:39:07
[[file:images/Key_pairs/2021-06-01_19-39-07_screenshot.png]]
So you're overwriting a return address to your function. Cool
- How do we prevent this:
  - Don't make the stack executable, only make it readable and writable
  - Return to libc attack:
    - You find sections of code that do some of the instructions that we want to
      perform. overwrite the return address. Push gadgets to stack which are
      popped which go to executable areas of code
  - Don't allow for infinite input into =gets=
  - Detect stack smashing

** Preventing Stack overflow attacks
- Buffer length checks before storing data into the buffer/stack
- Canaries: place a random value before the return address. If this value is
  modified then the stack has been compromised
- Data Execution Protection (DEP): use the memory management system to not allow
  execution of locations on the stack
  - Return oriented programming can get around this (gadgets)
  - So the OS randomises the location of our code (ASLR: Address Space Layout
    Randomisation), making it harder to work out where code "gadgets" are

** Authentication
- Even the best security system can be compromised if an intruder can
  sucessfully impersonate a legitimate user
- There also need to be policies that stop users /sharing/ their identities with
  others
- From the OS point of view we need a awy of allowing authenticated users access
  to the system but no one else
- We can use
  - possessions
  - Attributes
  - Knowledge
- and combinations of these

** Possessions
- keys or cards
- Locks can be picked and smart cards can be analysed
- Attackers (if they have access to a card) use techniques such as manipulating
  the power supply or clock to get secret information
  - The hope is that the card will get into an unkown state and produce
    informatoin it shouldn't
  - A type of fuzzing: provide random, invalid, unexpected data and observe what
    happens. Used to find bugs and secrutiy problems
- Or a side channel to extract information
  - Even just observing how long it takes to perform computations can be used to
    cut down the possibilities when trying to guess a secret key
- Of course keys and cards an be easily stolen as well

** Attributes
- Physical characteristics of the user
  - palm prints
  - finger prints
  - iris patterns
  - retina patterns
  - voice print
- Also the way things are done
  - typing patterns (different people type different sequences of characters at
    different speeds)
  - signatures: we include the speeds, directions and pressure of different
    strokes
- All of these bio metric methods can sufferer from false positives and false
  negatives
- The probabilities of each can be altered by changing parameters. Best to use a
  combination of techniques
>>>>>>> 86f3ab4a7fc4554da09c599109218474d9938943

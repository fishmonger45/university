
#+TITLE: 351
#+AUTHOR: Andreas
* Introduction
Everything that we have done so far is stored in files. What is wrong with this?
- Redundancies and inconsistencies
- Data access (who/what can access it)
- Data isolation (how do you isolate users from other peoples data)
- Integrity problems (can't make sure data stays the same though reads)
- Atomicity of updates
- Concurrent access by multiple users
- Security
** Definitions
- Database: A collection of related data
- Data: Known facts that can be recorded and have an implicit meaning
- Mini-world: Some part of the real world about which data is stored in a
  database. For example: student grades and transcripts at a university
- Database Management System (DBMS): A software system to facilitate the creation
  and maintenance of a database
- Database System: The DBMS software together with the data itself
** Database Simple environment
#+DOWNLOADED: screenshot @ 2021-03-02 13:36:22
#+attr_org: :width 400px
[[file:images/Introduction/2021-03-02_13-36-22_screenshot.png]]
** DBMS Functionality
- Define a particular database in terms of its data types, structures and
  constraints
- Construct or load the initial database contents on a secondary storage medium
- Manipulating the database (retrieval, modification)
- Processing and Sharing by a set of concurrent users and application programs
  while also keeping all the data valid and consistent
** Example of Database
- Miniworld = Part of a UNIVERSITY environment
- Some of the miniworld /entities/:
  - STUDENTS
  - COURSES
  - SELECTIONS
  - DEPARTMENTS
  - INSTRUCTORS
- Some miniworld relationships:
  - SECTIONS are of a /specific/ COURSE
  - STUDENTs take SECTIONs
  - COURSEs have prerequisite COURSEs
  - INSTRUCTORs teach SECTIONs
  - COURSEs are offered by DEPARTMENTs
  - STUDENTs major in DEPARTMENTs
- Note the above entities/relationships are typically expressed in a conceptual
  data model such as the entity-relationship (ER) data model
* Characteristics of Database Approach
- *Self-describing nature of a database system*
  - A DBMS *catalog* stores the description of a particular database (data
    structures, types and constraints) called *meta-data*
  - Meta-data allows DBMS software to worked with different database applications
- *Insulation between programs and data*:
  - Called *program-data independence*
  - Can modify the data structures and organisation without having to modify the
    DBMS
- *Data Abstraction*
  - A *data model* is used to hide details of storage and present users with a
    *conceptual* view of the database
  - Programs refer to these constructs rather than the actual full data storage
    details
- *Support multiple views of data*:
  - Each user may see a different view of the database, which describes *only*
    the data of interest to that user
- *Sharing of data and multi-user transaction processing*:
  - Allowing a set of concurrent users to read/write database
  - Concurrency control within DBMS guarantees each *transaction* is only either
    executed or aborted
  - /Recovery/ subsystem ensures each completed transaction has its effect
    permanently recorded in the database
  - *OLTP* (online transaction processing) is a major part of database
    applications allowing hundreds of concurrent transactions to execute per
    second
* Database users
This may be divided into two groups roughly. The whole thing is called *Actors
on the Scene*
- Database admins
- Database designers
- End users
- System analysts / Application programmers and business analysts
- *Those who design/manage DBMS are called WOrkers behind the Scene*
* Advantages of Using Databases
- Controlling redundancy in stores and reducing development and maintenance efforts
- Restricting data to certain users. Access permissions
- Providing storage structures (eg indexes) for efficient Query Processing
- Providing backup and recovery services
- Providing multiple interfaces to different classes of users
- Easy representation of complex relationships between data
- Enforcing integrity constraints on the database
* When not to use a DBMS
- Cost
  - High initial investment (skill + hardware)
  - Task overhead for increased functionality
- When a DBMS may be unnecessary:
  - If the database and applications are simple, well defined and are not
    expected to change
  - When access by multiple users is not required
- When a DBMS is infeasible
  - embedded systems/ limited resources (hardware + skill/people)
* Data Models
** Definitions
- *Data Model*: A set of concepts to describe the *structure* of a database, the
  *operations* for manipulating these structures and certain *constraints* that the
  database should obey.
- *Data Model Structure and Constraints*:
  - Constructs are used to define the database structure
  - Constructs typically include *elements* (and their *data types*) as well as
    groups of elements (e.g *entity, record, table*) and *relationships* among
    such groups
  - Constraints specify some restrictions on valid data; these constraints must
    be enforced at all times
- *Data Model Operations*:
  - These operations are used for specifying database /retrievals/ and /updates/
    by referring to the constructs of the data model. Operations on a database
    refer to the constructs of the data model.
  - Operations of the data model may include *basic model operations* (insert,
    delete, update) and *user-defined operations* (eg compute_student_gpa, etc)
** Categories of Data Models
*** Conceptual (high-level, semantic) data models:
- Provide concepts that are close to the way many users perceive data
*** Physical (low-level, internal) data models:
- Provide concepts that describe details of how data is stored in the
  computer. These are usually specified in an ad-hoc manner through DBMS design
  and administration manuals
*** Implementation (representational) data models:
- Provide concepts that fall between the Conceptual and Physical, used by many
  commercial DBMS implementations
*** Self-Describing Data Models:
- Combine the description of data with the data values. Examples include XML,
  key-value
** Database Schema vs Instances
The key distinction is that the *database schema* changes very infrequently,
while the *database state* changes every time that the database is updated.

Database Schema: Is the description of a database, this includes the database
structure, database types, and the constraints on the database. Schema also have
a form as a *illustrative* display of most aspects of the database schema. If
you want to refer to a part of the schema (eg STUDENT, COURSE) this is called a
*Schema Construct*

The database state is the actual data stored in the database at a particular
moment in time. The also includes the collection of all the data in the database
(also called an instance or snapshot). There are two states the database can be
in (or three I guess). The initial database state where the database is
initially loaded into the system and a valid state that satisfied the structure
and constrains of the database.
** Example of a database schema
#+DOWNLOADED: screenshot @ 2021-03-05 09:40:34
#+attr_org: :width 400px
[[file:images/Data_Models/2021-03-05_09-40-34_screenshot.png]]
** Three-Schema Architecture
DMBS uses three schemas to help support
1. Program-data independence
2. Support multiple views of the data
The three levels are:
1. *Internal Schema* at the internal level to describe physical storage
   structures and access paths (eg: indexes) and typically uses a *physical*
   data model
2. *Conceptual schema* at the conceptual level to describe the structure and
   constraints for the whole database for a community of users (uses a
   *conceptual* or an *implementation* data model
3. *External schemas* at the external level to describe the various user views
   (usually uses the same data model as the conceptual schema)

#+DOWNLOADED: screenshot @ 2021-03-05 09:54:40
#+attr_org: :width 400px
[[file:images/Data_Models/2021-03-05_09-54-40_screenshot.png]]

In order to translate these schemas between levels we need *mappings* which are
used to transform requests and data. Programs refer to an external schema and
are *mapped* by the DBMS to the internal schema for execution. Meaning that data
extracted from the internal DBMS level is reformatted (mapped) to meet the users
external view. An example of this is showing the results of an SQL query in a
webpage. Often if one layer schema changes is it *just the mappings* that
change to rebind the schemas properly.

* Database design process
** Introduction
There are two main activities in designing databases
- Database design
- Applications desgin (for preprocessing the data to eventually go into the
  database
We are going to fcous on the conceptual database design here (ie design a
schema) whereas applications design focuses on the programs and interfaces that
access the database

#+DOWNLOADED: screenshot @ 2021-03-08 10:17:19
[[file:images/Database_design_process/2021-03-08_10-17-19_screenshot.png]]

** Methodologies for Conceptual Design
- Entity relationship (ER) diagrams become pretty useful for a high level
  overview of the framework of the database
** ER Model Concepts
*** Entities and attributes
- Entity is a basic concept for the ER model. Entities are specific things or
  object in the mini-world that are represented in the database. We use entities
  to help us understand the database
  - For example the EMPLOYEE John Smith, the Research DEPARTMENT, the PoductX
    PROJECT
- Attributes are properties used to describe an entity.
  - We use attributes of entities to further scope out the structure of the
    database
  - For example an EMPLOYEE entity may have the attributes Name, SSN, Address,
    Sex, BirthDate
- A specific entity will have a value of each of its attributes
- Each attribute has a /value/ set (or data type) associated with it (int,
  string and so forth)
** Types of attributes
*** Derived Attribute
- Whose value is dynamic and derived from another attribute. It is represented
  by a dashed oval in ER diagrams
- Example: Person age is a derived attribute as it changes over time. It is
  derived from DOB
#+DOWNLOADED: screenshot @ 2021-03-08 10:29:38
[[file:images/Database_design_process/2021-03-08_10-29-38_screenshot.png]]
*** Composite/Multi-valued attributes
- Generally composite and multi-valued attributes may be nested to any number of
  levels but this is pretty rare
- Example: PreviousDegress of a STUDENT is a composite multi-valued attribute
  denoted by {PreviousDegress (College, Year, Degree, Field)}
- Another example

#+DOWNLOADED: screenshot @ 2021-03-08 10:38:02
[[file:images/Database_design_process/2021-03-08_10-38-02_screenshot.png]]

*** Key Attributes
- Entities with the same basic attributes are grouped or typed into an entity
  type
  - For example, the entity type EMPLOYEE and PROJECT
- An attribute of an entity type of which each entity must have a unique value
  is called a key attribute of the entity type
  - Eg SSN of EMPLOYEE where SSN has to be unique.
  - An entity can have /multiple/ key attributes
  - A key attribute may be a composite
  - In a ER diagram each key is underlined

*** Entity Set
- Each entity type will have a collection of entities stored in the database
  - Eg: CAR entity instrances
  - This is called the *entity set* or sometimes the *entity collection*
  - Entity set is the current /state/ of the entities of that type that are
    stored in the database
#+DOWNLOADED: screenshot @ 2021-03-08 17:12:57
[[file:images/Database_design_process/2021-03-08_17-12-57_screenshot.png]]
*** Value Sets (Domains) of Attributes
- Each simple attribute is associated with a value set
  - Eg: Lastname has a value which is a character string of up to 15 characters
  - Eg: Dat has a value consisting of MM-DD-YYY where each letter is an integer
- A *value set* specifies the set of values associate with an attribute which is
  very similar to data types in most programming languages (eg integer,
  character)
- This is basically an extra constraint on attributes
*** Attributes and Value Sets
- Mathematically an attribute A for an entity type E whose value set is V is
  defined as the function
- A : E -> P(V)
- We refer to the value of atrribute A for entity e as A(e)
*** Notation for ER diagrams
#+DOWNLOADED: screenshot @ 2021-03-08 17:20:31
[[file:images/Database_design_process/2021-03-08_17-20-31_screenshot.png]]
Example: Entity type CAR with two keys and a corresponding Entity Set
#+DOWNLOADED: screenshot @ 2021-03-08 17:21:20
[[file:images/Database_design_process/2021-03-08_17-21-20_screenshot.png]]
** Relationships
The initial design is not complete. Some aspects in the requirements will be
represented as *relationships*
A relationship relates two or more distinct entities with a specific
meaning. For example EMPLOYEE John Smith /works/ on the ProductX
PROJECT. Relationsihps of the same type are grouped or typed into a
*relationship type*. The degree of a relationships type is the number of
participating entity types. Usually we will deal with binary relationships.
*** Example

#+DOWNLOADED: screenshot @ 2021-03-10 16:11:40
[[file:images/Database_design_process/2021-03-10_16-11-40_screenshot.png]]

#+DOWNLOADED: screenshot @ 2021-03-10 16:12:01
[[file:images/Database_design_process/2021-03-10_16-12-01_screenshot.png]]

*** Relationship type vs relationship set
The relationship type is the schema description of the relationship (/WORKS/
/EATS/) and identifies the relationship name and participating entity types
while also identifying the relationship constraints

whereas a *relationship set* is the current set of relationship instances
represented in the database (aka the current /state/ of a relationship type.
In ER diagrams we represent the relationship type as follows:
- Diamond shaped box is used to display a relationship type
- Connected to the participating entity types via straight lines
- Note that the relationship type is not shown with an arrow. The name should be
  typically be readable form left to right and top to bottom

*** Discussion on Relationship Types
In the refined design some attributes from the inital entity types are refined
into relationships
- Manger of DEPARTMENT -> MANAGES
- Works_on of EMPLOYEE -> WORKS_ON
- Department of EMPLOYEE -> WORKS_FOR
So in general you should be careful when you are making your initial design of
the database such that when you have created all the attributes and then created
all the relationships for the description. You should then double check that you
are not double parking an attribute with a relationship. If there are conflicts
you should remove the attribute and keep the relationship (or whatever suits the
situation)
In general, more than one relationship type can exist between the same
participating entity types
- MANAGES and WORKS_FOR are distinct relationship types between EMPLOYEE and
  DEPARTMENT for example. The relationships just have different meaning for the
  same entity
*** Constraints on Relationship Types
Constraints are also known as ratio constraints. There is Cardinality Ratio
which specifies /maximum/ participation:
 - 1:1
 - One to many (1:N) or Many to one (N:1)
 - Many to many (M:N)
Existence Dependency Constraint (specifies /minimum/ participation) (also called
participation constraint):
- zero (optional participation, not existence-dependent)
- one or more (mandatory participation, existence-dependent)

**** Many to one
#+DOWNLOADED: screenshot @ 2021-03-10 21:51:08
[[file:images/Database_design_process/2021-03-10_21-51-08_screenshot.png]]
This example you can see that there are M people per department but a person can
only work for one department
**** Many to Many relationship

#+DOWNLOADED: screenshot @ 2021-03-10 21:52:23
[[file:images/Database_design_process/2021-03-10_21-52-23_screenshot.png]]

There can be multiple people per project and a person can be working on multiple
projects

*** Recursive Relationship Type
- A relationship type between the same participating entity type in *distinct roles*
- Also called a *self-referencing* relationship type
Example: The SUPERVISION relationship
- EMPLOYEE participate twice in two distinct roles:
  - supervisor role
  - supervisee role
- Each relationship instance relates two distance EMPLOYEE entities:
  - One employee in supervisor role
  - One employee in supervisee role

**** Displaying a recursive relationsihp
- In a recursive relationship type:
  - Both participation's are same entity type in different roles
In the following figure, first role participation labeled with 1 and the second
role participation labeled with 2. In ER diagram we need to display role names
to distinguish participation

#+DOWNLOADED: screenshot @ 2021-03-10 22:05:45
[[file:images/Database_design_process/2021-03-10_22-05-45_screenshot.png]]

*** Weak Entity Types
- An entity that does not have a key attribute and that is identification
  dependent on another entity type
- Entites are identified by the combination of:
  - A partial key of the weak entity type, which is denoted by underlined with a
    dashed line
  - The particular entity they are related to in the identifying relationship
    type
- Example:
  - A DEPENDENT entity is identified by the dependents first name /and/ the
    specific EMPLOYEE with whom the dependent is related
  - Name of DEPENDENT is the /partial key/
  - DEPENDENT is a /weak entity type/
  - EMPLOYEE is its identifying entity type via the identifying relationship
    type DEPENDENT_OF
*** Attributes of Relationship types
A relationship type can have attributes:
- For example HoursPerWeek of WORKS_ON
- Its value for each realtionship instance describes the number of hours per
  that an EMPLOYEE works on a PROJECT
  - A value of HoursPerWeek depends on a particular (employee, proect)
    combination
- Most relationship attributes are used with M:N relationships
  - In 1:N relationships, they can be transferred to the entity type on the
    N-side of the relationship to make it just a normal relationship without an
    attached attribute

#+DOWNLOADED: screenshot @ 2021-03-11 17:32:07
[[file:images/Database_design_process/2021-03-11_17-32-07_screenshot.png]]

*** Notation for Constraints on Relationships
- Cardinality ratio (of a binary relationship): 1:1, 1:N, N:1, or M:N which is
  shown by placing appropriate numbers on the relatinship edges
- Participation constraint (on each participating entity type): total (called
  existence dependency) or partial
  - Total participation is shown by a *double line*, partial is denoted by a
    single line
- A total participation of an entity set represents that each entity is the
  entity set must have at least one relationship in a relationship set.
- For example: Each college must have atleast one associated Student

#+DOWNLOADED: screenshot @ 2021-03-11 17:39:04
[[file:images/Database_design_process/2021-03-11_17-39-04_screenshot.png]]

*** Alternative (min, max) notation for relationship structural constraints:
- Specified on each participation of an entity type E in a relationship type R
- Specifies that each entity e in E participates in atleast /min/ and at most
  /max/ relationship instances in R
- Default (no constraint): min=0, max=n (signifying no limit)
- must ahve min<= max, min>=0, max >=1
- These limits are derived from the knowledge of mini-world constraints

Examples:
- A department has exactly one manager and an employee can manage at most one
  department
  - Specify (0,1) for participation of EMPLOYEE in MANAGES
  - Specify (1,1) for participation of DEPARTMENT in MANAGES
So each employee can manage 0 or 1 departments and there much be a manager for a
department

#+DOWNLOADED: screenshot @ 2021-03-11 18:45:53
[[file:images/Database_design_process/2021-03-11_18-45-53_screenshot.png]]

Another:
- An employee can work for exactly one department by a department can have any
  number of employees

#+DOWNLOADED: screenshot @ 2021-03-11 18:47:15
[[file:images/Database_design_process/2021-03-11_18-47-15_screenshot.png]]

READ THE MIN MAX NUMBERS NEXT TO THE ENTITY TYPE AND LOOKING AWAY FROM THE
ENTITY TYPE
Here is a summary of the ER diagram notation

#+DOWNLOADED: screenshot @ 2021-03-12 09:06:18
[[file:images/Database_design_process/2021-03-12_09-06-18_screenshot.png]]

*** Alternative diagrammatic notation
- ER diagrams are popular but there are alternatives where other notations exist
- *UML class diagrams* is representative of another way of displaying ER
  concepts that is used in several commercial design tools
Most of the notation can be deciphered for the example below:

#+DOWNLOADED: screenshot @ 2021-03-12 09:08:42
[[file:images/Database_design_process/2021-03-12_09-08-42_screenshot.png]]

*** Relationships of higher degrees
- degree 2 relationships are binary
- There exist higher degree relationships such as ternary and can go up to any
  number of entities involved
- In general an n-ary relationsihp is not equivalent to n binary relationships
- Constraints are harder to specify for higher degree relationships (n > 2) than
  for binary relationships

* Relational Model Concepts
** Informal Definitions
The *Relational Model* of data is based on the concept of a /Relation/. Where
the strength of the relational approach to data management comes from the formal
foundation provided by the theory of relations
A /Relation/ is a mathematical concept based on the idea of sets. Informally a
/relation/ will look like a *table of values*. A relation typically contains a
*set of rows*

The data elements in each *row* represent certain facts that correspond to and
actual state of the database (a rel world entity or relationship). In the
actual mode the rows are tuples. Each column has a column header that gives an
indication of the meaning of the data items in that column. This is called an
*attribute* in relational

#+DOWNLOADED: screenshot @ 2021-03-12 09:39:37
[[file:images/Relational_Model_Concepts/2021-03-12_09-39-37_screenshot.png]]

- Key of a relation
  - Each row has a value of a data time (or set of items) that uniquly
    identifies that row in the table (called the /key/)
  - In the STUDENT table, SSN is the key
- Sometime row-ids or sequential numbers are assigned as keys to identify the
  rows in a table which is called the /artificial/ key or /surrogate key/

** Formal Definitions
*** Schema
The *Schema* of a Relation:
- Denoted by R(A_1, A_2, ..., A_n)
- R is the *name* of the relation
- The attributes of the relation are A_1, A_2, ..., A_n
For example: CUSTOMER(Cust-id, Cust-name, Address, phone#)
- Customer is the relation name
- with 4 attributes
- Each attribute has a *domain* (set of valid values) that is defined by it's
  type and any other additional constraints that you put on the attribute (eg:
  Cust-id *must* be a 6 digit number)
*** Tuple
A tuple is an ordered set of values that are enclosed in angle brackets <>
For example a row in the CUSTOMER relation would be:
<345345, "John", "123 fake street", 0201234235>
This set of tuples constitutes the rows of the database
- A relation is a *set of tuples*
*** Domain
A domain is a set of rules that must be enforced for a given attribute. For
example date attributes might enforce a particular format (yyyy-mm-dd). The data
type of the attribute (which is tied into the domain obviously) also constrains
the set of allowed values
- The attribute name designates the role played by a domain in a relation
  - Used to interpret the meaning of the datat elements corresponding to that
    attribute
  - Example: The domain Date may be used to define two attributes named
    "invoice-date" and "payment-date" with different meanings
*** State
The *relation state* is a subset of the *Cartesian product of the domains of
its attributes*. Where each domain contains the set of all possible values the
attribute can take
- Example: attribute Cust-name is defined over the domain of the character of
  strings of maximum length 25
  - dom(Cust-name) is =varchar(25)=
*** Characteristics of Relations
- Notation
  - We refer to component values of a tuple t by t[Ai] or t.Ai
  - This is the value vi of the attribute Ai for tuple t
- Similarly t[Au, Av, ..., Aw] refers to the sub-tuple of t containing the
  values of attributes Au, Av, ... Aw
** Constraints
- Constrains determine which values are permissible and which are not in the
  database. They are of three main types
  - *Inherent or Implicit Constraints*: These are based on the data model
    itself. (Eg: relational model does not allow a list as a value for any
    attribute)
  - *Schema-based or Explicit Constraints*: They are expressed in the schema by
    using the facilities provided by the model. (Eg max carnality ratio
    constraint in the ER model)
  - *Application based or semantic constraints*: These are beyond the expressive
    power of the model and must be specified and enforced by the application
    programs
*** Relational Integrity Constraints
- Constraints are *conditions* that must hold on *all* valid relation states.
- There are three /main/ types of (explicit schema based) constraints that can
  be expressed in the relational model:
  - Key constraints
  - Entity integrity constraints
  - Referential integrity constraints
- Another schema based constraint is the *domain* constraint which we have
  already covered (every value in a tuple must be from its attribute domain) or
  it can be *NULL* if that *is allowed*
**** Key Constraints
A *Superkey* of R is:
- A set of attributes SK of R with the following condition:
  - No two tuples in any valid relation state r(R) will have the same value for
    SK
  - That is, for any distinc tuples t1 and t2 in r(R), t1[SK] != t2[SK]
A *Key* of R:
- Is a *minimal* superkey
- That is, a key is a superkey K such that removal of any attribute from K
  results in a set of attributes that is not a superkey (does not possess the
  superkey unique property)
- A key is a superkey but not the other way around
***** Example
Consider the CAR relation schema:
- CAR(State, Reg#, SerialNo, Make, Model, Year)
- CAR has two keys:
  - Key1 = {State, Reg#}
  - Key2 = {SerialNo}
  - Both are also superkeys of CAR
  - {SerialNo, Make} is a superkey but /not/ a key
In general:
- Any /key/ is a /superkey/ not not the other way around
- Any set of attributes that /includes a key/ is a superkey
- A /minimal/ superkey is also a key
***** Candidate keys
- If a relation has several *canditate keys*, one is chosen arbitrarily to be
  the *primary key*
- The primary key value is used to /uniquely identify/ each tuple in a relation
  (provides the tuple identity)
- Also used to /reference/ the tuple from another tuple
  - General rule: Chose as primary key the smallest of the candidate keys (in
    terms of size)
**** Entity Integrity
- Entity Integrity:
  - The /primary key/ attributes PK of each relation schema R in S cannot have
    null values in any tuple of r(R)
    - This is because primary key values are used to /identify/ the individual
      tuples
    - t[PK] != null for any tuple t in r(R)
    - If PK has several attributes, null is not allowed in any of those
      attributes
  - Other attributes of R may be constrained to disallow null values even
    though they are not members of the primary key
* Mapping ER to Relational Algorithm
We want to map out ER diagrams to an actual database. To do that we have a
predefined algorithm with the following wants from the algorithm
- Preserve all information (that includes all attributes)
- Maintain the constraints to the extent possible. (Relational model cannot
  preserve all constraints: eg max cardinality ratio such as 1:10 in an ER
  relationship type)
- Minimize null values
** Step 1: Mapping of Regular Entity Types
- For each regular (strong) entity type E in the ER diagram, create a relation R
  that includes all the simple attributes of E
- Composite attributes are represented by their ungrouped components
- Chose one of the key attributes of E as the primary key for R
- If the chosen key of E is composite the set of simple attributes that form it
  will together form the primary key of R
- Example: We create the relations EMPLOYEE, DEPARTMENT and PROJECT in the
  relational schema corresponding to the regular entities in the ER diagram
  - SSN DNUMBER, and PNUMBER are the primary keys for the relations EMPLOYEE,
    DEPARTMENT, and PROJECT

#+DOWNLOADED: screenshot @ 2021-03-15 14:06:14
[[file:images/Mapping_ER_to_Relational_Algorithm/2021-03-15_14-06-14_screenshot.png]]
- The most important thing here is that you're only choosing one key from the
  candidate keys

** Step 2: Mapping of Weak entity types
- For each weak entity type W in the ER diagram with owner entity type E, create
  a relation R & include all simple attributes (or simple components of
  composite attributes) of W as attributes of R
- Also include as foreign key attributes of R the primary key attributes(s) of
  the relations(s) that correspond to the owner entity types(s)
- The primary key of R is the /combination/ of the primary key(s) of the
  owner(s) and the partial key of the weak entity type W, if any

Example:
- Include the primary key SSN of the EMPLOYEE relation as a foreign key
  attribute of DEPENDENT (renamed to ESSN)
- The primary key of the DEPENDENT relation is the combination {ESSN,
  DEPENDENT_NAME} because DEPENDENT_NAME is the partial key of DEPENDENT

#+DOWNLOADED: screenshot @ 2021-03-15 14:14:08
[[file:images/Mapping_ER_to_Relational_Algorithm/2021-03-15_14-14-08_screenshot.png]]

** Step 3: Mapping of Binary 1:1 Relation Types
- For each binary 1:1 relationship type R in the ER diagram, identify the
  relations S and T that correspond to the entity types participatin gin R
- There are three possible approaches:
  1. *Foreign Key (2 relations) approach*: Choose one of the relations (eg S)
     and include a foreign key in S the primary key of T. It is better to chose
     an entity type with total participation in R in the role of S
Example: 1:1 relation MANAGES is mapped by choosing the participating entity
type DEPARTMENT to serve in the role of S, because its participation in the
MANAGES relationship type is total 

#+DOWNLOADED: screenshot @ 2021-03-15 14:17:13
[[file:images/Mapping_ER_to_Relational_Algorithm/2021-03-15_14-17-13_screenshot.png]]

  2. *Merged relation (1 relation) option*: An alternate mapping of a 1:1
     relationship type is possible by mergin the two enitty types and the
     relationship into a single relation. This may be appropriate when both
     participation's are total.
  3. *Cross reference or relationship relation (3 relations) option*: The third
     alternative is to set up a third relation R for the propose of cross
     referencing the primary keys of the two relations S and T representing the
     entity types

** Step 4: Mapping of Binary 1:N relationship types
- *The foreign key approach*:
  - For each regular binary 1:N relationship type R, identify the relation S
    that represent that participating entity type at the N side of the
    relationship type
  - Include as a foreign key in S the primary key of the relation T that
    represents the other entity type participating in R
  - Include any simple attributes of the 1:N relation type as attributes of S
- *The relationship relation approach*: An alternative approach uses a
  relationship relation (cross referecing relation) option as the third option
  for binary 1:1 relationships
Example: 1:N relationship types WORKS_FOR, CONTROLS and SUPERVISION in the
figure
- For WORKS_FOR we include the primary key DNUMBER of the DEPARTMENT relation as
  foreign key in the EMPLOYEE relation and call in DNO

#+DOWNLOADED: screenshot @ 2021-03-15 14:27:33
[[file:images/Mapping_ER_to_Relational_Algorithm/2021-03-15_14-27-33_screenshot.png]]

** Step 5: Mapping of Binary M:N Relationship Types
- *The relationship relation (cross-reference) option*:
  - For each regular binary M:N relationship type R, /create/ a new relatin S to
    represent R. This is a /relationship relation.
  - Include as foreign key attributes in S the primary keys of the relations
    that represent the participating entity types; /their combination wil form
    the primary key/ of S
  - Also include any simple attributes of the M:N relationship type (or simple
    components of composite attributes) as attributes of S
Example : The M:N relationship type WORKS_ON from the ER diagram is mapped by
creating a relation WORKS_ON in the relational database schema
- The primary keys of the PROJECT and EMPLOYEE relations are included as foreign
  keys in WORKS_ON and renamed PNO and ESSN respectvely
- Attribute HOURS in WORKS_ON represents the HOURS attribute of the relations
  type. The primary key of the WORKS_ON relation is the combination of the
  foreign key attributes {ESSN, PNO}

#+DOWNLOADED: screenshot @ 2021-03-15 14:50:25
[[file:images/Mapping_ER_to_Relational_Algorithm/2021-03-15_14-50-25_screenshot.png]]

** Step 6: Mapping of Multivalued attributes
- For each multivalued attribute A, create a new relation R
- This relation R will include an attribute corresponding to A, plus the primary
  key attribute K-as a foreign key in R-of the relation that represents the
  entity type of relationship tpe that has A as an attribute
- The primary key or R is the combination of A and K, if the multivalued
  attribute is compsoite, we include its simple components
Example: The relation DEPT_LOCATIONS is created
- The attribute DLOCATION represents the multivalued attribute LOCATIONS or
  DEPARTMENT while DNUMBER-as foreign key represents the primary key of the
  DEPARTMENT relation
- The primary key of R is the combination of {DNUMBER, DLOCATION}

#+DOWNLOADED: screenshot @ 2021-03-15 14:54:57
[[file:images/Mapping_ER_to_Relational_Algorithm/2021-03-15_14-54-57_screenshot.png]]

** Step 7: Mapping of Nary relationshpi types
Example:

#+DOWNLOADED: screenshot @ 2021-03-15 14:55:47
[[file:images/Mapping_ER_to_Relational_Algorithm/2021-03-15_14-55-47_screenshot.png]]
should be turned into the following

#+DOWNLOADED: screenshot @ 2021-03-15 14:56:15
[[file:images/Mapping_ER_to_Relational_Algorithm/2021-03-15_14-56-15_screenshot.png]]

** Final result
#+DOWNLOADED: screenshot @ 2021-03-15 14:56:37
[[file:images/Mapping_ER_to_Relational_Algorithm/2021-03-15_14-56-37_screenshot.png]]

** Summary

#+DOWNLOADED: screenshot @ 2021-03-15 14:56:52
[[file:images/Mapping_ER_to_Relational_Algorithm/2021-03-15_14-56-52_screenshot.png]]

* SQL
- Considered one of the major reasons for the commerical success of relational
  databases
- The original of SQL is relational predicate calculus called tuple calculus
- SQL = structured query language
- SQL is an informal or practical rendering of the *relational data model* with syntax
** Terminology
- *Table*, *row*, and *column* used for relational model terms /relation/,
  /tuple/ and /attribute/
- =CREATE= statement
  - Main SQL command for data definition
    - To create schemas, tables (relations), types and domains
    - And other constructs, eg: views, assertions and triggers
- The language has features for: Data definition, data manipulation, transaction
  control, indexing, security specification, active databases, multimedia,
  distributed databases,etc
- *SQL schema*
  - Idnetified by a *schema name*
  - Includes an *authorization identifier* and *descriptors* for each element
- *Schema elements* include
  - Tables, types, constraints, views, domains, and other constructs (such as
    authorizations grants)

#+DOWNLOADED: screenshot @ 2021-03-18 10:58:02
[[file:images/SQL/2021-03-18_10-58-02_screenshot.png]]
- Each statement in SQL ends with a *semicolon*
- The privilege to create schemas, tables and other constructs must be
  explicitly granted to the relevant user accounts by the system administrator
  or DBA
** TODO Put the notes from box1 in here! (split between computers)
** Catalog Concepts
Catalog is a number collection of schemas in an SQL environment
- A database installation typically have a default environment and schema
- A catalog always contains a special schema called *INFORMATION_SCHEMA*, which
  provides information on all schemas in the catalog and all the element
  descriptors in these schemas
- Schemas within the same catalog can share certain elements, such as type and
  domain definitions
- SQL also has the concept of a cluster of catalogs
** Create table command
- Specifying a new relation
  - Provide name of the table
  - Specify attributes, their types and inital constraints
- The attributes are specified:
  - Each attribute given a name, a datatype to specify its domain of values
  - and possible attribute constraints such as NOT NULL
- The key, enttity integrity and referential integrity constraints can be
  specified with the CREATE_TABLE statement after the attributes are declared
  - Or specified later using ALTER TABLE
- Can attach the schema name to the relation name
  -CREATE TABLE COMPANY.EMPLOYEE ...
or
- CREATE TABLE EMPLOYEE
- *Base tables (base relations)*
  - Relation and its tuples are actually created and soterd as a file by the
    DBMS
- *Virtual relations (views)*
  - Created though the CREATE_VIEW statement. May or may not correspond to an
    actually physical file

Here is a schema that we have been following in class a lot...

#+DOWNLOADED: screenshot @ 2021-03-20 19:46:15
[[file:images/SQL/2021-03-20_19-46-15_screenshot.png]]

And here is one of the possible database states for the COMPANY relational
database

#+DOWNLOADED: screenshot @ 2021-03-20 19:46:48
[[file:images/SQL/2021-03-20_19-46-48_screenshot.png]]

#+DOWNLOADED: screenshot @ 2021-03-20 19:47:03
[[file:images/SQL/2021-03-20_19-47-03_screenshot.png]]

You can create these tables in SQL using the following commands
#+DOWNLOADED: screenshot @ 2021-03-20 19:47:39
[[file:images/SQL/2021-03-20_19-47-39_screenshot.png]]

#+DOWNLOADED: screenshot @ 2021-03-20 19:47:50
[[file:images/SQL/2021-03-20_19-47-50_screenshot.png]]

- In the previous example, some foreign keys might cause errors:
  - Specified either via:
    - Circular references
    - Or because they refere to a table that has not yet been created
  - BDA's have ways to stop referential integrity enforcement to get around this
    problem
  - These constraints can be left out of the initial CREATE TABLE statement, and
    then added later using the ALTER TABLE statement
** Attribute Data Types and Domains in SQL
- Basic data types
  - *Numberic data types*
    - Integer numbers: INTEGER, INT, and SMALLINT
    - Floating point (real nubmers) =FLOAT=, or =REAL= and =DOUBLE PRECISION=
    - Formatted numbers: =DECIMAL(i, j)= or =DEC(i,j)=, or =NUMBERIC(i,j)=
      - =i= stands for /Precision/, the total number of digits in the value (on
        both sides of the decimal point)
      - =j= stands for /Scale/, number of digits after the decimal point
  - *Boolean data type*
    - Values of =TRUE= or =FALSE= (or =NULL=)
  - *Character-string data types*
    - Fixed length: =CHAR(n), CHARACTER(n)=
    - Varying length: =VARCHAR(n), CHAR VARYING(n), CHARACTER VARYING(n)=
  - *Bit-string data types*
    - Fixed length: =BIT(n)=
    - Varying length: =BIT VARYING(n)=
  - *Date and Time data types*
    - DATE has 10 positions, made of YEAR, MONTH, and DAY in the form YYYY-MM-DD
    - TIME has at least 8 positions, make of HOUR, MINUTE, and SECOND in the
      form HH:MM:SS
  - *additional data types*
    - *Timestamp* data type: includes the DATE and TIME fields
      - Plus a minimum of six positions for decimal fractions of seconds
      - Optional WITH TIME ZONE qualifier
  - *INTERVAL data type*
    - Specifies a relative value that can be used to increment or decrement an
      absolute value of a date, time, or timestamp
  - *DATE, TIME, Timestamp INTERVAL data types* can be *cast* or converted to
    string formats for comparison
** Attribute Data Types and Domains in SQL
- *Domain*:
  - Name used with the attribute specification
  - makes is easier to change the data type for a domain that is used by
    numerous attributes
  - Improves schema readability
  - Example:
    - CREATE DOMAIN SSN_TYPE as CHAR(9);
- *TYPE*
  - User Defined Types (UDTs) are supported for object oriented
    applications. Uses the command =CREATE TYPE=
** Specifying Attribute Constraints and Attribute Defaults
*Basic constraints*:
- Restrictions on attribute domains and NULL
- Default value of an attribute
  - =DEFAULT <value>=
- NOT NULL constraint
  - NULL is not permiteed for a particular attribute
- For example: 

#+DOWNLOADED: screenshot @ 2021-03-20 20:31:15
[[file:images/SQL/2021-03-20_20-31-15_screenshot.png]]

*Basic constraints*
- Constraints on individual tuples within a relation using the CHECK clause
  - =Dnumber INT NOT NULL CHECK (Dnumber >0 AND Dnumber <21);=
- The CHECK clause can also be used in conjunction with the CREATE DOMAIN
  statement
- For example
  - =CREATE DOMAIN D_NUM AS INTEGER CHECK (D_NUM AND D_NUM < 21);=
** Specifying Constraints in SQL
- *Basic constraints*:
  - Relational Model has 3 basic constraint types that are supported in SQL
    - *Key* constraint: A primary key value cannot be duplicated
    - *Entity Integrity* constraint: A primary key value cannot be null
    - *Referential integrity* constraints: The /foreign key/ must have a value
      that is already present as a /primary key/, or may be null.
** Specifying Key and Referential Integrity Constraints
- *PRIMARY KEY* clause
  - Specifies one or more attriubtes that make up the primary key of a relation
  - =Dnumber INT PRIMARY KEY=
- *UNIQUE* clause
  - Specifies alternate (secondary) keys (called CANDIDATE keys in the
    relational model)
  - =Dname VARCHAR(15) UNIQUE;=
- *FOREIGN KEY* clause
=FOREIGN KEY (Super_ssn) REFERENCES EMPLOYEE(Ssn) ON DELETE SET NULL ON UPDATE
CASCADE;=
- Default action for violation is to reject update operation
- Attach *referential triggered action* clause
  - Options include =SET NULL, CACASE= and =SET DEFAULT=
  - Action taken by the DBMS for the =SET NULL= or =SET DEFAULT= is the same for
    both =ON DELETE= and =ON UPDATE=
  - =CASCADE= option suitable for "relationship" relations
** Giving Names to Constraints
- Using the keyword *CONSTRAINT*
  - Name a constraint
  - Useful for later altering
Example:

#+DOWNLOADED: screenshot @ 2021-03-19 11:09:43
[[file:images/SQL/2021-03-19_11-09-43_screenshot.png]]

** Specifying Constraints on Tuples Using CHECK
- Additional Constraints on individual tuples within a relation are also
  possible using CHECK
- =CHECK= clauses at the end of =CREATE TABLE= statement
  - Apply to each tuple individually
    - =CHECK (Dep_create_date <= Mgr_start_date);=
- These are row-based constraints, because they apply to each row individually
  and are checked whenever a row is inserted or modified

* Basic Retrieval Queries in SQL
- SELECT statment
  - One basic statement for retrieving informatoin from a database
- Important distrinction between the practical SQL model and the formal
  *Relational* model discussed in topic 4.
  - SQL allows a table to have two or more tuples that are identical in all
    their attribute values
    - Unlike relational model (relational model is strictly set-theory based)
    - SQL table is not a set of tuples, but a *Maltiest (or a bag)*
    - Some SQL relations are constrained to be /sets/ because a key constraint
      has been declared
** The SELECT-FROM-WHERE Structure of Basic SQL Queries
- Basic for of the SELECT statement

#+DOWNLOADED: screenshot @ 2021-03-20 20:46:04
[[file:images/Basic_Retrieval_Queries_in_SQL/2021-03-20_20-46-04_screenshot.png]]
where
- <attribute list> is a list of attribute names whose values are to be retrieved
  by the query
- <table list> is a list of the relation names required to process the query
- <condition> is a conditional (Boolean) expression that identified the tuples
  to be retrieved by the query

- Basic logical comparison operators for comparing attribute values
  - =, <, <=, >, >=, and <>
- *Projection attributes*
  - attributes whose values are to be retrieved
- *Selection condition*
  - Boolean condition that must be true for any retrieved tuple Selection
    conditions include join conditions when multiple relations are involved
- Consider an implicit tuple variable 9or iterator) looping over individual
  tuples and evaluating against the conditions

** Basic Retrieval Queries
#+DOWNLOADED: screenshot @ 2021-03-20 20:54:13
[[file:images/Basic_Retrieval_Queries_in_SQL/2021-03-20_20-54-13_screenshot.png]]


#+DOWNLOADED: screenshot @ 2021-03-20 20:55:28
[[file:images/Basic_Retrieval_Queries_in_SQL/2021-03-20_20-55-28_screenshot.png]]
** Ambiguous Attribute Names
- Name name can be used for two (or more) attributes in different relations
  - As long as the attributes are in different relations
  - Must *qualify* the attribute name with the relation name to prevent
    ambiguity

#+DOWNLOADED: screenshot @ 2021-03-20 21:07:52
[[file:images/Basic_Retrieval_Queries_in_SQL/2021-03-20_21-07-52_screenshot.png]]

** Aliases or tuple variables
- Declare alternative relation names E and S refer to the EMPLOYEE relation
  twice in a query:
*Query 8.* For each employee, retrieve the employee's first and last name and
 the first and list name of his or her immediate supervisor

#+DOWNLOADED: screenshot @ 2021-03-20 21:10:06
[[file:images/Basic_Retrieval_Queries_in_SQL/2021-03-20_21-10-06_screenshot.png]]
- Recommended practice to abbreviate names and to prefix same or similar
  attribute from multiple tables
** Aliasing, Renaming and Tuple Variables
- The attribute names can also be renamed, eg:

#+DOWNLOADED: screenshot @ 2021-03-20 21:11:25
[[file:images/Basic_Retrieval_Queries_in_SQL/2021-03-20_21-11-25_screenshot.png]]
- Note that the relation EMPLOYEE now has a variable name E which corresponds to
  a tuple variable
- WE can use alias-naming or renaming mechanism in any SQL query to specify
  tuple variables. Eg,

#+DOWNLOADED: screenshot @ 2021-03-20 21:12:23
[[file:images/Basic_Retrieval_Queries_in_SQL/2021-03-20_21-12-23_screenshot.png]]

** Unspecified WHERE Clause and Use of the Asterisk
- Missing WHERE clause
  - Indicates no condition on tuple selection
- Effect is a CROSS PRODUCT
  - Result is all possible tuple combinations result

#+DOWNLOADED: screenshot @ 2021-03-20 21:13:43
[[file:images/Basic_Retrieval_Queries_in_SQL/2021-03-20_21-13-43_screenshot.png]]

- Specify an asterisk (*)
  - Retrieve all the attribute values of the selected tuples
  - The * can be prefixed by the relation name, eg. EMPLOYEE.* refers to all
    attributes of EMPLOYEE table

#+DOWNLOADED: screenshot @ 2021-03-20 21:14:52
[[file:images/Basic_Retrieval_Queries_in_SQL/2021-03-20_21-14-52_screenshot.png]]

** Table as Sets in SQL
- SQL does not automatically eliminate duplicate tuples in query results
- For aggregate operations duplicates must be accounted for
- Use the keyword DISTINCT in the SELECT clause
  - Only distinct tuples should remain in the result

#+DOWNLOADED: screenshot @ 2021-03-20 21:16:19
[[file:images/Basic_Retrieval_Queries_in_SQL/2021-03-20_21-16-19_screenshot.png]]

** Tables as Sets in SQL
- SQL has directly incorporated some of set operations
  - UNION, EXCEPT (ie., set difference), INTERSECT

#+DOWNLOADED: screenshot @ 2021-03-20 21:18:05
[[file:images/Basic_Retrieval_Queries_in_SQL/2021-03-20_21-18-05_screenshot.png]]

- Corresponding multiset operations: UNION, ALL, EXCEPT, ALL INTERSECT ALL
- Type compatibility is needed for these operations to be valid
** Substring Pattern Matching and Arithmetic Operators
- *LIKE* comparison operator
  - Used for string *pattern matching*
  - % replaces an arbitrary number of zero or more characters
  - underscore (_) replaces a single character
  - Examples: *WHERE* Address *LIKE* '%HOUSTON, TX%';
  - *WHERE* Ssn *LIKE* '_ _ 1 _ _ 8901';
- *BETWEEN* comparison operator,eg:

#+DOWNLOADED: screenshot @ 2021-03-20 21:22:02
[[file:images/Basic_Retrieval_Queries_in_SQL/2021-03-20_21-22-02_screenshot.png]]

** Aritmetic Operations
- Standard arithmetic operators
  - Addition (+), subtraction (-), multiplication (*), and division (/) may be
    included as a part of *SELECT*

#+DOWNLOADED: screenshot @ 2021-03-20 21:23:32
[[file:images/Basic_Retrieval_Queries_in_SQL/2021-03-20_21-23-32_screenshot.png]]

** Ordering of Query Results
- Use *ORDER BY* clause

#+DOWNLOADED: screenshot @ 2021-03-20 21:24:24
[[file:images/Basic_Retrieval_Queries_in_SQL/2021-03-20_21-24-24_screenshot.png]]
- The default order is ascending order of values
- Keyword *DESC* to set a descending order of values
- Keyword *ASC* to specify ascending order explicitly
- Typically placed at the end of the query

#+DOWNLOADED: screenshot @ 2021-03-20 21:25:35
[[file:images/Basic_Retrieval_Queries_in_SQL/2021-03-20_21-25-35_screenshot.png]]

** Basic SQL Retrieval Query Block
- A simple retrieval query in SQL can consist of up to four clauses
  - only the first two SELECT and FROM are mandatory

#+DOWNLOADED: screenshot @ 2021-03-20 21:26:58
[[file:images/Basic_Retrieval_Queries_in_SQL/2021-03-20_21-26-58_screenshot.png]]

- SELECT: lists the attributes to be retrieved
- FROM: specifies all relations (tables) needed
- WHERE: identifies conditions for the selection
- ORDERBY: specifies an order for displaying the results

** INSERT, DELETE, and UPDATE statements in SQL
- Three commands used to modify the database:
  - INSERT, DELETE, UPDATE
- *INSERT* typically insets a single tuple (row) into a relation (table)
- *UPDATE* modifies attribute values of a number of tuples (rows) in a relation
  (table) that satisfy the condition
- *DELETE* removes a number of tuples (rows) in a relation (table) that satisy
  the condition
- Both UPDATE and DELETE may propagate to tuples in other relations if
  referential triggered actions are specified in the referential integrity
  constraints of the DDL

*** INSERT
- In its simplest form, it is used to add one or more tuples to a relation
- Attribute values should be listed in the same order as the attributes were
  specified in the *CREATE TABLE* command. They have to match the attribute
  order
- Constraints on data types are observed automatically
- Any integrity constraints as a part of the DDL specification are enforced
- Specify the relation name and a list of values for the tuple. All values
  including nulls are supplied

#+DOWNLOADED: screenshot @ 2021-03-20 21:33:28
[[file:images/Basic_Retrieval_Queries_in_SQL/2021-03-20_21-33-28_screenshot.png]]

- The variation below inserts multiple tuples where a new table is loaded from
  the result of a query
- Data inserted from selected values (kinda like copying). Populating using a
  combination of the INSERT INTO. In this command we are renaming things but I
  don't know why he's doing that? 
#+DOWNLOADED: screenshot @ 2021-03-20 21:34:04
[[file:images/Basic_Retrieval_Queries_in_SQL/2021-03-20_21-34-04_screenshot.png]]

*** Bulk Loading of TABLEs
- Another variation of *INSERT* is used for bulk-loading of several tuples into
  tables
- A new table NEW can be created with the same attributes as T using LIKE and
  DATE in the syntax, it can be loaded with entire data
- This allows us to create a table and insert data at the same time!
- Example:

#+DOWNLOADED: screenshot @ 2021-03-20 21:35:40
[[file:images/Basic_Retrieval_Queries_in_SQL/2021-03-20_21-35-40_screenshot.png]]

*** DELETE
- Removes tuples from a relation
  - Includdes a WHERE-clause to select the tuples to be deleted
  - Referential integrity should be enforced
  - Tuples are deleted from only /one table/ at a time (unless CASCADE is
    specified on a referential integrity constraint)
  - A missing WHERE-clause specifies that all /tuples/ in the relation are to be
    deleted; the table then becomes an empty table
  - The number of tuples deleted depends on the number of tuples in the relation
    that satisfy the WHERE-clause
- Removes tuples from a relation
  - Includes a WHERE clause to select the tuples to be deleted. The number of
    tuples deleted will vary

#+DOWNLOADED: screenshot @ 2021-03-20 21:38:42
[[file:images/Basic_Retrieval_Queries_in_SQL/2021-03-20_21-38-42_screenshot.png]]

*** UPDATE
- Used to modify attribute values of one or more selected tuples
- A WHERE-clause selects the tuples to be modified
- An additional SET-clause specifies the attributes to be modified and their new
  values
- Each command modifies tuples /in the same relation/
- Referential integrity specified as part of DDL specification is enforced
**** The UPDATE Command
- And addition SET-clause specifies the attributes to be modified and their new
  values
- For example
  - Change the location and controlling department number of project number 10
    to 'Bellaire' and 5, respectively

#+DOWNLOADED: screenshot @ 2021-03-20 21:41:43
[[file:images/Basic_Retrieval_Queries_in_SQL/2021-03-20_21-41-43_screenshot.png]]

- For example: Give all employees in the 'Research' department a 10% raise in
  salary

#+DOWNLOADED: screenshot @ 2021-03-20 21:42:45
[[file:images/Basic_Retrieval_Queries_in_SQL/2021-03-20_21-42-45_screenshot.png]]

- In this request, the modified SALARY value depends on the original SALARY
  value in each tuple
  - The reference to the SALARY attribute on the right of = refers to the old
    SALARY value before modification
  - The reference to the SALARY attribute on the left of = refers to the new
    SALARY value after the modification

** Additional Features of SQL
- Techniques for specifying complex retrieval queries
- Writing programs in various programming languages that include SQL statements:
  Embedded and dynamic SQL, SQL/CLI (Call level interface) and its predecessor
  ODBC, SQL/PSM (Persistent Stored Module)
- Set of commands for specifying physical database design parameters, file
  structure for relations, and access paths, (eg CREATE INDEX)
- Transaction control commands
- Specifying the granting and revoking of privileges to users
- Constructs for creating triggers
- Enhanced relational systems known as object-relational define relations as
  classes. Abstract data types (called User defined types (UDTs)) are supported
  with CREATE TYPE
- New technologies such as XML and OLAP are added to versions of SQL
* More SQL
** Comparisons Involving NULL and Three-Valued Logic
- Meanings of NULL
  - *Unknown value*:
    - Value exists but is not known, or it is not known whether the value exists
      or not
  - *Unavailable or withheld value*
    - Value exists but is purposely withheld
  - *Not applicable attribute*
    - The attribute does not apply to this tuple or is undefined for this tuple,
      or is undefined for this tuple
- SQL does not distinguish among different meanings of NULL
- When a NULL value is involved in a comparison, the result is considered to be
  UNKNOWN
- SQL uses a three-valued logic
  - TRUE, FALSE, UNKNOWN

#+DOWNLOADED: screenshot @ 2021-03-22 10:33:06
[[file:images/More_SQL/2021-03-22_10-33-06_screenshot.png]]

- SQL allows queries that check whether an attribute value is =NULL=
- Rather than using = or <> to compare an attribute value to NULL, SQL uses the
  comparison operator:
  - IS NULL or IS NOT NULL
  - Each individual NULL value is considered to be distinct from every other
    NULL value

#+DOWNLOADED: screenshot @ 2021-03-22 12:58:49
[[file:images/More_SQL/2021-03-22_12-58-49_screenshot.png]]

** Nested Queries, Tuples, adn Set/ Multiset Comparisons
- Nested queries
  - Complete select-from-where blocks within WHERE clause of another query
  - *Outer query and nested subqueries*
- Comparison operator =IN=
  - Compares value v with a set (or multiset) of values V
  - Evaluates to =TRUE= if v is one of the elements in V
- Some queries require that existing values in the database be fetched and then
  used in a comparison condition
  - can be formulated using nested queries
  - the other query is called the outer query

#+DOWNLOADED: screenshot @ 2021-03-22 13:03:28
[[file:images/More_SQL/2021-03-22_13-03-28_screenshot.png]]

** Nested Queries
- SQL allows the use of tuples of values in comparison by:
  - Place them within parentheses

#+DOWNLOADED: screenshot @ 2021-03-22 13:04:25
[[file:images/More_SQL/2021-03-22_13-04-25_screenshot.png]]

- In addition to the IN operator, a number of other comparison operators can be
  used to compare a single vvalue v to a set (or mulitset) of values V.
  - = ANY (or = SOME) operator
  - Returns =TRUE= if the value v is equal to some value in the set V, and is
    hence equivalent to =IN=

- Use other comparison operators to compare a single value v
  - Other operators that can be combined with =ANY= (or =SOME=): >, >=, <, <=,
    and <>
  - The keyword =ALL= can also be combined with each of these operators
    - value must exceed all values from nested query

#+DOWNLOADED: screenshot @ 2021-03-22 13:07:19
[[file:images/More_SQL/2021-03-22_13-07-19_screenshot.png]]

- In the case of several levls of nested queries, we may have possible ambiguity
  among the attribute names (if attributes of the same name exist).
- Avoid potiential errors and ambiguities
  - Create tuple variables (aliases) for al tables referenced in the SQL query

#+DOWNLOADED: screenshot @ 2021-03-22 13:08:45
[[file:images/More_SQL/2021-03-22_13-08-45_screenshot.png]]

** Correlated Nested Queries
- Two queries are *correlated*:
  - A condition in the WHERE clause of a nested query references some attributes
    of a relation declared in the outer query
  - Is evaluated once for each tuple in the outer query
- Queries that are nested using the = or IN comparision operator can be
  collapsed into one single block, e.g: Q16 can be written as:

#+DOWNLOADED: screenshot @ 2021-03-22 13:11:13
[[file:images/More_SQL/2021-03-22_13-11-13_screenshot.png]]

** The EXISTS and UNIQUE Functions in SQL for correlating queries
- =EXIST= and =UNIQUE= are Boolean functions that return =TRUE= or =FALSE=,
  hence can be used in the =WHERE= clause
- =EXISTS= and =NOT EXISTS= functions
  - Check whether the result of a correlated nested query is empty or not
- SQL function =UNIQUE(Q)=
  - Returns =TRUE= if there are no duplicate tuples in the result of query Q

*** Use of EXISTS
- =EXIST(Q)= returns =TRUE= if there is atleast one tuple in the results of the
  nested query Q, and return =FALSE= otherwise

#+DOWNLOADED: screenshot @ 2021-03-22 13:14:41
[[file:images/More_SQL/2021-03-22_13-14-41_screenshot.png]]

*** Use of NOT EXISTS
- To achieve the "for all" (universal quantifier) effect, we use /double
negation/ the way in 
- Query: List first and list name of employees who work on /ALL projects
  controlled by Department 5/

#+DOWNLOADED: screenshot @ 2021-03-22 13:16:17
[[file:images/More_SQL/2021-03-22_13-16-17_screenshot.png]]

** Explicity sets and renaming of Atributes in SQL
- Can use explicit set of values in where clause,
  - eg: retrieve the SSN of all employees who work on project numbers 1, 2, or 3

#+DOWNLOADED: screenshot @ 2021-03-22 13:21:37
[[file:images/More_SQL/2021-03-22_13-21-37_screenshot.png]]

- Use qualifier AS followed by desired new name
  - Rename any attribute that appears in the result of a query. This will be
    what shoes up at the table column headers

#+DOWNLOADED: screenshot @ 2021-03-22 13:25:23
[[file:images/More_SQL/2021-03-22_13-25-23_screenshot.png]]

** Specifying Joined Tables in the FROM Clause of SQL
- *Joined table (or joined relation)*
  - Permits users to specify a table resulting from a join operation in the
    FROM clause of a query
- The FROM clause in Q1A
  - Contains a single joined table, which has all the attributes of the first
    table EMPLOYEE followed by all the attributes of the second table DEPARTMENT

#+DOWNLOADED: screenshot @ 2021-03-22 13:30:10
[[file:images/More_SQL/2021-03-22_13-30-10_screenshot.png]]

*** Different TYpes of JOINed Tables in SQL
- Allow users to specify different types of join, such as
  - NATURAL JOIN
  - Various types of OUTERJOIN (LEFT, RIGHT, FULL)
- NATURAL JOIN on two relations R and S
  - No join condition specified
  - Is equivalent to an implicit EQUIJOIN (implicit equal join)  condition for each pair of attributes
    with the same name from R and S
  - Each such pair of attributes is included only once in the resulting relation

**** Natural Join
- If the names of the join attributes are not the same in the base relations, it
  is possible to *rename* attributes so that they match and then to apply the
  NATURAL JOIN. Because DEPARTMENT doesn't have a Dno as employee has, we have
  to rename it.

#+DOWNLOADED: screenshot @ 2021-03-22 13:34:09
[[file:images/More_SQL/2021-03-22_13-34-09_screenshot.png]]

- The above works with EMPLOYEE.Dno = DEPT.DNo as an implicit join condition

**** INNER and OUTER Joins
- INNER JOIN (vs OUTER JOIN)
  - Default type of join in a joined table
  - Tuple is included in the result only if a matching tuple exists in the other
    relation
- LEFT OUTER JOIN
  - Every tuple in left table must appear in the result
  - If no matching tuple, added with NULL values for attributes of right table
- RIGHT OUTER JOIN
  - Every tuple in right table must appear in result
  - If no matching tuple, padded with NULL values for attributes of left table

OUTER JOINS are based on the condition, where if it doesn't meet the condition
then it wont be included in the joined table. But in OUTER JOIN if there isn't a
match it just puts a null value for the right hand side of the table. Different
ways of handling this
***** Example: LEFT OUTER JOIN
- LEFT OUTER JOIN

#+DOWNLOADED: screenshot @ 2021-03-22 13:37:44
[[file:images/More_SQL/2021-03-22_13-37-44_screenshot.png]]

- In some DBMS, a different syntax was used to specify outer joins by using the
  comparision operators, eg
  - In Oracle, +=, =+, and +=+ symbols are used for left, right and full outer
    join respectively

#+DOWNLOADED: screenshot @ 2021-03-22 13:38:56
[[file:images/More_SQL/2021-03-22_13-38-56_screenshot.png]]

** MULTIway JOIN in the FROM clause
- FULL OUTER JOIN: combines results of the LEFT and RIGHT OUTER JOIN
- Can nest JOIN specifications for multiway join

#+DOWNLOADED: screenshot @ 2021-03-22 13:40:04
[[file:images/More_SQL/2021-03-22_13-40-04_screenshot.png]]

** Aggregate Functions in SQL
- Used to summarize information from multiple tuples into a single tuple summary
- Built-in aggregate functions
  - COUNT, SUM, MAX, MIN, and AVG
- *Grouping*
  - Create subgroups of tuples before summarizing
- To select entire groups, HAVING clause is used
- Aggregate functions can be used in the SELECT clause or in a HAVING clause

** Renaming Results of Aggregation
- Following query returns a single row of computed values from EMPLOYEE table:

#+DOWNLOADED: screenshot @ 2021-03-22 13:42:16
[[file:images/More_SQL/2021-03-22_13-42-16_screenshot.png]]

- The result can be presented with new names in the output table

#+DOWNLOADED: screenshot @ 2021-03-22 13:42:41
[[file:images/More_SQL/2021-03-22_13-42-41_screenshot.png]]

** Aggregate Functions in SQL
- NULL values are discraded when aggregate functions are applied to a particular
  column

#+DOWNLOADED: screenshot @ 2021-03-22 13:44:32
[[file:images/More_SQL/2021-03-22_13-44-32_screenshot.png]]

- Use the COUNT function to count values in a column (attribute) rather than
  tuples, eg:

#+DOWNLOADED: screenshot @ 2021-03-22 13:45:11
[[file:images/More_SQL/2021-03-22_13-45-11_screenshot.png]]
- We can specify a correlated nested query with an aggregate function, eg. to
  retrieve the names of all employees who have two or more dependents (Query 5)

#+DOWNLOADED: screenshot @ 2021-03-22 13:46:07
[[file:images/More_SQL/2021-03-22_13-46-07_screenshot.png]]

** Grouping: The GROUP BY Clause
- *Partition* the relation into non overlapping subsets (or groups) of tuples
  - Each group (partition) consists of the tuples that have the same value of
    some attributes, called *grouping attributes(s)*
  - Apply function to ech such group independently to produce summary
    information about each group
- GROUP BY clause
  - Specifies grouping attributes
- COUNT (*) counts the number of rows in the group

*** Examples of GROUP BY
- The grouping attribute must appear in the SELECT clause

#+DOWNLOADED: screenshot @ 2021-03-22 13:48:46
[[file:images/More_SQL/2021-03-22_13-48-46_screenshot.png]]
- Creates a number of groups that are grouped by the Dno, then count each of the
  number of things in the subgroup and perform average salary on the subgroup

- If the grouping attribute has NULL as a possible value, then a separate group
  is created  for the null value (eg NULL Dno in the above query)
- GROUP BY may be applied to the result of a JOIN:

#+DOWNLOADED: screenshot @ 2021-03-22 13:49:53
[[file:images/More_SQL/2021-03-22_13-49-53_screenshot.png]]

*** Grouping: The GROUP BY and HAVING Clauses
- *HAVING* clause
  - Provides a condition to select or reject an entire group
- *Query 26*: For each project /on which more than two employees work/, retrieve
  the project number, the project name and the nubmer of employees who work on
  the project

#+DOWNLOADED: screenshot @ 2021-03-22 13:51:30
[[file:images/More_SQL/2021-03-22_13-51-30_screenshot.png]]

** Combining the WHERE and the HAVING Clause
- Consider the query:
  - We want to count the /total/ number of employees whose salaries exceed
    $40,000 in each department but only for departments where more than 5
    employees work
THIS IS THE INCORRECT QUERY

#+DOWNLOADED: screenshot @ 2021-03-29 11:54:48
[[file:images/More_SQL/2021-03-29_11-54-48_screenshot.png]]

Correct specifiation of the query:
 - Note: the WHERE clause applies tuple by tuple whereas having applies to
   entire group of tuples

#+DOWNLOADED: screenshot @ 2021-03-29 11:55:54
[[file:images/More_SQL/2021-03-29_11-55-54_screenshot.png]]


** Other SQL Construct: WITH
- The WITH clause allows a user to define a table that will only be used in a
  particular query (may not be available in all SQL base DBMS implemenattions)
- Used for convenience to create a temporary "View" and use that immediately in
  a query
- Allows a more straightforward way of looking a step-by-step query
- This temporary "View" (or table) will be discarded after the query is executed
*** Example of WITH
- An alternative approach to performing Q28:

#+DOWNLOADED: screenshot @ 2021-03-29 11:58:58
[[file:images/More_SQL/2021-03-29_11-58-58_screenshot.png]]
- We define in the WITH clause a temporary table BIGDEPTS, and use this table in
  the subsequent query

** Another SQL construct: CASE
- SQL also has a CASE construct
  - Used when a value can be different based on certain conditions
  - Can be used in any part of an SQL query where a value is expected
  - Applicable when querying, inserting or updating tuples
  - For example: Employees are receiving different raises in different
    departments (A variation of the update U6)


#+DOWNLOADED: screenshot @ 2021-03-29 12:17:38
[[file:images/More_SQL/2021-03-29_12-17-38_screenshot.png]]

** Recursive Queries in SQL
- An example of a recursive relationship between tuples of the same type is the
  realtionship between an employee and a supervisor
- This relationship is described by the foreign key =Super_snn= or the
  =EMPLOYEE= relation
- A example of a *recursive operation* is to retrieve all supervisees of a
  supervisory employee =e= at all levels, ie:

#+DOWNLOADED: screenshot @ 2021-03-29 12:19:03
[[file:images/More_SQL/2021-03-29_12-19-03_screenshot.png]]

*** Example of a RECURSIVE Query

#+DOWNLOADED: screenshot @ 2021-03-29 12:19:19
[[file:images/More_SQL/2021-03-29_12-19-19_screenshot.png]]

- The above query starts with an empty SUP_EMP and successively builds SUP_EMP
  table by computing immediate superviesees first, then second level
  supervisees, etc, until a *fixed point* is reached and no more supervisees can
  be added

*** Summary

#+DOWNLOADED: screenshot @ 2021-03-29 12:22:03
[[file:images/More_SQL/2021-03-29_12-22-03_screenshot.png]]

** Specifying Constraints as Assertions and Actions as Triggers
- CREATE ASSERTION
  - Specify additional types of constraints that are outside scope of the
    built-in realtional mode constraints (ie /primary/ and /unique keys/ ,
    /entity integrity/ and /referential integrity/
- CREATE TRIGGER
  - Specify automatic actions that database sysetm will perform when certain
    events and conditions occur
  - This type of functinality is genreally referred as *active databases*
*** Specifying General Constraints as Assertions in SQL
- *CREATE ASSERTION*
  - Specify a query that selects any tuples that violate the desired condition
  - Use only in cases where it goes beyond a simple =CHECK= which applies to
    individual attributes and domains
  - EG: a constraint specifies that the salary of an employee must not be
    greater than that of the manager in the same department

#+DOWNLOADED: screenshot @ 2021-03-29 12:25:46
[[file:images/More_SQL/2021-03-29_12-25-46_screenshot.png]]

** Introduction to Triggers in SQL
- CREATE TRIGGER statement
  - Specify the type of action to be taken when ceratin events occur and when
    certain conditions are satisfied
    - Eg: a manager may want to be informed if an employee's travel expenses
      exceed a certain limit in the database staet
  - Used to monitor the database, and implement such actions in BDMS
- Typical trigger has three components which make it a rule for an "active database":
  - *Event(s)*: usually database update operations
  - *Condition*: determines whether rule actions should be executed
  - *Action*: to be taken, usually SQL statements or transactions, etc.
*** Example use of TRIGGERs
- Supporse we want to check whenever an employee's salary is greater than the
  salary of this or her direct supervisor in the COMPANY database
  - Severl events can trigger this rule: inserting a new employee record,
    changing an employee's salary, or changing an employee's supervisor
  - Suppose that the action to take woould be to call an exernal stored
    procedure INFORM_SUPERVISOR, which wll notify the supervisor

#+DOWNLOADED: screenshot @ 2021-03-29 12:47:03
[[file:images/More_SQL/2021-03-29_12-47-03_screenshot.png]]

** Views (Virtual Tables) in SQL
- Concept of a view in SQL
  - Single table derived from other tables (can be base tables or previously
    defined views)
  - Does not necessaryily exist in physical form, is considered to be a *virtual
    table* (in contrast to base tables)
  - As a way of specifying a table that we need to reference frequently, even
    though it may not exist physically
- Once a View is defined, SQL queries can use the View relation in the FROM
  clause
- View is always up-to-date
  - Responsibility of the DBMS and not the user

*** Sepcification of Views in SQL
- *CREATE VIEW* command
  - Give table name, list of attribute names, and a query to specify the
    contents of the view
  - In V1, attributes retain the names from base tables. In V2 attributes are
    assigned names

#+DOWNLOADED: screenshot @ 2021-03-29 12:51:08
[[file:images/More_SQL/2021-03-29_12-51-08_screenshot.png]]

*** View Implementation, View Update, and Inline Views
- Complex problem of efficiently implementing a view for querying
**** Strategy 1: Query modification approach
- Compute the view as and when needed. Do not store permanently
- Modify view query into a query on underlying base tables
- Disadvtange: inefficient for views defined via complex queries that are
  time-consuming to execute
**** Strategy 2: View materialization
- Physically create a temporary view table when the view is first queried or
  created
- Keep that table on the assumption that other queries on the view will follow
- Requers efficient strategy for automatically updating the view table when the
  base tables are updated
  - *Incremental update strategy for materialized views*
    - DBMS determines what new tuples must be inserted, deleted, or modified in
      a a materialized view table
*** View Materialization
- Multiple ways to handle materialization:
  - *immediate update* strategy updates a view as soon as the base tables are
    changed
  - *lazy update* strategy updates the view when needed by a view query
  - *periodic update* strategy updates the view periodically (in the latter
    stategy, a view query may get a result that is not up to date). This is
    commonly used in Bank, Retail store operations, etc.
** View update
- update on a view defined on a single table without any aggregate functions
  - Can be mapped to an update on underlying base table-possible if the pimary
    key is preserved in the view
  - Update not permitted on aggregate views, eg:

#+DOWNLOADED: screenshot @ 2021-03-29 13:00:48
[[file:images/More_SQL/2021-03-29_13-00-48_screenshot.png]]
- Cannot be processes because Total_sal is a computed value in the view
  definition


*** View Update and Inline Views
- View involving joins
  - Often not possible for DBMS to determin which of the updates is intended
- Clause *WITH CHECK OPTION*
  - Must be added at the end of the view definition if a view is to be  updated
    to amke sure that tuples being updated stay in the view
- *In-line view*
  - Defined in the FROM clause of an SQL query (eg: we saw it used in the WITH
    example)

*** Views as authorization mechanism
- SQL query authorizatoin statements (GRANT and REVOKE) are described in later
  topics
- Views can be used to hid certain attributes or typles from unauthorizaed users
- Eg: for a user who is only allowed to see employee information for those who
  work for department 5, they only access the view DEPT5EMP:

#+DOWNLOADED: screenshot @ 2021-03-29 13:09:15
[[file:images/More_SQL/2021-03-29_13-09-15_screenshot.png]]

** Schema Change Statements in SQL
- *Schema evolution commands*
  - Can be used to alter a schema by adding or dropping tables, attributes,
    constraints and other schema elements
  - DBA may want to change the schema while the database is operational
  - Does not require recompilation of the database schema
  - Certain checks must be performed by the DBMS to ensure that the changes do
    not affect the rest of the database and cause  inconsistencies
** The DEOP Command
- DROP command
  - used to drop named schema elements, such as tables, domains or constraint
- Drop behaviour options:
  - CASCADE and RESTRICT
- Example:
  - DROP SCHEMA COMPANY CASCADE;
  - This removes the schema and all its elemens including tables, views,
    constraints, etc.
  - DROP TABLE DEPENDENT RESTRICT;
  - Table is dropped only if not refernced in any constraints

** The ALTER table command
- Alter table cations include:
  - Adding or dropping a column (attribute)
  - Chagning a column definition
  - Addding or dropping table constraints
- Example:

#+DOWNLOADED: screenshot @ 2021-03-29 13:18:28
[[file:images/More_SQL/2021-03-29_13-18-28_screenshot.png]]
- We must still enter a vlue for the new attribute job for each individual
  EMPLOYEE tuple
  - By using the default clause or the UPDATE command
** Dropping Columns, Default Values
- Drop a column must choose either CASCASDE or RESTRICT
  - CASCASDE: all constraints and views that refernce the column are dropped
    automatically
  - RESTRICT: the drop is sucessful only when no views or constraints (or other
    schema elements) reference the column

#+DOWNLOADED: screenshot @ 2021-03-29 13:23:08
[[file:images/More_SQL/2021-03-29_13-23-08_screenshot.png]]
- Default values can be dropped and altered:

#+DOWNLOADED: screenshot @ 2021-03-29 13:23:26
[[file:images/More_SQL/2021-03-29_13-23-26_screenshot.png]]

** Adding and Dropping Constraints
- We can change the constraints specified on a table by
  - Add or drop a named constraint
  - For example, to drop the constraint named EMPSUPERFK from the EMPLOYEE table

#+DOWNLOADED: screenshot @ 2021-03-29 13:24:15
[[file:images/More_SQL/2021-03-29_13-24-15_screenshot.png]]
- After removing the constraint, we can redefine a replacement constraint using
  the ADD CONSTRAINT keyword in the ALTER TABLE statement, eg:

#+DOWNLOADED: screenshot @ 2021-03-29 13:24:48
[[file:images/More_SQL/2021-03-29_13-24-48_screenshot.png]]

* Summary of SQL Syntax
c
#+DOWNLOADED: screenshot @ 2021-03-29 13:25:07
[[file:images/Summary_of_SQL_Syntax/2021-03-29_13-25-07_screenshot.png]]

#+DOWNLOADED: screenshot @ 2021-03-29 13:25:20
[[file:images/Summary_of_SQL_Syntax/2021-03-29_13-25-20_screenshot.png]]

** Combining the WHERE and HAVING Clause
- Consider the query:
- We want to count the /total/ number of employees whose salaries exceed $40000
  in each department, but only for departments where more than five employees
  work
The following is an incorrect query

#+DOWNLOADED: screenshot @ 2021-03-26 11:02:27
[[file:images/More_SQL/2021-03-26_11-02-27_screenshot.png]]
- =WHERE= clause is executed first to select individual or joined tuples
- =HAVING= clause is applied later to select individual groups of tuples

Correct specification of the query

* Relational Algebra
** Overview
- Relational algebra is the basic set of operations for the relational model
  - Provides a formal foundation for relational model operations
- These operations enable a user to specify *basic retrieval requests* (or
  *queries*) as /relational algebra expressions/
- The result of an operation is a new relation, which may have been formed from
  one or more /input/ relations
  - This property makes the algebra "closed" (all objects in relational algebra
    are relations)
- The *algebra operations* thus produce new relations
  - These can be further manipulated using operations of the same algebra
- A sequence of realtional algebra operations forms a *relational algebra
  expression*
  - The result of a relational algebra expression is also a relation that
    represents the result of a database query (or retrieval request)
  - Used as a basis for implementing and optimizing queries in the query
    processing and optimization of RDBMS
- Relational Algebra consists of several groups of operations
  - Unary Relational Operations
    - SELECT (symbol  (sigma))
    - PROJECT (symbol  (pi))
    - RENAME (symbol  (rho))
  - Relational Algebra Operations From Set Theory
    - UNION (U) INTERSECTION (N), DIFFERENCE (or MINUS, -)
    - CARTESIAN PRODUCT (X)
  - Binary Relational Operations
    - JOIN (several variations of JOIN exist)
    - DIVISION
  - Additional Relational Operations
    - OUTERJOINS, AGGREGATE FUNCTIONS (these compute summary of information:
      eg. SUM, COUNT AVG, MIN, MAX)

** Database State for COMPANY

#+DOWNLOADED: screenshot @ 2021-03-31 15:07:16
[[file:images/Relational_Algebra/2021-03-31_15-07-16_screenshot.png]]

** Unary Relational Operations: SELECT
- The SELECT operation (denoted by ) is used to select a /subset/ of the tuples
  from a relation based on a *selection condition*.
  - The selection condition acts as a *filter*
  - Keeps only those tuples that satisfy the qualifying condition
  - Tuples satisfying the condition are /selected/ whereas the other tuples are
    discarded (filtered out)
- Examples
  - Select the EMPLOYEE tuples whose department number is 4

#+DOWNLOADED: screenshot @ 2021-03-31 15:09:11
[[file:images/Relational_Algebra/2021-03-31_15-09-11_screenshot.png]]

- Select the employee tuples whose salary is greater than $30,000

#+DOWNLOADED: screenshot @ 2021-03-31 15:09:34
[[file:images/Relational_Algebra/2021-03-31_15-09-34_screenshot.png]]

** Unary Relational Operations: SELECT
- In general the /select/ operation is denoted by the   <selection condition>
  (R) where
  - The symbol  is used to denote the /select operator/
  - The selection condition is a Boolean (conditional) expression specified on
    the attributes of relation R
  - tuples that make the condition true are selected
    - Appear in the result of the operation
  - tuples that make the condition false are filtered out
    - discarded form the result of the operation

*** SELECT Properties
#+DOWNLOADED: screenshot @ 2021-03-31 15:14:20
[[file:images/Relational_Algebra/2021-03-31_15-14-20_screenshot.png]]
** Unary Relational Opoerations: PROJECT
- PROJECT Operation is denoted by  (pi)
- This operation keeps certain /columns/ (attributes/) from a relation and
  discards the other columns
  - PROJECT creates a vertical paritioning
    - The list of specified columns (attributes) is kept in each tuple
    - The other attributes in each tuple are discarded
- Example: To list each employee's first and last name and salary, the following
  is used:

#+DOWNLOADED: screenshot @ 2021-03-31 15:24:29
[[file:images/Relational_Algebra/2021-03-31_15-24-29_screenshot.png]]

- The general form of the /project/ operation is:

#+DOWNLOADED: screenshot @ 2021-03-31 15:25:07
[[file:images/Relational_Algebra/2021-03-31_15-25-07_screenshot.png]]

 -  (pi) is the symbol used to represent the /project/ operation
 - <attribute list> is the desired list of attributes from relation R
 - The project operation /removes any duplicate tuples/
   - This is because the result of the /project/ operation must be a /set to
     tuples/
     - Mathematical sets do not allow duplicate elements

 - PROJECT Operation Properties
   - The number of tuples in the result of projection  <list> (R) is always
     less than or equal to the number of tuples i R
     - If the list of attrubtes includes a /key/ of R, then the number of tuples
       in the result of PROJECT is /equal/ to the number of tuples in R
   - PROJECT is /not communicative

#+DOWNLOADED: screenshot @ 2021-03-31 15:28:14
[[file:images/Relational_Algebra/2021-03-31_15-28-14_screenshot.png]]

*** Example

#+DOWNLOADED: screenshot @ 2021-03-31 15:28:32
[[file:images/Relational_Algebra/2021-03-31_15-28-32_screenshot.png]]

** Relational Algebra Expressions
- We may want to apply several relational algebra operations one after the other
  - Either we can write the operations as a single *relational algebra
    expression* by nesting the operations, or
  - We can apply one operation at a time and create *intermediate result relations*
- In the latter case, we must give names to the relations that hold the
  intermediate results
*** Single expressions vs sequence of relational operations (Example)
- To retrieve the first name, last name, and salary of all employees who work in
  department number 5, we must apply a select and a projection operations
- We can write a /single relational algebra expression/ as follows:

#+DOWNLOADED: screenshot @ 2021-03-31 15:33:33
[[file:images/Relational_Algebra/2021-03-31_15-33-33_screenshot.png]]
- OR We can explicitly show the /sequence of operations/, giving a name to each
  intermediate relation:

#+DOWNLOADED: screenshot @ 2021-03-31 15:34:05
[[file:images/Relational_Algebra/2021-03-31_15-34-05_screenshot.png]]

- Left arrow symbol is the *assignment operation*

** Unary Relational Operations: RENAME 
- The RENAME operator is denoted by  (rho)
- In some cases, we may want to /rename/ the attributes of a relation or the
  relation name or both
  - Useful when a query requires multiple operations
  - Necessary in some cases (see JOIN operation later)
- IN SQL, a single query typically represents as a complex relational algebra
  expressions
  - Renaming in SQL is accomplished by aliasing using AS

#+DOWNLOADED: screenshot @ 2021-03-31 15:39:20
[[file:images/Relational_Algebra/2021-03-31_15-39-20_screenshot.png]]

- The general RENAME operation  can be expressed by any of the following forms:

#+DOWNLOADED: screenshot @ 2021-03-31 15:39:55
[[file:images/Relational_Algebra/2021-03-31_15-39-55_screenshot.png]]

- For convenience, we also use a shorthand for renaming attributes in an
  intermediate relation:
  - If we write:

#+DOWNLOADED: screenshot @ 2021-03-31 15:40:49
[[file:images/Relational_Algebra/2021-03-31_15-40-49_screenshot.png]]
  - If we write:

#+DOWNLOADED: screenshot @ 2021-03-31 15:41:03
[[file:images/Relational_Algebra/2021-03-31_15-41-03_screenshot.png]]
** Relatoinal Algebra Operations from Set Theory: UNION
- UNION Operation
  - Binary operation denoted by 
  - The result of R  S, is a relation that includes all tuples that are either
    in R or in S or in both R and S
  - Duplicate tuples are eliminated
  - The two operand relations R and S must be "type compatible) (or UNION
    compatible)
    - R and S must have same number of attributes
    - Each pair of corresponding attributes must be type compatible (have same
      or compatible domains)
- For example:
  - To retrieve the SSNs of all employees who either work in department 5
    (RESULT1 below) or directly supervise an employee who works in department 5
    (RESULT2 below)
  - We can use the UNION operation as follows:

#+DOWNLOADED: screenshot @ 2021-04-01 12:50:19
[[file:images/Relational_Algebra/2021-04-01_12-50-19_screenshot.png]]
- The union operation produces the tuples that are in either RESULT1 or RESULT2
  or both
- Visually this union operation can be described as

#+DOWNLOADED: screenshot @ 2021-04-01 12:52:10
[[file:images/Relational_Algebra/2021-04-01_12-52-10_screenshot.png]]
** Relational Algebra Operations from Set Theory
- Type compatibility of operands is required for the binary set operations, such
  as UNION , INTERSECTION , and SET DIFFERENCE 
- R1(A1, A2, ..., An) and R2(B1, B2, ..., Bn) are type compatible if:
  - They have the same number of attributes, and
  - the domains of corresponding attributes are type compatible (ie: dom(Ai) =
    dom(Bi) for i=1..n
- The resulting relation for R1R2 (also for R1R2, or R1-R2) has the same
  attribute names as the /first/ operand relation (by convention)

** Operations from Set Theory: INTERSECTION / SET DIFFERENCE
- INTERSECTION is denoted by 
  - The result of the operation RS is a relation that includse all tuples that
    are in both R and
  - The attribute names in the result will be the same as the attribute names in
    R
- SET DIFFERENCE (also called MINUS or EXCEPT) is denoted by -
  - The result of R - S is a relation that includes all the tuples that are in R
    by not in S
  - The attribute names in the result will be the same as the attribute names in
    R
- Two operand relation R and S must be 'type compatible'
*** Example

#+DOWNLOADED: screenshot @ 2021-04-01 13:02:10
[[file:images/Relational_Algebra/2021-04-01_13-02-10_screenshot.png]]

** Some properties of UNION, INTERSECT and DIFERENCE
- Notice taht both union and intersecton are commutative operations; that is
  - R  S = S  R, and R  S = S  R
- Both union and intersection can be treated as n-ary operations applicable to
  any number of relations as both are associative operations; that is
  - R  (S  T) = (R  S)  T
  - (R  S)  T = R  (S  T)
- The minus operations is not commutative; that is, in general
  - R  S  S  R 
** Relational Algebra Operations from Set Theory: CARTESIAN PRODUCT
- CARTESIAN PRODUCT (or CROSS JOIN) Operation
  - This operation is used to combine tuples from two realtions in a
    combinatorial fashion
  - Denoted by R(A1, A2, ..., An) x S(B1, B2, ..., Bm)
  - Result is a relation Q with degree n + m attributes:
    - Q(A1, A2, .., An, B1, B2, ... , Bm), in that order
  - The resulting relation state has one tuple for each combination of tuples -
    one from  Rand one from S
  - Hence, if R has nR types (denoted as |R| = nR), and S has nS tuples then R x
    S will have nR * nS tuples
  - The two operands do NOT have to be 'type compatible'
- Generally, the CROSS PRODCUT operation applied by itself is meaningless
  - could become meaningful when followed by other operations, such as a
    selection that matches values of attributes coming from the component
    relations.
- Example (not meaningful):

#+DOWNLOADED: screenshot @ 2021-04-01 13:21:50
[[file:images/Relational_Algebra/2021-04-01_13-21-50_screenshot.png]]

- EMP_DEPENDENTS will contain every combination of EMPNAMES or DEPENDENT
  - whether or not they are actually related
- To keep only combinations where the DEPENDENT is related to the EMPLOYEE, we
  add a SELECT operation as follows
- Example (meaningful)

#+DOWNLOADED: screenshot @ 2021-04-01 13:23:20
[[file:images/Relational_Algebra/2021-04-01_13-23-20_screenshot.png]]

- RESULT will now contain the name of female employees and their dependents
This can be explained more here

#+DOWNLOADED: screenshot @ 2021-04-01 13:26:46
[[file:images/Relational_Algebra/2021-04-01_13-26-46_screenshot.png]]

#+DOWNLOADED: screenshot @ 2021-04-01 13:26:53
[[file:images/Relational_Algebra/2021-04-01_13-26-53_screenshot.png]]


#+DOWNLOADED: screenshot @ 2021-04-01 13:29:25
[[file:images/Relational_Algebra/2021-04-01_13-29-25_screenshot.png]]

- The CARTESIAN PRODUCT creates tupels with the combined attributes of two
  relations
- We can SELECT related tuples only from the two relations by specifying an
  appropriate selection condition after the Cartesian product

** Binary Relational Operations: JOIN
- JOIN opreatoins (denoted by )
  - The sequence of CARTESIAN PRODUCT followed by SELECT is used quite commonly
    to identify and selected related tuples from two relations
  - A special operation, called JOIN combines this sequence into a single
    operation 

#+DOWNLOADED: screenshot @ 2021-04-01 13:34:14
[[file:images/Relational_Algebra/2021-04-01_13-34-14_screenshot.png]]

- This operatoin is very important for any relational database with more than a
  single relation, because it allows us to /combine related tuples/ from various
  relations

- The general form of a join operation on two relations R(A1, A2, ..., An) and
  S(B1, B2, ..., Bm) is

#+DOWNLOADED: screenshot @ 2021-04-01 13:35:29
[[file:images/Relational_Algebra/2021-04-01_13-35-29_screenshot.png]]
- where R and S can be any relations that result from /relational algebra
  expressions/
- For example: Suppose that we want to retrieve the name of the manager of each
  department
  - To get the manager's name, we need to combine each DEPARTMENT tuple with the
    EMPLOYEE tuple whose SSN value matches the Mgr_ssn value in the
    department tuple
    - We do this by using the join operation

#+DOWNLOADED: screenshot @ 2021-04-01 13:38:19
[[file:images/Relational_Algebra/2021-04-01_13-38-19_screenshot.png]]
- Mgr_ssn is the join condition
  - Combines each department record with the employee who manages the department
  - The join condition can also be specified as:
    - DEPARTMENT.Mgr_ssn = EMPLOYEE.Ssn

#+DOWNLOADED: screenshot @ 2021-04-01 13:39:15
[[file:images/Relational_Algebra/2021-04-01_13-39-15_screenshot.png]]

*** Some properties of JOIN
- Consider the following JOIN operation

#+DOWNLOADED: screenshot @ 2021-04-01 13:41:31
[[file:images/Relational_Algebra/2021-04-01_13-41-31_screenshot.png]]
- Result is a relation Q with degree n + m attributes:
  - Q(A1, A2, ..., An, B1, B2, ..., Bm), in that order
- The resulting relation state hasone tuple for each combination of tuples: r
  from R and S from S, /but only if they satisfy the join condition/ r[Ai] = s[Bj]
- Hence if R has nR tuples, and S has nS tuples then the join result will
  generally have /less/ than nR * nS tuples
- Only related tuples (based on the join condition) will appear on the result

- The general case of JOIN operation is called a *THETA JOIN*

#+DOWNLOADED: screenshot @ 2021-04-01 13:44:19
[[file:images/Relational_Algebra/2021-04-01_13-44-19_screenshot.png]]

- The join condition is of the form A_i  B_j
  - Where Ai is an attribute of R and Bj is an attribute of S
  - Ai and Bj have the same domain and
  -  (theta) is one of the comparison operators { =, <, <=, >, >=. != }
  - For example
    - R.Ai < S.Bj AND (R.Ak = S.Bi OR R.Ap <K S.Bq)
- Most join conditions involve one or more equality conditions 'AND'ed together,
  eg
  - R.Ai=S.Bj AND R.Ak=S.Bl AND R.Ap=S.Bq

** Binary Relation Operations: EQUIJOIN
- *EQUIJION* Operation
- The most common use of join involves join condition with /equality/
  comparisions '=' only
- Such a join, where the only comparison operator used is = is called an
  *EQUIJOIN*
  - In the result of an EQUIJOIN we always have one or more pairs of attributes
    (whose names need not be identical) that have identical values in every
    tuple

#+DOWNLOADED: screenshot @ 2021-04-01 13:53:59
[[file:images/Relational_Algebra/2021-04-01_13-53-59_screenshot.png]]
- The JOIN seen in the previous examples were all EQUIJOINs
** Binary Relational Operations: NATURAL JOIN
- *NATURAL JOIN* Operation
  - Another variation of JOIN called NATURAL JOIN - denoted by * - was created
    to get rid of the second (superfluous) attribute in an EQUIJOIN condition
    - Because one of each pair of attributes with identical values is
      superfluous
  - The standard definition of natural join requires that the two join
    attributes, or each pair of corresponding join attributes, /have the same
    name/ in both relations
  - If this is not the case, a renaming operation is applied first
- Example: To apply a natural join on the Dnumber attributes of DEPARTMENT and
  DEPT_LOCATIONS, it is sufficient to write
  - DEPT_LOCS <- DEPARTMENT * DEPT_LOCATIONS
  - Only attribute with the name is Dnumber
  - An implicit join condition is created based on this attribute:
    DEPARTMENT. DNumber - DEPT_LOCATION.Dnumber

Another example: Q <- R(A, B, C, D) * S(C, D, E)
- The implicit join condition includes /each pair/ of attributes with the same
  name, "AND"ed together:
- Implicit join condition: R.C = S.C AND R.D = S.D
- Result keeps only one attribute of each such pair: Q(A, B, C, D, E)

*** Example of NATURAL JOIN operation

#+DOWNLOADED: screenshot @ 2021-04-01 14:04:53
[[file:images/Relational_Algebra/2021-04-01_14-04-53_screenshot.png]]

** Complete Set of Relational Operations
- The set of operations, including SELECT 

* Additional Relational Operations
** Query Tree Notation
- An internal data structure to represent a query
- Standard technique for estimating the work involved in executing the query,
  the generation of intermediate results, and the optimization of execution
- Nodes stand for operations like selection, projection, join, renaming,
  division, and so on.
- Leaf nodes represent base relations
- A tree gives a good visualization of the complexity of the query and the
  operations involved
- Algebraic query Optimization consists of rewriting the query or modifying the
  query tree into an equivalent tree
** Example of Query Tree 
- Example of Q2: For every project located in "stafford", list the project
  number, the controlling department, and the department managers last name,
  address and birthdate.


#+DOWNLOADED: screenshot @ 2021-04-19 08:40:28
[[file:images/Additional_Relational_Operations/2021-04-19_08-40-28_screenshot.png]]

** Additional Relational Operations: Aggregate Functions and Grouping
- A type of request that cannot be expressed in the basic realtional algebra is
  to specify mathematical *aggregate functions* on collections of values from
  the database
- Examples of such functions include retrieving the average or total slary of
  all employees or the total number of employee typles
  - These functions are used in simple statistical queries that summarize
    information from the database tuples
- Common functions applied to collections of numeric values include:
  - SUM, AVERAGE, MAXIMUM, and MINIMUM
- The COUNT function is used for counting tuples or values

** Aggregate function Operation

#+DOWNLOADED: screenshot @ 2021-04-19 08:45:50
[[file:images/Additional_Relational_Operations/2021-04-19_08-45-50_screenshot.png]]

*** Using Grouping with Aggregation
- The previous examples all summarized one or more attributes for a set of
  tuples
  - Maximum Salary of Count (number of) Ssn
- Grouping can be combined with Aggregate functions
- Example: For each department, retrieve the Dno, COUNT Ssn, and AVERAGE Salary
- A variation of aggregate operation  allows this:
  - Grouping attribute placed to left of symbol
  - Aggregate functions to the right of symbol

#+DOWNLOADED: screenshot @ 2021-04-19 09:00:10
[[file:images/Additional_Relational_Operations/2021-04-19_09-00-10_screenshot.png]]
- Above operation groups employees by Dno (department number) and computes the
  count of employees and average salary per department

** The aggregate function operation

#+DOWNLOADED: screenshot @ 2021-04-19 09:01:15
[[file:images/Additional_Relational_Operations/2021-04-19_09-01-15_screenshot.png]]

** Additional Relational Operations
- The OUTER JOIN Operation
  - In NATURAL JOIN and EQUIJOIN, tules without a /matching/ (or related) tuples
    are eliminated from the join result
    - Tuples with null in the join attributes are also eliminated
    - The amounts to loss of information
  - A set of operations, called OUTER joins, can be used when we want to keep
    all the tuples in R, or all those in S, or all those in both relations in
    the results of the join, regardless of whether or not they have matching
    tuples in teh other relation
- The LEFT OUTER JOIN operation keeps _every tuple_ in the first or left
  relation R in R  S; if no matching tuple is found in S, then the attributes
  of S in the JON result are filled or "padded" with null values.

#+DOWNLOADED: screenshot @ 2021-04-19 09:12:02
[[file:images/Additional_Relational_Operations/2021-04-19_09-12-02_screenshot.png]]

- A similar operation, right outer join, keeps every tuple in the second or
  right relation S in the result of R  S.
- A third operation, full outer join, denoted by R  S keeps all tuples _in both
  the left and the right relations_ when no matching tuples are found, padding
  them with null values as needed

*** Examples

#+DOWNLOADED: screenshot @ 2021-04-19 09:13:41
[[file:images/Additional_Relational_Operations/2021-04-19_09-13-41_screenshot.png]]

* Informal Design Guidelines for Relational Databases
We will go through some information methods to understand if our databases are
good

- What is relational database design?
  - The grouping of attributes to form "good" relation schemas
- Two levels of relation schemas
  - The logical "user view" level
  - The storage "base relation" level
- Design is concerned mainly with base relations
- What are the criteria for "good" base relations

** Semantics of the Relational Attributes must be clear
- GUIDELINE 1: Informally, each tuple in a relation should represent one entity
  or relationship instrance. (applies to individual relations and their
  attrubtes)
  - Attributes of different entities (EMPLOYEEs, DEPARTMENTs, PROJECTs) should not
    be mixed in the same relation
  - Only foreign keys should be used to refer to other entities
  - Entity and relationship attributes should be kept apart as much as possible
- _Bottom Line_: Design a schema that can be explained easily relation by
  relation. The semantics of attributes should be easy to interpret

** A simplified COMPANY relational database schema and sample state

#+DOWNLOADED: screenshot @ 2021-04-22 12:21:41
[[file:images/Informal_Design_Guidelines_for_Relational_Databases/2021-04-22_12-21-41_screenshot.png]]

** Redundant Information in Tuples and Update Anomalies
- Information is stored redundantly
  - Wastes storage
  - Causes problems with update anomalies
    - Insertion anomalies
    - Deletion anomalies
    - Modification anomalies
Most importantly it causes anomalies. The example below

*** Two relation schemas suffering from update anomalies

#+DOWNLOADED: screenshot @ 2021-04-22 12:22:54
[[file:images/Informal_Design_Guidelines_for_Relational_Databases/2021-04-22_12-22-54_screenshot.png]]

** Example of on update anomaly
- Consider the relation:
  - EMPY_PROJ(Emp#, Proj#, Ename Pname, No_hours)
- Update Anomaly:
  - Changing the name of project number P1 from "Billing" to
    "Customer-Accounting" may cause this update to be made for all 100 employees
    working on project P1.
** Example of an inset anomaly
- Consider the relation
  - EMP_PROJ(Emp#, Proj#, Ename, Pname, No_hours)
- Insert Anomaly:
  - Cannot insert a project unless an employee is assigned to it
  - Conversely we cannot insert an employee unless he/she is assigned to a
    project
** Example of a delete anomaly
- Consider the relation:
  - EMP_PROJ(Emp#, Proj#, Ename, Pname, No_Hours)
- Delete anomaly:
  - When a project is deleted, it will result in deleting all the employees who
    work on that project
  - Alternately, if an employee is the sole employee on a project, deleting
    that employee would result in deleting the corresponding project
** Guideline fo Redundant Information in Tuples and Update Anomalies
- GUIDELINE 2:
  - Design a schema that does not suffer form the insertion, deletion and update
    anomalies
  - If there are any anomalies present, then note them so that applications can
    be made to take them into account
** Null values in tuples
- GUIDELINE 3
  - Relations should be designed such that their tuples wil have as few NULL
    values as possible
  - Attributes that are NULL frequently could be placed in separate relations
    (with the primary key)
- Reasons for nulls:
  - Attribute not applicable or invalid
  - Attribute value unknown (may exist)
  - Value know to exist, but unavailable
** Generation of Spruios tuples (avoid!)
- Bad designs for a relational database may result in erroneous results for
  certain JOIN operations
- GUIDELINE 4:
  - Design relation schemas so that they can be joined with equality conditions
    on attributes that are appropriately related (primary key, foreign key)
    pairs in a awy that guarantee that no spurious tuples are generated
  - Avoid relations that contain matching attributes that are not (foreign key,
    primary key) combinations because joining on such attributes may project
    spurious tuples
** Summary of problematic relation schemas
- Anomalies that cause redundant work to be done during insertion into and
  modification of a relation, and that may cause accidental loss of information
  during a deletion from a relation
- Waste of storage space due to NULLs and the difficulty of performing
  selections, aggregation operations and joins due to NULL values
- Generation of invalid and spurious data during joins on base relations with
  matched attributes that may not represent a proper (foreign key, primary key)
  relationship
** Functional Dependencies
- Functional dependencies (FDs)
  - Are used to specify /formal measures/ of the quality ("goodness") of
    relational designs
  - And keys are used to define *normal forms* for relations
  - Are *constraints* that are derived from the /meaning/ and
    /interrelationships/ of the data attributes
- A set of attributes X /functionally determines/ a set of attributes Y, if the
  values of X determines a unique value for Y
*** Defining Functional Dependencies
- X -> Y holds if whenever two tuples have the same value for X, they /must/
  have the same value for Y
  - For any two tuples t1 and t2 in any relation instance r(R): if t1[X] =
    t2[X}, then t1[Y] = t2[Y]
- X -> Y in R specifies a /constraint/ on all relation instances r(R)
- Written as X-> which can be displayed graphically on a relation schema as
  denoted by the arrow

#+DOWNLOADED: screenshot @ 2021-04-25 21:11:18
[[file:images/Informal_Design_Guidelines_for_Relational_Databases/2021-04-25_21-11-18_screenshot.png]]
- FDs are derived from the real-world constraints on the attributes
*** Examples of FD constraints
- Social security number determines employee name
  - Ssn -> Ename
- Project number determines project name and location
  - Pnumber -> {Pname, Plocation}
- Employee ssn and project number determines the hours per week that the
  employee works on the project
  - {Ssn, Pnumber} -> Hours
- A functional dependency is a property of the attributes in the relation schema
  R
- The constrain must hold on /every/ relation instance of r(R)
- If K is a key of R, then K functionally determines all attributes in R
  - Since we NEVER have two distinct tuples iwth t1[K] = t2[K]

*** Defining FDs from instances
- Note that in order to define the FDs, we need to understand the meaning of the
  attrubtes involved and the relationship between them
- FD is a property of the attributes in the schema R
- Given the instance (population) of a relation all we can conclude is that an
  FD _may exist_ between certain attributes
- What we can definitely conclude is
  - that certain FDs _do not exist_ because there are tuples that show a
    violation of those dependencies
*** Rulling Out FDs
Note that given the state of TEACH relation we can say that FD: Text -> Couse
may exist. However the FDs Teacher -> Course, Teacher -> Text and Course -> Text
are ruled out

#+DOWNLOADED: screenshot @ 2021-04-25 21:22:26
[[file:images/Informal_Design_Guidelines_for_Relational_Databases/2021-04-25_21-22-26_screenshot.png]]
* Normalization of Relations
- Normalization of data:
  - A process of analyzing the given relation schemas based on their FDs and
    primary keys to achieve the desired properties of:
    - Minimizing redundancy
    - Minimizing the insertion, deletion and update anomalies
- Normal form:
  - Condition using keys and DFs of a relation to certify whether a realtion
    schema is in a particular form
  - Refers to the highest normal form condition that it meets, indicate the
    degree of normalization
  - Note that there is an ordering. For something to be in 3NF it has to be in
    2NF and 1NF
- 2NF, 3NF, BCNF
  - Based on keys and Fds of a relation schema
- 4NF
  - Based on keys, multi-valued dependencies (MVD)s
- 5NF
  - Based on keys, join dependencies (JD)s
- Additional properties may be needed to ensure a good relational design
  (lossless join, dependency preservation)
** Additional properties
- There are two important properties of normalization through decomposition
  - *Non-additive join* (or *lossless join*) property guarantees that the
    spurious tuples generation problem does not occue with respect to the
    relation schema after decomposition
    - Is extremely important and _canot_ be sacrificed
  - *Dependency preservation* property ensures that each functional dependency
    is represented in some individual relation resulting after the decomposition
    - Is less stringent and may be sacrificed
** Practical Use of Normal Forms
- *Normalization* is carried out in practice so that the resulting designs are
  of high quality and meet the desirable properties
- The practical utility of higher normal forms (eg, 4NF, and 5NF) becomes
  questionable when the constraints on which they are based are /hard to
  understand/ or to /detect/
- The database designers /need not/ normalize to the highest possible normal
  form
  - Usually up to 3NF and BCNF (Boyce-Codd NF), 4NF rarely used in practice
  - Sometimes, relations may be lef tin lower normalization status (eg: 2NF) for
    performance reasons
- *Denormalization*:
  - The process of storing the join of higher normal form relations as a base
    realtion which is in a lower normal form
** Definitions of Keys and Attributes Participating in Keys
- A *superkey* of relation schema R = {A1, A2, ..., An} is a set of attributes S
  /subset-of/ R with the property that NO two tuples t1 ans t2 in any legal
  relation state r of R will have t1[S] = t2[S]
- A *key* K is a *superkey* with the /additional property/ that removal of any
  attribute from K will cause K not to be a superkey any more
- If a relation schema has more than on key, each is called a *candidate* key
  - One of the candidate keys is /arbitrarily/ designated to be the *primary
    key*, and the others are called *secondary keys*
- An attribute of relation R is called a *Prime attribute* of R, if it is a
  member of /some/ candidate key or R
- A *Nonprime attribute* is not a prime attribute: that is, it is not a member of
  any candidate key
** First Normal Form
- First Normal Form (1NF) is considered to be part of the formal definition of a
  relation in basic (flat) relation model. It was defined to disallow:
  - multivalued attributes
  - composite attributes
  - their combinations, ie, *nested relations*: attributes whose values for an
    /individual tuple/ are non atomic
- The only attribute values permitted by 1NF are single atomic (or indivisible)
  values
- Most RDBMSs allow only those relations to be defined that are in First Normal
  Form (1NF) anyways because SQL systems don't allow for multivalued attributes
*** Normalization into 1NF

#+DOWNLOADED: screenshot @ 2021-04-26 10:37:14
[[file:images/Normalization_of_Relations/2021-04-26_10-37-14_screenshot.png]]

- c) isn't really that good because it wastes space and causes various anomalies

** Normalizing into 1NF
- Three solution to achieve 1NF of the previous example:
  - Remove the attribute Dlocation that violates 1NF and place it in a separate
    relation DEPT_LOCATIONS along with the primary key Dnumber of the DEPARTMENT
  - Expand the key to (Dnumber, Dlocation) so that there will be a separate
    tuple in the original DEPARTMENT relation for each location of a DEPARTMENT
  - If a maximum number of values is known for the attribute, eg: 3, replace the
    Dlocation attribute by tree atomic attributes: Dlocation1, Dlocation2,
    Dlocation3
- The first solution is the best, because it does not suffer from redundancy and
  it is more generic
- It also doesn't suffer from null values like solution 3
** Second Normal Form
- Uses the concepts of *Functional Dependencies (FDs), primary key*
- Definitions
  - *Prime attribute*: An attribute that is member of the primary key K
  - *Full functional dependency*: A FD X -> Y where removal of any attribute
    from X means that the FD does not hold any more
- Examples:
  - {SSn, Pnumber} -> Hours is a full FD since neither Ssn -> Hours nor Pnumber
    -> hours hold
  - {SSn, Pnumber} -> Ename is not a full FD (is is called a partial dependency_
    since SSn -> Ename also holds
- *Definition*: A relation schema R is in second normal form (2NF) is every
  non-prime attribute A in R if *fully functionally dependent* on the primary
  key
- The test for 2NF involves testing for functional dependencies whose left hand
  side attributes are part of the primary key (or the full primary key)
- If a relation schema R is not in 2NF, it can be "second normalized" (or 2NF
  normalized) into a number of 2NF relations
  - Nonprime attributes are associated only with the part of the original
    primary key on which they are fully functional dependent in the decomposed
    2NF relations


#+DOWNLOADED: screenshot @ 2021-04-26 10:49:15
[[file:images/Normalization_of_Relations/2021-04-26_10-49-15_screenshot.png]]

- You can see that X ({Ssn, Pnumber}) don't determine all of the non prime
  attributes, for example Ssn -> Hours. Thus not in 2NF
** Third Normal Form
- A functional dependency X -> Y in relation schema R is a *transitive
  dependency* if there exists a set of attributes Z in R that is neither a
  candidate key nor a subset of any key of R, and both X -> Z and Z -> Y hold
- Examples:
  - Ssn -> Dmgr_snns is a *transitive* FD
    - Since Ssn -> Dnumber of Dnumber -> Dmgr_ssn hold
    - Ssn -> Ename is *non-transitive*
      - Since there is no set of attributes X where Ssn -> X and X -> Ename
- *Definition* A relation schema R is in *third normal form (3NF) if it
  satisfied 2NF /and/ has no non-prime attribute of R is transitively dependent
  on the primary key
- R can be decomposed into 3NF realtions via the process of 3NF normalization
- Note:
  - In X -> Y and Y -> Z, with X as the primary key, we consider this a problem
    on if Y is not a candidate key
  - When Y is a candidate key, there is no problem with the transitive
    dependency
  - Eg: Consider EMP(Ssn, Emp#, Salary)
    - Here Ssn -> Emp# -> Salary and Emp# is a candidate key
*** Normalizing into 3NF

#+DOWNLOADED: screenshot @ 2021-04-26 10:55:59

[[file:images/Normalization_of_Relations/2021-04-26_10-55-59_screenshot.png]]
- {Dnumber} -> {Dname, Dmgr_ssn}
** Summary

#+DOWNLOADED: screenshot @ 2021-04-26 10:56:16
[[file:images/Normalization_of_Relations/2021-04-26_10-56-16_screenshot.png]]

** Exercises
* General Normal Form Definitions (For Multiple Keys)
- Normal forms defined informally
  - 1NF: All attributes depends on the key
  - 2NF: All attributes depend on the whole key
  - 3NF: all attributes depend on nothing but the key
- The above definitions consider the primary key only
- The following more general definitions take into account relations with
  multiple candidate keys
- And attribute involved in a candidate key is a _prime attribute_
- All other attributes are called _non-prime attributes+
- A relation schema R is in *second normal form (2NF) if every non-prime
  attribute A in R is fully functionally dependent on /every/ key of R

#+DOWNLOADED: screenshot @ 2021-04-30 10:35:22
[[file:images/General_Normal_Form_Definitions_(For_Multiple_Keys)/2021-04-30_10-35-22_screenshot.png]]


** General Definition of Third Normal Form
- *Superkey* of a relation schema R - a set of attributes S or R that contains a
  key of R
- A relation schema R is in *third normal form (3NF)* if whenever a nontrivial
  function dependency X -> A holds in R then either:
- a) X is a superkey of R, *or*
- b) A is a prime attribute of R


#+DOWNLOADED: screenshot @ 2021-04-30 10:39:54
[[file:images/General_Normal_Form_Definitions_(For_Multiple_Keys)/2021-04-30_10-39-54_screenshot.png]]

** Interpreting the General Definition of Thrid Nromal Form
- A relation schema R violates the general definition of 3NF if whenever a FD X
  -> A holds in R that meets either of the two conditions (a) X is a superkey of
  R and (b) A is a prime attribute of R
- Condition (a) catches two types of violates:
1. A non prime attribute functionally determines another non-prime
   attribute. This catches 3NF violates due to a transitive dependency
2. A proper subset of a key of R functionally determines a non-prime
   attribute. This catches 2NF violations due to non-full functional
   dependencies
*** Alternative definition
- *ALTERNATIVE DEFINITION of 3NF*: We can restate the definition as:
  - A relation schema R is in *third normal form (3NF)* if every non-prime
    attribute in R meets both of these conditions
    - It is fully functionall dependent on every key of R
    - It is non-transitively dependent on every key of R
    - Note that stated this way, a relation in 3NF also meets the requirements
      of 2NF
  - The condition (b) from the previous slide takes care of the dependencies
    that "slip through" (are allowable to) 3NF but are "caught by" BCNF which we
    discuss next
** Boyce-Codd Normal Form (BCNF)
- A relation schema R is in *Boyce-Codd Normal Form (BCNF)* if whenever a
  nontrivial functional dependency *X -> A* holds in R, then *X is a superkey of
  R*
  - Every 2NF relation is in 1NF
  - Every 3NF relation is in 2NF
  - Every BCNF relation is in 3NF
- There exist relations that are in 3NF but not in BCNF
- The goal is to have each relation in BCNF (or 3NF)

#+DOWNLOADED: screenshot @ 2021-04-30 10:57:48
[[file:images/General_Normal_Form_Definitions_(For_Multiple_Keys)/2021-04-30_10-57-48_screenshot.png]]

** A relation TEACH that is in 3NF but not in BCNF
- Two FDs  exist in the relation TEACH:
  - FD1: {Student, Course} -> Instructor
  - FD2: Instructor -> Course
- {Student, Course} is a candidate key for this relation
- So the relation is in 3NF /but not/ in BCNF
- A relation NOT in BCNF should be decomposed so as to meet this property, while
  possibly forgoing the preservation of all functional dependencies in the
  decomposed relations

* Transaction Processing
How to perform composite updates safely for a remotely and concurrently accessed
system

** Motivating Example: Flight booking
- An online multi-leg flight booking system (web or app).
- Users submit their orders and expect them to be:
  - Processed as /quickly as possible/, also meaning that the user gets quick
    feedback on success/no success
  - /Exactly/ as requested, in particularly /complete/:
    - Book all legs of the flight, or do nothing
- Example problem Atomicity:
  - My system or the server or the network can fail
  - But I want my order done as a whole
- The solutions are discussed in transaction processing

** Atomicity
Another Example
- In a transfer transaction from accoutn 123 to account 321:
  - Withdraw $100 from account 123
  - put $100 on account 321
- It must not happen that the transaction stops after the first step and makes
  the first step persistent (durable) and never executes the second step
- Why not? This would violate an application level consistency constraint:
  - Overall balance must be kept

** Transaction Demarcation for Atomicity
- Example problem Atomicity
  - My system or the server or the network cna fail
  - But I want my order done as a whole
- Needs *transaction demarcation*:
  - System must know:
    - When does one user order (start and) *end*:
    - Operation to signal end of transaction: *commit()*
  - *Transaction:*: A sequence of operations that form a logical unit; sequence
    of operations delimited by TA demarcation
- Transaction script: A subpogramm issuing a transaction if called

** Typical architecture for use of databases

#+DOWNLOADED: screenshot @ 2021-05-03 13:15:50
[[file:images/Transaction_Processing/2021-05-03_13-15-50_screenshot.png]]

** Rollback for atomicity
1. The client needs to issue a commit() request. Commit must be requested by
   client: Only if commit has been requested, writes in the transaction may
   become durable. This protects against failures that lead to incomplete
   transaction processing. Hence *Database must take care of the rollback in
   case no commit has been registered*
1. b. Client can request abort as a programming feature
2. Atomicity: Either all writes of the transaction or none become durable: If
   any write becomes durable, all must become durable

** How a transaction script may look like
- (not referring to a specific database technology)

#+DOWNLOADED: screenshot @ 2021-05-03 13:18:49
[[file:images/Transaction_Processing/2021-05-03_13-18-49_screenshot.png]]

** Atomicity requires rollback
- For atomicity, the database must be able to undo write operations of
  uncommitted transactions
- We consider here strategies that work with a *log*
- The log is a list of records for write operations in the order of execution
  - Optionally with some data structures
  - For easier access
- The log will be also important for recovery and ensuring durability and will
  contain more information than we immediately need for rollback in the context
  of Atomicity

** Database internal architecture

#+DOWNLOADED: screenshot @ 2021-05-03 13:21:04
[[file:images/Transaction_Processing/2021-05-03_13-21-04_screenshot.png]]

- Keeps a copy in memory and a copy on disk (durable and non durable)

** The Database Log
- Is a central feature of typical database managers
- The log contains a list of
  - The write operations performed
  - The transaction demarcations: (BOT), abort, commit
- We consider undo/redo logs: For each write operation, a log entry is written
  that contains:
  - A log sequence number of LSN (denoted as "nr:")
  - The transaction ID of the executing transaction ("ta:")
  - An identifier of the object affected ("obj:")
  - The value before the write operation (before-image, "b:")
  - The value after the write operation (after image, "a:")

** Example log and database
- The log of the database and the database buffer at a certain point in time
  could look the following way (new operatoins are appended at lower end)
- Transaction demarcations contain only LSN and TA

#+DOWNLOADED: screenshot @ 2021-05-03 13:25:53
[[file:images/Transaction_Processing/2021-05-03_13-25-53_screenshot.png]]

** Rollback
- A transaction that is aborted has to be undone in a *rollback*
- Rollback is the undoing of a transactoin during normal operation (also known
  as transaction recovery, but we do not use this term)
- This rollback is based on the log
  - An abort record is entered into the log
- For all write operations performed so far
  - The before image is restored

*** Example: rollback of TA23

#+DOWNLOADED: screenshot @ 2021-05-03 13:27:47
[[file:images/Transaction_Processing/2021-05-03_13-27-47_screenshot.png]]

** ACID atomicity
1. Only if the client has requested commit, write operations in the transaction
   may become durable.
   Atomicity 1 protects against client failures that lead to incomplete
   transaction processing:
   Hence: *Database must take care of the rollback in case of abort*
1. b. client can request abort as a programming feature
2. Either all writes of the transaction, or none become durable: If any write
   becomes durable, all must become durable

** ACID consistency (high level)
a) Is referring to additional high level features of the DB (and it's not part
of the basic transactoin model we will use) Declarative integrity constraints,
such as referential integrity, must hold between transactions
b) During the transaction, certain integrity constraints might be violated, but
on commit, they must be fulfilled
 - by explicit operations in the transactions,
 - By automatics mechanisms ( ON DELETE CASCADE),
 - By user defined triggers
Any transaction still violating constraints will be aborted

** ACID Isolation
- Database operations in a transaction appear isolated from database operations
  of all other transactions
  - Does not apply to non database operations of scripts
- Isolation is expensive:
  - May lead to delays, deadlocks, and/or aborts
  - And can therefor be relaxed: *Isolation levels*
- Highest isolation level:
- All commited transactions must have the same effect as the execution of the
  transactions with DB-wide mutual exclusive access (MutEx) in some order:
  *Serializability*
- Transactions must have same view as with DB wide MutEx

** ACID durability
- Once the user has been notified of success of transaction t:
a) All write of t are permanent: must be changed through lasted commited writes,
eg: Compensating transactions
b) The effect of t is kept in a crash resistant way
 - Minimum requirement: transaction is written to persistent storage: resistant
   against
   - Os crash
   - System outage
c) preferred: protection against loss of persistent memory:
 1. Resistance against persistent storage failure
 2. Resistance against catastrophes, geographic distribution
* Into to ACID Isolation
** ACID atomicity
1. Only if the client has requested commit, write operations in the transactoin
   my become durable. Atomicity protects against failures that lead to incomplete
   transaction processing: Hence: *Database must take care of the rollback in
   the case of abort*
1b. client can request abort as a programming feature
2. Either all writes of the transaction, or none become durable: If any write
   becomes durable, all must become durable.
** Example problem 2: Concurrency
- An online multi-leg flight booking system (web or app)
- User submits their orders and expect them to be:
  - Processed as quickly as possible, also meaning that the user gets quick
    feedback on success/ no success
  - Exactly as requested, in particularly complete:
    - Book all legs of the flight, or do nothing.
- Problem *Concurrency*: different clients might interfere
  - System checked that all legs were available
  - But while booking: first flight ok, last flight is full.

** Solution Proposal
- Simple *Solution* proposal for Concurrency Problem:
  - *DB-wide Mutual exclusion*:
    - User orders are executed one at a time
    - One exclusive lock for the whole database
    - One transaction gets this lock at a time
- System again needs to know when user order ends: commit.
- Must know when the next user's order can start
- Needs again transaction demarcation!
  - *Transaction*: A sequence of operations that form a logical unit; sequence
    of operations delimited by TA demarcation
- In mutual exclusion, Transactions can get delayed
** Analysis of Solution Proposal
- *Scalability limits:* DB-wide Mutual exclusion puts a strict limit on the
  *throughput* of the system:
  - DB-wide MutEx *serializes* all transaction attempts
  - *transaction script* needs all parameters beforehand, does not talk to human
    operator
  - Assume for sake of argument: each transaction takes on average 0.04 seconds
    to process on current hardware:
  - Average number of transactions per second for MutEx: 25
** Case against DB-wide MutEx: data disjoin TAs
- *Conflict object*: a data object accessed by more than one transaction in a
  set
- A set of transactoins is *data disjoint*, if it has no conflict object
- Thought experiment: If concurrent transactions *access* random data in a huge
  database,
  - Then often they are data disjoint
- Also, if transactions are concerned with unrelated matters:
  - They are probably data disjoint

#+DOWNLOADED: screenshot @ 2021-05-06 15:23:36
[[file:images/Into_to_ACID_Isolation/2021-05-06_15-23-36_screenshot.png]]

** Disjoint-access parallelism (ADP)
- Conflict object: a data object accessed by more than one transaction in a set
- A set of transactions is data disjoint, if it has no conflict object
- Data disjoint transactions should not delay each other:
  - Disjoint-access parallelism.(DAP)

** DB-wide MutEx bad for data disjoint TAs
- Principle: data disjoint transactoins do not interfere
- Hence should not be delayed
- But DB-wide MutEx does not delay them:
- DB-wide MutEx is too blunt a tool:
- So: how to detect whether transactions are data

#+DOWNLOADED: screenshot @ 2021-05-06 15:30:40
[[file:images/Into_to_ACID_Isolation/2021-05-06_15-30-40_screenshot.png]]

** Implicit TM, continuous feedback
- *Implicit transaction  processing*: the script has to give no further locking
  commands beyond transactoin demarcation:
  - Eg: names second leg of flight only after checking first
- Consequence: scripts don't need to care whether anything else than DB-wide
  MutEx is applied
- *Continuous feedback*: The transaction script gets feedback from the Db after
  individual operations (even smaller timescale than what Online TP requires)
- *Note*: Implicit Online TP, continuous feedback, fits to language independence
  and a dialogue-session style interface
** Fist Solution simple scheduler
- Idea:
- The /simple scheduler/ uses *MutEx per data object*, aka one exclusive lock
  per object
- When to set lock?
  - Scheduler knows only at the first access of x by TA1:
- So TA1 gets excl. lock on x with its first operation on x:
  - latest time allowed, first time possible (implicit OLTP)
- Release all locks held by one transaction at end of transaction (commit/abort
  time)
- If transactions wait in a cycle for each other to release locks: deadlock,
  abort one transaction in the cycle

** Rigorous two phase locking (R2PL)
- The simple scheduler as described uses:
- /Rigorous two phase locking (R2PL)/:
- Transactions acquire locks during their lifetime (first phase)
- When should one transaction release any lock?
- We do not want to require further info from clients (implicit TP);
- we have only agreed on transaction demarcation:
- So release all locks at end of transaction

** Rigorous two phase locking (R2PL)
- The simple scheduler as described uses:
- Rigorous two phase locking (R2Pl):
- Transactions scquire locks during their lifetime (first phase)
- They release all locks immediately after commit/abort, but not earlier (second
  phase)
- R2Pl used by most lock-based schedulers
- No explicit lock command necessary (only commit): no break of abstraction for
  the programmers writting transactions
- R2PL avoids cascading aborts in case of errors

#+DOWNLOADED: screenshot @ 2021-05-06 15:42:38
[[file:images/Into_to_ACID_Isolation/2021-05-06_15-42-38_screenshot.png]]
** Simple scheduler and data disjoint TAs
- The simple scheduler does not delay data disjoint TAs
- *Theorem*: For the simple scheduler:
  - If a transaction T is data disjoint with all other transactions open during
    its lifetime (fro Beginning of Transaction of T to its commit/abort)
  - Then the simple scheduler does not delay this transaction
- *Proof*:
  - The transactoin does ont encounter any lock by another transaction
Result: The simple scheduler is *much* better than DB-wide mutual exclusion

** Deadlock resolution
- If transactions wait in a cycle on each other (deadlock) abort one transaction
  in the cycle
- We make use of the rollback mechanism that we already have defined
- The transaction under rollback *keeps the locks* until the rollback is
  completed
- The transaction to pick for abort may have to be chosen judiciously: we need
  to prevent starvation: that one transaction script always gets its
  transactions aborted
- Deadlocks can be reduced by following patterns, eg: naming a preferred order
  to access objects: if everyone follows the order, then no deadlocks appear!

** Retry loop for transaction script
- In best practice for transaction programming, the transaction script must
  always be embedded in a /retry loop/

#+DOWNLOADED: screenshot @ 2021-05-06 15:52:13
[[file:images/Into_to_ACID_Isolation/2021-05-06_15-52-13_screenshot.png]]
- All info for correct request answering must be collected beforehand as
  parameter for the request:
- Script will not make essential communication outside DB
- The loop can be implicitly given eg: by certain containers for scripts, or
  messaging systems that do the retry, but we have to make sure that it happens
  somewhere

** Transactions wait for locks
For a scheduler:
- If a transactions TA1 makes any access to a /locked/ object x and gets /blocked/:
- The scheduler does not answer to TA1 for now
- From the perspective of TA1. the call to access x simply takes long
  - TA1 does not have to care about scheduling
  - Best of all possible worlds, solve problem by doing nothing
- Transaction scripts are typically stricly sequential: TA1 while blocked,
  cannot execute any other operation

** Waiting queues for exclusive locks
For the simple scheduler:
- If a transaction TA1 gets blocked accessing x:
- The scheduler puts TA1 into the /waiting queue/ for x. each data object has
  one such queue
- The waiting queue is managed first-in-first-out (FIFO)
- Because of how blocking is defined, one transaction can be only in one queue
- If a lock is released, and any transaction are /waiting/, the scheduler
  dequeues one transaction (takes the longest waiting transaction out of the
  queue) and /grants/ the lock to this transaction, which is then /unblocked/

** Challenge Transaction processing:
- Reflection on what we have achieved
- Scalability limits: DB-wide Mutual exclusion user order puts a strict limit on
  the throughput of the system
- Challenge: Design a solution that offers clients mostly the same interface as
  for MutEx, but doesn not unnecessary delya transactions,
  - EG: does not delay data disjoint transactions

** Problem Statement: Transaction Processing
- *Define* which kind of interference between different transactions for
  different users is harmful
- *Design* a /transaction manager/, a system preventing such interference that
  is
  - /Problem independent/: We can handle arbitrary business logic with it (not
    just flight booking)
  - /Language independent/: Transaction scripts can be written in any language
    (that has a DB interface)
  - /Scalable/: no unecessary limit on the throughput (but may require more
    hardware), should not break
  - /Fast/: no unnecessary delay on transactions
** Challenge *Testing* Transaction processing
- DB-wide Mutual exclusion may still work for light system loads
- Big risk that inadvertent MutEx patterns in complex enterprise application
  code are not detected with testing, even with load testing if it too light
- We need realistic load testing!

** ACID Isolation
- Database operations in a transactoin appear isolated from database operations
  of all other transactions
  - Does not apply to non-database operations of scripts
- Isolation is expensive
  - May lead to delays, deadlocks, and/or aborts
  - and can therefor be relaxed: /isolation levels/
- Highest isolation level:
- Transactions must have same view as with DB-wide MutEx
All commiteted transactions must have the same effect as the execution of the
transactions with Mutual exclusion in some order: *Serializability*
* Schedules in the read/write model
- example: making a transfer between two accounts
  - i) withdraw $100 from account 123
  - ii) put $100 on account 321
- The operations in the transfer example:

#+DOWNLOADED: screenshot @ 2021-05-08 20:04:16
[[file:images/Schedules_in_the_read/write_model/2021-05-08_20-04-16_screenshot.png]]

** Linear schedules
- A (linear) schedule orders operations of a *set* of transactions in
  time. /Good for discussing isolation issues (serializability)/

#+DOWNLOADED: screenshot @ 2021-05-08 20:05:42
[[file:images/Schedules_in_the_read/write_model/2021-05-08_20-05-42_screenshot.png]]

** Schedulers produce schedules
- Since schedulers cannot execute operations before they are issued by the
  clients, schedulers delay operations

#+DOWNLOADED: screenshot @ 2021-05-08 20:06:25
[[file:images/Schedules_in_the_read/write_model/2021-05-08_20-06-25_screenshot.png]]
- The scheduling happens by waiting
- Local transactions are merged into a single /schedule/

** Example Deadlock for the simple scheduler
- Consider

#+DOWNLOADED: screenshot @ 2021-05-09 10:26:06
[[file:images/Schedules_in_the_read/write_model/2021-05-09_10-26-06_screenshot.png]]
- A deadlock is a cyclical wait-relation between transactions in a scheduler
- Deadlocks are detected by the scheduler (several approaches, eg: graph based
  detecting cycles in a waits- for graph)
- This is a deadlock because TA1 has locked x and then reads y, but TA2 has
  locked y (meaning TA1 halts). TA2 tries to read x but that's locked by TA1
  thus the two TAs are waiting for each other to finish but are both locked on
  one another
** Deadlock resolved
- Consider

#+DOWNLOADED: screenshot @ 2021-05-09 10:33:55
[[file:images/Schedules_in_the_read/write_model/2021-05-09_10-33-55_screenshot.png]]
- One transaction is aborted
- Its locks are released
- Here a2 means that we are aborting TA2 which frees the locks, then TA1 can
  proceed
** Data disjoint transactions
- *Conflict object*: a data object accessed by more than one transactions in a
  set
- A set of transactions T is a data disjoint if it has no conflict object
- Principle: Data disjoint transactions do not interfere
- *They always fulfil ACID isolation
- *Every schedule* for a set of *data-disjoint* transactions is *serializable*
- Hence should not be delayed

#+DOWNLOADED: screenshot @ 2021-05-09 10:39:17
[[file:images/Schedules_in_the_read/write_model/2021-05-09_10-39-17_screenshot.png]]

** Write-Disjoint Transactions
- For a set T of transactions, a conflict object x is a /write conflict object/
  if it is written by atleast one transactions from T
- A set of transactions is /write-disjoint/, if it has no write conflict objects
  (meaning that it can still have other conflict objects, aka /read conflict objects/
- *Every schedule* for a set of *write-disjoint* transactions is *serializable*
- They should not be delayed either

#+DOWNLOADED: screenshot @ 2021-05-09 10:42:26
[[file:images/Schedules_in_the_read/write_model/2021-05-09_10-42-26_screenshot.png]]
The above has no write conflict objects
** The simple scheduler is pessimistic
- The simple scheduler expects for every read, that it might be followed by a
  write

#+DOWNLOADED: screenshot @ 2021-05-09 10:43:10
[[file:images/Schedules_in_the_read/write_model/2021-05-09_10-43-10_screenshot.png]]
- Treats all types of conflicts the same way
- Meaning that if you're just reading then it's going to lock the object which
  isn't really necessary. We can do better than this
** Infor for transaction scheduling
- Keep a record of which *open* transactions accessed (or tried to do so) which
  object with which operation. Tag object with:
  - The operation performed (r,w)
  - The transaction ID (the index)

#+DOWNLOADED: screenshot @ 2021-05-09 10:46:07
[[file:images/Schedules_in_the_read/write_model/2021-05-09_10-46-07_screenshot.png]]
- Scheduling strategies can be seen as deciding access based on the tags on x,
  whether a next operation on x can proceed or needs to be blocked in order to
  avoid harm.
- Blocked operations with dotted lines

#+DOWNLOADED: screenshot @ 2021-05-09 10:47:04
[[file:images/Schedules_in_the_read/write_model/2021-05-09_10-47-04_screenshot.png]]

* Declarative definition of schedulers
- Simple scheduler: Block any operation that would cause the set of open
  transaction to be not *data disjoint* any more
- New scheduler:
  - /*Common*/ scheduler: Block any operation that would cause the set of open
    transaction to be not *write-djoint* any more
- Can all operations below pass in the common scheduler?

#+DOWNLOADED: screenshot @ 2021-05-09 10:53:46
[[file:images/Declarative_definition_of_schedulers/2021-05-09_10-53-46_screenshot.png]]
- Everything in the common scheduler passes above
The above defintion has double negations. We should fix this definition
** Declarative definition of schedulers
- *Simple* scheduler: Block any operation that would cause the set of open
  transaction to have a *conflict
- New Scheduler
  - Common scheduler: Block any operation that would cause the set of open
    transaction to have a *write-conflict*

** Operations tags are natural
- Schedulers are often defined directly with locks instead of tags
- Advantage of operation tags
- Their definition is /declarative/, not /imperative/:
  - We do *NOT* say *HOW* and when to place and remove locks (imperative)
  - We say *WHAT* the the operations should reflect: all opreations of ongoing
    transactions (declarative):
    - A natural concept
* The common scheduler, explained with locks
- Akin to schedulers used in practice
- Uses several kinds of locks:
  - *Read locks*, also known as shared locks (S locks)
    - Several transactions can have a read lock on the same object
    - An object with read locks can only be read, not written
    - Advantage: *Write disjoint transactions are not delayed*
  - Write locks, also known as exclusive locks (X locks)
    - If an object has a write lock on x, no other lock can be set on x
    - The owner can read and write the object
    - A lone read lock on x can be /upgraded/ to a write lock by owner
  - Upgrade locks (aka update locks, U locks)
    - Help in acquiring write locks in certain conditions
** Scheduling with shared locks

#+DOWNLOADED: screenshot @ 2021-05-09 11:03:08
[[file:images/The_common_scheduler,_explained_with_locks/2021-05-09_11-03-08_screenshot.png]]
- Here TA1 has a read lock on x and has not commited thus W3[x] must wait until
  c1 is in schedule
** Problem with read and write locks:
- As long as several transactions have a read lock on the same object, a write
  lock cannot be acquired. Writing transactions have to wait
- Without further precautions, a writing transactions might never be able to
  acquire the write lock, because new transactions continuously start to read:
  the writing transaction would be in a /live-lock/ (a form of *starvation*

#+DOWNLOADED: screenshot @ 2021-05-09 11:06:29
[[file:images/The_common_scheduler,_explained_with_locks/2021-05-09_11-06-29_screenshot.png]]

** Solution: Upgrade locks
- The first writing transaction gets the third type of lock, the *upgrade lock* (Ulock):
  - No new reader can access this object, before the write was dequeued and
    executed
  - Once all the readers have finished, the writing transaction gets the exclusive
    lock: if it has read is a /lock upgrade/


#+DOWNLOADED: screenshot @ 2021-05-09 11:08:35
[[file:images/The_common_scheduler,_explained_with_locks/2021-05-09_11-08-35_screenshot.png]]

** Upgrade locks continued
- Only one transactions can acquire a U lock
- Later transactions that try to do so will be blocked
- Often expressed in a matrix

#+DOWNLOADED: screenshot @ 2021-05-09 11:10:42
[[file:images/The_common_scheduler,_explained_with_locks/2021-05-09_11-10-42_screenshot.png]]
** Conflicts in a schedule
- Conflicts are a concept to capture the pairwise relationship of operations and
  transactions accessing a write conflict object in a schedule:
  - Each operations writing the object has one conflict with each operations by
    a different transaction accessing this object. We write these conflict for
    object x:

#+DOWNLOADED: screenshot @ 2021-05-09 11:16:38
[[file:images/The_common_scheduler,_explained_with_locks/2021-05-09_11-16-38_screenshot.png]]
- As ordered pairs of operations:

#+DOWNLOADED: screenshot @ 2021-05-09 11:16:51
[[file:images/The_common_scheduler,_explained_with_locks/2021-05-09_11-16-51_screenshot.png]]
* Precedence graph
- We interpret the conflicts are directed edges, wihch we see as going from the
  transaction of the earlier operation in the conflict to the transaction of the
  later operation
- The resulting directed graph we call the *precedence graph*, of that schedule
- Idea still: order in which transactions commit should be compatible with this
  order
- Only if the precedence graph is free of cycles the schedule is serializable


#+DOWNLOADED: screenshot @ 2021-05-09 11:19:20
[[file:images/Precedence_graph/2021-05-09_11-19-20_screenshot.png]]
* Relaxed Isolation, Isolation levels
- Transaction design for high throughput
- ANSI Isolation Levels
- Phenomena
** Transaction tuning, relaxed isolation
- Isolation is expensive: High isolation levels ensure serializability, but
  reduce /throughput/, the number of transactions per time unit
- Methods of increasing transaction throughput by reducing isolation in
  different ways
  - Transaction chopping
  - Reduced transaction isolation levls
  - Optimistic locking
  - Other lock types

** Phenomenon: Dirty read
- A Phenomenon is when the ACID isolation thing is relaxed and suddenly
  Transactions are to some degree aware of other transactions
- Transaction TA2 performs a dirty read if it reads an uncommitted write result
  of TA1

#+DOWNLOADED: screenshot @ 2021-05-10 12:10:57
[[file:images/Relaxed_Isolation,_Isolation_levels/2021-05-10_12-10-57_screenshot.png]]
- Dirty reads can happen, if transaction TA2 does not react to a write lock on
  x.
- Dirty reads might be no problem for:
  - Transactions that gather overview data
  - Transactions that investigate options for later transactions
- But they are dangerous for other operations
  - Might lead to inconsistent results
  - Isolation level READ UNCOMMITED allows dirty reads, but the transactoins
    have to be read only

** ACID Isolation
- Database operations in a transaction appear isolated form database operations
  of all other transactions
  - Does not apply to non-database operations of client programs
- One crucial property: *Repeatable Read:*
  - If a transaction reads a data field in the DB twice, and does not change it
    inbetween, it must receive the same value
- Isolation with Repeatable Read is expensive
  leads to delays, deadlocks, and/or aborts
- and can be relaxed: /Isolation levels/

** Repeatable Read: 
- One crucial property: Repeatable Read:
  - if a transaction reads a data field in the DB twice, does not change it in
   between, it must receive the same value
- Pseudo code:

#+DOWNLOADED: screenshot @ 2021-05-10 12:17:07
[[file:images/Relaxed_Isolation,_Isolation_levels/2021-05-10_12-17-07_screenshot.png]]
- Isolation with Reapeatable Read is expensive, can be relaxed

#+DOWNLOADED: screenshot @ 2021-05-10 12:17:39
[[file:images/Relaxed_Isolation,_Isolation_levels/2021-05-10_12-17-39_screenshot.png]]
- *Above means that writes remain stable!*. Write then wait then you should get
  the same value when you read after _any_ period of time
** ANSI/ISO transaction isolation levels
- Isolation levels are defined with respect to three different phenomena
  (results of reduced isolation)
  - Direty read: reading an uncommited value for x
  - Fuzzy read: reading different, committed values for x
  - Phantom: reading a new committed inserted row
    - 
    #+DOWNLOADED: screenshot @ 2021-05-10 12:20:36
    [[file:images/Relaxed_Isolation,_Isolation_levels/2021-05-10_12-20-36_screenshot.png]]

** Fuzzy read
- Transaction TA1 encounters a fuzzy read phenomenon if it reads two or more
  different commited values for x

#+DOWNLOADED: screenshot @ 2021-05-10 12:21:26
[[file:images/Relaxed_Isolation,_Isolation_levels/2021-05-10_12-21-26_screenshot.png]]
- Fuzzy reads can happen, if transactions do
  - Not observe the read-lock of transaction TA1
- Note, that fuzzy reads require two read operations by TA1, while dirty reads
  can happen with a single read
- Fuzzy read situations can lead to lost updates in a rather counterfactual way:
  - If the second read does not happen or is not acted upon (next slide)

** Fuzzy read continued: lost updates
- A serious consequence of not using REPEATABLE RAD: a committed transaction
  might miss an update: *lost update*

#+DOWNLOADED: screenshot @ 2021-05-10 12:23:44
[[file:images/Relaxed_Isolation,_Isolation_levels/2021-05-10_12-23-44_screenshot.png]]
- *IT DOESN"T DO THE SECOND READ IN TA1* if it does it works! otherwise it
  doesn't. 

** Phantom
- A phantom (row) is a phenomenon that is possible in the relational data modle,
  but goes beyond the basic read/write model
- Are caused by inserts, not by updates
- The following situation describes a phantom row:

#+DOWNLOADED: screenshot @ 2021-05-10 12:31:11
[[file:images/Relaxed_Isolation,_Isolation_levels/2021-05-10_12-31-11_screenshot.png]]
- So you're writing new data based on old (bad) data

** Dirty read is worse than fuzzy read
- a case, where a transaction reads two different values, but one of them is a
  dirty value

#+DOWNLOADED: screenshot @ 2021-05-10 12:32:07
[[file:images/Relaxed_Isolation,_Isolation_levels/2021-05-10_12-32-07_screenshot.png]]
- This is called a dirty read and not a fuzzy read
- Rationale: dirty read is a more serious phenomenon than fuzzy read

** Describing phenomena
- The names of the phenomena intuitively refer to one operation in the schedule,
  bolded in the following examples

#+DOWNLOADED: screenshot @ 2021-05-10 12:34:28
[[file:images/Relaxed_Isolation,_Isolation_levels/2021-05-10_12-34-28_screenshot.png]]
- However, these operations are very variable for seemingly similar situations
  and therefor not good for identifying
- Instead we can also name the conflict object (this is always x in the examples
  above), and the *first transaction writing* the conflict object (always here
  TA1) of the phenomenon

** Serial schedules
- A /serial/ schedule is a schedule s, where one transaction starts only after
  all previous transactions have finished

#+DOWNLOADED: screenshot @ 2021-05-10 12:36:05
[[file:images/Relaxed_Isolation,_Isolation_levels/2021-05-10_12-36-05_screenshot.png]]
- Serial schedules would be the result of DB-wide MutEx
- Database uses a single lock!
- Serial schedules fufil ACID isolation

** Definition of serializable schedules
- For each ordering r of a set T of n transactions let ser(r, T) be the serial
  schedules running the transactions in that ordering r
- *Def*: A schedule s for a set of transactions T is /serializable/ iff:
  - There exists on ordering r of T so that s has the same effect as ser(r,T) on
    the database *and* clients, that means: for each data object x:
    - the value of x after s is the same as the value would be after ser(r,T)
      *(effect of writes)* and
    - All clients got the same read results for x *(effects of reads)*
  - In short: A schedule is serializable, if it is equivalent to a serial
    schedule in its effect on the database and the clients
  - Highest ACID isolation: only serializable schedules are run
** Lock-base scheduling for isolation
- We have seen two different unproblematic cases: Transactions disjoint in time
  or disjoint on data
- So schedulers do the following:
- If two transactions have no conflict object, do nothing
- If they have a conflict object: order their execution in time
  - Define an order in time in which they should be applied logically
  - The scheduler ensures that the schedule that is performed is equivalent to
    the serial schedule that would execute them in that order
- Hence the produced schedules are /serializable/

* Multiversion Concurrency
- Motivation:
  - Distributed databases: Replication
- Snapshots
- Access sets
- Conflict detection
  - Snapshot Isolation
  - Optimistic strategies
- Phenomenon: Write skew
** In-place Update vs Multiversion
- So far we considered transaction isolation techniques where only a single copy
  of each data object exists
- This approach is connected to shedulers
- There is an alternative approach that allows *multiple versions* of a data
  object to be active during *concurrent open transactions*: multiversion
  concurrency (MVC)
- This approach arises naturally in several designs for data intensive systems
- We focus on one motivation in which the semantics of multiversion concurrency
  is easy to understand
- A specific *database replication* protocol
** Motivation: Replication with Master copy
- A /master copy/ of a database is replicated in several /working copies/ (aka
  database caching, duplication)
- Clients work on working copies
- Motivation: eg: faster read access
- Work (writes) shall be consolidated in the master copy
- All databases together should behave as a single database: important case of a
  distributed database
- ACID properties for all clients: serializability across whole DB

#+DOWNLOADED: screenshot @ 2021-05-13 18:54:49
[[file:images/Multiversion_Concurrency/2021-05-13_18-54-49_screenshot.png]]

** In-place update vs Multiversion
- In-place update strategies motivate concurrency by scheduling
- In concurrency by scheduling, transactions are /delayed/ in case of conflict
- Multiversion concurrency will not use locks, so transactions will not be
  delayed. Instead transactions /compete/:
  - If they come into conflict, one transaction will be aborted
  - Several strategies:
    - First commited wins. Will fit will to our simple model of multiversion
      concurrency  as replication protocol
    - Other strategies are possible: first write wins etc.

** Replication
- Master copy is authoritative; transactions must be committed at master copy
- Idea: cooredination necessary only on commit
- Clients perform a local transaction on their working copy
- They do that through a /replication middleware/
- Replication middleware does the cooredination with the master copy

#+DOWNLOADED: screenshot @ 2021-05-13 18:58:17
[[file:images/Multiversion_Concurrency/2021-05-13_18-58-17_screenshot.png]]

** State of the working copy as a snapshot
- For the moment we can assume the following:
  - Each working copy is only used for a single transaction at a time, no
    concurrency at the working copy.
  - At the start of a local transaction, the working copy is in a recent state
    of the master copy that is *clean*: A state with no writes by open
    transactions. (but all durable results) (We discuss later how to achieve
    that)
  - This clean state is a *snapshot* and dated with the *commit timestamp* of
    the latest committed transaction
  - This commit timestamp is therefor also saying how up-to-date the working
    copy is

** Replication Middleware
- The replication middleware logs all the commands of the client. In the basic
  transaction model, it will create /access sets/ and /write sets/
- During the open transaction, the replication middleware is just eavesdropping
- At commit, the replication middleware interferes and communicates

** Access sets and write sets
- The commands executed by the local transaction are logged by the replication
  middleware in /access sets/ and /write sets/. These are object sets, but
  values are attached.
- Since the transaction is alone ion the working copy, we have to record for
  each object at most one original read value (before image) per object and at
  most one final write result (after image)

#+DOWNLOADED: screenshot @ 2021-05-13 19:13:30
[[file:images/Multiversion_Concurrency/2021-05-13_19-13-30_screenshot.png]]


** Local transactions work on a snapshot
- If the working copy is at timestamp s and local transaction TA1 is starting,
  then the working copy will not be update with more data from the master copy
  until TA1 commits
- Hence the local transaction works on a single *snapshot* of the database: the
  state of the master at timestamp s
- We may consider the snapshot as a tiny moment e later than the commit
  timestamp s, but before any other commit
- Therefore we call s+e the snapshot of TA1 for uniqueness
- This way it is clear that the snapshot s+e sees the clean state after s

#+DOWNLOADED: screenshot @ 2021-05-13 19:17:00
[[file:images/Multiversion_Concurrency/2021-05-13_19-17-00_screenshot.png]]

** Commit as re-stamping
- The snapshot s+e is the state the transaction TA1 has seen

#+DOWNLOADED: screenshot @ 2021-05-13 19:23:16
[[file:images/Multiversion_Concurrency/2021-05-13_19-23-16_screenshot.png]]

- Let's assume, TA1 requests commit at time s+e+d. Other transactions might have
  committed meanwhile on the master copy /(interlopers)/. They are durable. What
  now?
- ACID durability (for interlopers) requires: no lost update!
- TA1 can only commit, if the master copy is still in the same state as it was
  at time s+e for everything concerning TA1
- We can say it must be possible tate:
  - TA1 can be re-stamped with timestamp s+e+d
- The new commit timestamp is used for later transactions

** Conditions for re-stamping
- TA1 can only commit, if database is at time s+e+d still in the same state as
  at time s+e for all data relevant for TA1
- Two differently strict definitions of that
- Strict case. If TA1 and each interloper TAx are:
  - Write disjoint: no problem possible
- Checking for write disjoint will be called optimistic locking (historic name)
  will guarantee best isolation
- The more generous definition:
  - no update that TA1 overwrites: no lost update
- Will lead to a new, interesting, relaxed isolation level: /snapshot isolation/

** Commit through replication middleware
- At commit the replication middleware interferes and communicates with the
  master copy:
- It wil luse the recorded information about the local transaction and work on
  the master copy
- It will check if  restamping is possible
- If yes, the it will enact the changes of the local transaction at the master
  copy

#+DOWNLOADED: screenshot @ 2021-05-13 19:35:43
[[file:images/Multiversion_Concurrency/2021-05-13_19-35-43_screenshot.png]]

** Two strategies to check for changes
- At commit, the replication middleware will check on the master copy if the
  data affected by the transactio nhas been changed since the snapshot was taken
- Based on the access sets and write sets cwe can see if the transaction and
  each interloper (pairwise) are sufficiently disjoint
- Two ways to check
  - One for the stricter condition requiring write disjointness: Optimistic
    locking
  - One for the less strict lower isolation just preventing lost updates:
    Snapshot isolation

** Check for write conflicts
- Optimistic locking:
  - Check the whole access set (superset of write set)
  - For each object: is the read value still the curreny value in the master
    copy?
- Snapshot isolation
  - Check only write set
  - For each object: Is the read value still the current value in the master copy?
  - If yes: test passd, transaction is allowed to write
  - If not: test failed transaction is aborted

** Replication middleware executes transaction
- The check andt he writes happen in a single transaction on the master copy
- Replication middleware executes transaction on the master copy
- The master copy must be transactional
- Might be a conventional database
- Or might offer simpler mechanisms, based on the limited type of transaction
  that we use

** Write Skew
- Snapshot isolation allows a phenomenon: *write skew*
  - Two transactions "getting wires crossed"
  - A "double almost lost update on different objects"
- Can be expressed as a schedule

#+DOWNLOADED: screenshot @ 2021-05-13 19:45:37
[[file:images/Multiversion_Concurrency/2021-05-13_19-45-37_screenshot.png]]
- We can see that the snapshot isolation test will succeed since the read/write
  sets are disjoint
- Nevertheless violates serializability
- Can be a problem
- Is rare and considered "mostly harmless"

** Write skew business logic example
- An example where a write skew could appear
- Bank grants overdraft based on general liquidity
- Customer has two accounts, x and y
- Bank allows any one account to go into negative if overall balance stay
  positive. eg: consider debiting account x
  - 
  #+DOWNLOADED: screenshot @ 2021-05-14 10:08:52
  [[file:images/Multiversion_Concurrency/2021-05-14_10-08-52_screenshot.png]]

** Write skew in numbers
- Account x = $1000
- Account y = $800
- Planned TA1: withdraw $1100 from x; ok since overall balance remains $700
- Planned TA2: withdraw $900 from y; ok since overall balance remains $900
- With snapshot isolatoin, if both transactoins are executed concurrently they
  might go through!
- Balance will be unwanted (bad)
  - But is less dramatic than lost update

** The REPEATABLE READ debate
- The write skew phenomenon does does strictly contain a non-repeatable read

#+DOWNLOADED: screenshot @ 2021-05-14 10:11:38
[[file:images/Multiversion_Concurrency/2021-05-14_10-11-38_screenshot.png]]
- In lock based schedulers those scheduling strategies that prevent fuzzy reads
  usually also prevent write skew
- So fact that the write skew is not on the list of phenomena creates the follow
  problem: Strong supporteds of snapshot isolation argue that it still filfils
  REPEATABLE READ
- Common practice is to define a new lower isolation level  (Snapshot
  isolation), which is better than READ COMMITTED, but below REPEATABLE READ

** Replication: keepign local copies up-to-date
- Local copies can be kept up to date with incremental changes. The write sets
  can be used
- There is an instance of the replication middleware for each client; these
  instrances can communicate the write sets of commited transactions


#+DOWNLOADED: screenshot @ 2021-05-14 10:16:52
[[file:images/Multiversion_Concurrency/2021-05-14_10-16-52_screenshot.png]]

** Timestamped and serializability
- At commit, TA1 is *re-stamped* with timestamp s+e+d as commit timestamp for
  snapshots for later transactions
- Serializability requires an equivalent serial schedule; which serial schedules
  are equivalent to transactions in MVC?
- Many, because the re-stamping shows that the transaction had no interference
  by other transactions for a while
- We need only one for serializability, we can list the transactions in order of
  their final commit time stamp

#+DOWNLOADED: screenshot @ 2021-05-14 10:18:37
[[file:images/Multiversion_Concurrency/2021-05-14_10-18-37_screenshot.png]]
** In-place Update vs Multiversion
- Concurrency by scheduling
  - Tries to minimize aborts
  - Can prevent starvation
- Problematic case for scheduling: Wait for graphs with paths of length 2 or
  above
  - One transaction waits for another blocked transaction
  - Reduces concurrency
  - Multiversion will have more aborted transactions
  - Not clear how to prevent starvation
  - Scheduling and competing strategies do not mix well
* ACID durability
- Write ahead logging
- buffer management
- steal, no-force strategies
- media recovery
** THE ACID properties
- requirements that a transaction manager must meet for the transactions
  - Atomicity: either all of the operations of a transaction are made durable or
    non of them are
  - Consistency: after the transaction, the database is in a consistent state
  - Isolation: operations in a transaction appear isolated from all other
    operations. Transactions have a virtual serial view on the system
  - Durability: once the user has been notified of success, the transaction will
    persist, and not be undone

** ACID durability
- Once the user has been notified of success of transaction t:
a) All writes of t are permanent: must be changed through later commited writes,
eg. compensating transactions
b) The effect of t is kept in a crash resistant way
    - minimum requirement: transaction is written to persistent storage:
      resistant against
      - OS crash
      - system outage
c) preferred: protection against loss of persistent memory:
  1. Resistance against persistent storage failure
  2. resistance against catastrophes, geographic distribution

** System architecture

#+DOWNLOADED: screenshot @ 2021-05-14 11:11:15
[[file:images/ACID_durability/2021-05-14_11-11-15_screenshot.png]]
- log buffer is a cache
- database buffer stores the database
** Buffer management in a DBMS

#+DOWNLOADED: screenshot @ 2021-05-14 11:11:41
[[file:images/ACID_durability/2021-05-14_11-11-41_screenshot.png]]

** Structure of the database
- Database buffer and stable database content is partitioned into pages
- Every buffer page has exactly one page on the stable database
- Pages are read from and written to the stable database on disk as a whole
  - Must be read when not yet in buffer
- Buffer is fast, stable database is vast
- Remember difference in access time: RAM 1ns, database 1ms; ratio?

#+DOWNLOADED: screenshot @ 2021-05-14 11:13:18
[[file:images/ACID_durability/2021-05-14_11-13-18_screenshot.png]]

** The database Log
- is a central feature of typically database managers
- The log contains a list of
  - The write operations performed
  - The transaction demarcations: (BOT), abort, commit
- We consider undo/redo logs: for each write operation, a log entry is writen
  that contains
  - a log sequence number, or LSN (denoted as "nr:")
  - A transaction ID of the executing transactoin ("ta:")
  - An identifier of the object affected ("obj:")
  - The value before the write operation (before-image, "b:")
  - The value after the write operation (after-image, "a:")

** Example log and database
- The log of the database and database buffer at a certain point in time could
  look the following way (new operations are appended at lower end)
- Transaction demaractions contain contain only LSN and TA

#+DOWNLOADED: screenshot @ 2021-05-14 11:20:19
[[file:images/ACID_durability/2021-05-14_11-20-19_screenshot.png]]

** Crash recovery: write ahead logging 1
- A log has a /log buffer/ in main memory and a /stable log/ on persistent
  storage
- Semantics of a /system crash/: At an arbitrary point in time, database buffer
  as well as log buffer are lost
- Recovery must be based on stable log and stable database alone
- A transaction is conceived as commited only /after/ the commit entry of this
  transaction is written to the persistent and reliable log file storage: /write
  ahead loggin/ (WAL)
- Main policy/semantics of database crash recovery: The /stable log/ is
  authoritative

** Crash recover: stable log is authoritative
- The stable log decides about the correct status of transactions:
  - Committed transactions have a commit record in the stable log because of
    write ahead logging. They are /winners/ and considered committed:
    crash-durability
  - Those that are not committed are /losers/ and considered aborted

** Goal of crash recovery: clean stable database
- The stable database is *clean* iff the stable database is consistent with the
  winner/loser decision of the stable log
- All writes of the winners, only only the writes of the winners are reflected
  in the stable database
- THe log entries of uncommited transactions must be without effect
- Task of crash recovery: the stable database must be made clean based on the
  stable log

** Remark: Media Recovery
- Address a much more severe failure sitation, but is semantically much easier
- Addresses the situation that the stable database is lost by media failure (ie
  harddisk dies)
- Important semantic specification: The clean stable database can be
  reconstructed from the complete stable log at any point in time: Redo all transactions!
- Liekwise, the current clean stable database can be reconstructed from a
  historic clean stable database copy and the stable log from the point in time:
  /media recovery/
- Ergo: backups are important!

** database buffer management
- The buffer is a write cache: Changes to the data in the buffer are not
  immediately written to the stable database
- Database buffer management
  - If a cache miss occurs, load requested pages. This means, old pages must be replaced
  - Pages with changes need to be written back
    - Cache miss means that the database buffer (memeory) the cache misses and
      we must go to disk
  - We have to distinguish commited caginess and uncommitted changes

** Crash recovery
- Database buffer management can be aligned in different ways to transactions
- Easiest situation: Stable database is always clean. Unfortunately this will
  turn out to be not practical
- Alternative, more complex sitation:
  - Tehs table database can contain pages with the following problems
    - writes by uncommited transactions
    - Old data not reflecting writes by committed transactions
  - Both kinds of problems can appear on the same page

** Buffer management pilicy alternatives
- Policies for buffer pages with committed write
  - *force*: At commit, such pages have to be written to the stable
    database. Leads to performance bottlenecks
  - *no-force*: drops this requirement. Leads to redo. Meaning that you have to
    redo those transactions (i think)
- Policies for buffer pages  with uncommitted write
  - *no-steal*: Such pages must not be written to stable database. Can lead to
    buffer bottlenecks (all pages written)
  - *steal*: drops this requirement. Leads to undo. where you have to undo
    changes when the power goes out

- Force, no-steal is easy crash recovery, ensure the stable database is always
  clean: not practical enough

** Buffer management: no-force, steal policy
- Is today's preferred solution: no alignment between buffer page swapping and
  trascations: no-force, steal policy
- Buffer pages are swapped according to demand
- Avoids more bottlenecks, more difficult to implement
- *no-force*: some committed writes are not in the stable database yet. Makes
  redo after crash necessary
- *steal*: some uncommitted writes are in the stable database; undo also after
  crash

** Write ahead logging 2
- enabling crash recovery for steal policy
- Stable database pages are changed by loser transactions
- The information to undo the loser transactions must be in the log
- Therefore
  - Before a buffer pages is written back to the stable database
  - All log entries for that page have to be written back to the stable log
- This is another application of write ahead logging

** Example log and database
- Situation at the time of the crash
- Database buffer is then lost
- Writes are shown on top of old values

#+DOWNLOADED: screenshot @ 2021-05-14 13:56:24
[[file:images/ACID_durability/2021-05-14_13-56-24_screenshot.png]]

#+DOWNLOADED: screenshot @ 2021-05-14 13:56:29
[[file:images/ACID_durability/2021-05-14_13-56-29_screenshot.png]]

** Example log and database
- Situation at the time of the crash
- Database buffer is then lost
- Writes are shown on top of old values

#+DOWNLOADED: screenshot @ 2021-05-14 13:57:01
[[file:images/ACID_durability/2021-05-14_13-57-01_screenshot.png]]

#+DOWNLOADED: screenshot @ 2021-05-14 13:57:05
[[file:images/ACID_durability/2021-05-14_13-57-05_screenshot.png]]
** crash recovery for no-force, steal policy
- Has to redo winners and undo losers
- Has to go through the log:
- For redo in positive time direction
  - Identify, whether the write (or its TA) is committed
  - Write for each committed operation the after image
- For unodo in negative time direction
  - Identify whether write (or its TA) is not committed
  - Write for each uncommitted operation the before image

** Example log and database (cont)
- Situation at the time of the crash
- Database buffer is then lost
- Writes are shown on top of old values

#+DOWNLOADED: screenshot @ 2021-05-14 13:59:02
[[file:images/ACID_durability/2021-05-14_13-59-02_screenshot.png]]

#+DOWNLOADED: screenshot @ 2021-05-14 13:59:08
[[file:images/ACID_durability/2021-05-14_13-59-08_screenshot.png]]

** Requirement of easy crash recovery
- If we want to always maintain a clean stable database, we need pagewise write
  locks: only one trasaction can write a page at a time
- Proof by contradiction: Consider a page written by TA1 and TA2 (not using
  pagewise write lock)
- Now TA1 commits while TA2 does not commit. What to do?
- write back? steal page
- Not write back? outdated page
- Database is not in clean state in either case
* From Applications to Solutions
** Access times of one byte

#+DOWNLOADED: screenshot @ 2021-05-17 12:25:48
[[file:images/From_Applications_to_Solutions/2021-05-17_12-25-48_screenshot.png]]
** Jim Gray's Storage Lantency Analogy: How far away is the data?

#+DOWNLOADED: screenshot @ 2021-05-17 12:26:12
[[file:images/From_Applications_to_Solutions/2021-05-17_12-26-12_screenshot.png]]
** Storage Hierarchy

#+DOWNLOADED: screenshot @ 2021-05-17 12:26:26
[[file:images/From_Applications_to_Solutions/2021-05-17_12-26-26_screenshot.png]]
- Volatile, Non-volatile
- Volatile
  - Smaller, faster, expensive
  - Random access, byte addressable
- Non-volatile
  - Sequential access, block addressable
  - Slower, larger, cheaper

** Magnetic Disk
- Spindle, platters, surfaces, tracks, sectors, blocks, arm assembly, read-write
  heads
- Read data starting from sector x, track y
  - Access time
    - Seek time: disk arm to the y-th track
    - Rotate delay: align the read/write head to the sector x
  - Transfer delay: rotate the spindle so that the data that stored
    consecutively on the disk can be transferred
- *Sequential access is far more efficient than random access!*

** Magnetic Disk: Block and Page
- Block: A logical unit consisting of a fixed number of contiguous sectors
  - Remember that a sector is a part of on arc on a track, where a block is a
    sequence of these sectors that have been read to form a block
- Important concept in OS and Databases
- Block size is fixed during initalization and cannot be changed
  dynamically. The block size ranges from 512B to 8KB
- The OS always reads (from the disk) and writes (to the disk) in
  blocks. Database is an application on OS .Following the same data access
  method
- Input/Output (I/O): The transfer of one block between main memory and the disk
- The number of IO is used to *evaluate the efficiency* of a data
  structure/algorithm
- Database alternatively calls a block a *page*

** File Storage

#+DOWNLOADED: screenshot @ 2021-05-17 12:32:44
[[file:images/From_Applications_to_Solutions/2021-05-17_12-32-44_screenshot.png]]
- A block is a page, a block is a sequence of sections, (series of arcs)

** Tuples in a Page

#+DOWNLOADED: screenshot @ 2021-05-17 12:33:46
[[file:images/From_Applications_to_Solutions/2021-05-17_12-33-46_screenshot.png]]
- Compacting means that you shift everything from the pages to the right of the
  delete to the left so that there is no more memory inbetween that is
  zero'd. This is pretty slow. This is periodically applied so that we don't do
  this inefficient operation too often

** Another approach (Tuples in a page)

#+DOWNLOADED: screenshot @ 2021-05-17 12:34:59
[[file:images/From_Applications_to_Solutions/2021-05-17_12-34-59_screenshot.png]]
- Keep a free list of blocks and when that list gets too large we can compact
  using the method covered before

** Variable length Tuples
- *Slotted Pages*
1. The header keeps the # of used slots and the offset of the smallest offset of
  a used slot
2. The slot array maps the slots to the tuples' starting positions offsets


#+DOWNLOADED: screenshot @ 2021-05-17 12:37:05
[[file:images/From_Applications_to_Solutions/2021-05-17_12-37-05_screenshot.png]]

** Ids
1. Each page has a unique *Page ID*: The page directory layer will map the Page
   ID to the physical address
2. Each tuple has a unique Tuple ID: Page ID + offset/slot
3. These IDs can only be used for checking errors, no application can rely on
   them

#+DOWNLOADED: screenshot @ 2021-05-17 12:38:23
[[file:images/From_Applications_to_Solutions/2021-05-17_12-38-23_screenshot.png]]

** Fields in Tuples
A tuple is essentially a sequence of bytes, it is the DBMS who interprets the
bytes into attributes types of values

#+DOWNLOADED: screenshot @ 2021-05-17 12:39:08
[[file:images/From_Applications_to_Solutions/2021-05-17_12-39-08_screenshot.png]]
** Recap
- Understanding the implementations of a database is useful for choosing the
  *appropritate* database solution for a real application
- Data storage medias of different natures for a storage hierarchy to create an
  illusionary *quickly accessible* and *non-volatile massive* storage - *how?*
- Data is transferred between the main memory (volatile) and the disk
  (non-volatile) in blocks where a block is a fixed sized chunk of data
- Database is a collection of file, a file is a collection of blocks (pages), a
  page is a collection of records, a record is a sequence of bytes interpretable
  to attribute types and values

** Storage
- *Buffer Pool*: available main memory used for storing copies of disk blocks
- *Buffer Manager*: A DMBS subsystem that manages the buffer pool, aiming at
  minimizing I/O: the number of blocks transferred between the memory and the
  disk

#+DOWNLOADED: screenshot @ 2021-05-20 11:04:50
[[file:images/From_Applications_to_Solutions/2021-05-20_11-04-50_screenshot.png]]

** Buffer Manager
- The buffer pool is an array of *frames*, the size of a frame is the size of a
  page
- When the DBMS requests a page, a copy is placed in a frame
- The *page table* maps the IDs of the pages that are currently in the buffer
  pool to the corresponding frames, and maintains the meta-data for each page
  - *Dirty flag: a binary state. The page is *dirty* if the page is updated
    since loaded from the disk
  - *Pin*: an integer, representing the number of threads that is using the
    page. The page is *unpinned if /pin = 0/*

#+DOWNLOADED: screenshot @ 2021-05-20 11:07:03
[[file:images/From_Applications_to_Solutions/2021-05-20_11-07-03_screenshot.png]]
- Page table MAPS from the pageID to the buffer frame. So you give it a pageID,
  if there exists that pageID in that page table then it iwll go the the buffer
  pool and give back the frame in the buffer pool that is linked to that pageid

** Buffer Manager: Read
- *Read request*: A page ID X
- *Buffer manager*, upon receiving a read request
  - Check the page table, if the page is not in the buffer pool,
    - if the buffer pool is full, perform *buffer replacement* to get an empty
      frame
    - Load X to the buffer pool and update the page table
    - Return the content of the page from the corresponding frame
- *Buffer replacement*: Choose, among all the unpinned pages, one page Y based
  on a *replacement policty*
  - If Page Y is dirty, *write back* Y to the disk, then kick the page out of
    the buffer pool

#+DOWNLOADED: screenshot @ 2021-05-20 11:31:54
[[file:images/From_Applications_to_Solutions/2021-05-20_11-31-54_screenshot.png]]


** Buffer Manager - Replacement Policy
- *Buffer replacement policy*. When the DBMS needs to free up a frame to make
  room for a new page, it must decide which page to *evict* from the buffer pool
  - Goal: to enable an illusionary *quickly accessible non-volatile massive*
    disk storage, ie: to increase the "hit rate" - the proportion of read
    requests whose page is in the buffer pool without triggering an I/O
    operation
- *Policy 1: Least Recently Used (LRU)*
  - Maintain a single timestamp of when each page was last accessed
  - Select the one with the oldest timestamp to be evicted
    - Remember that the page table keeps *metadata* on the frames(pages) that
      are in the buffer pool, meaning that it will know the last access time of
      a given page/frame.
Heuristic: The page that is used more recently is more likely to be used later
- You should note that keeping the timestamps has an overhead
- *Policy 2: Clock*
  - Maintain a binary bit for each page, when access the page, set the bit to 1
  - Reset the bit to 0 using a "sweeping clock hand", in particular
  - When the clock hand reaches a page and the bit of the page is 0: the page has
    not been accessed for the past whole circle, evict the page; otherwise, set
    the bit to 0
    - The clock goes around and around, when a page hasn't been accessed the bit
      is 0 and it's evicted from the buffer page table, otherwise if it's 1 it's
      set to zero. Easy.
Heuristic: Approximate LRU without keeping a timestamp per page
- Display all the pages in a circle like a clock and whenever you use a page
  then you mark it with a 1, when you need to evict then you do a spin. if it's
  a 1 then reset to 0, if it's a zero then evict that! reset all to zero then
  reset the hand!
** Buffer Manager: Replacement Policy
- *Sequential flooding of Policies 1-2*
  - A query performs a sequential scan that reads every page
  - This pollutes the buffer pool with pages that are read once and then never
    again
  

#+DOWNLOADED: screenshot @ 2021-05-20 11:42:05
[[file:images/From_Applications_to_Solutions/2021-05-20_11-42-05_screenshot.png]]
This is bad because a whole lot of blocks are scanned and then used once and
then never again. This means that our Buffer table has become bloated with pages
that we are never going to use again!
- *Policy 3: Most Recently Used (MRU)*
  - Maintain a single timestamp of when each page was last accessed
  - Select the one with the youngest timestamp to be evicted
Heuristic: The page that is used more recently is less likely to be used later

** Buffer Manager
- Goal: To enable an illusionary *quickly accessible non-volatile massive* disk
  storage, ie: to increase the "hit rate": the proportion of read requests whose
  page is in the buffer pool without triggering an IO
*** Other heurtistics in improving the buffer management
- Run time statistics and query execution algorithms can be analyzed to *predict
  which page is less likely to be used in the future*
- Replacing a dirty page is slower than replacing a clean page
  - This is because you have to first write the dirty frames page to disk before
    you can evict it from the buffer pool
  - Background writing: The DBMS can periodically walk though the page table,
    write dirty pages back to the disk and reset the dirty flag:
    - What this does is that is reduces proportion of dirty pages that we will
      be seeing when we are doing our eviction, meaning that things are sped up
  - Engaging multiple buffer pools with different replacement policies
    - So you have a different type of buffer page eviction policy for different
      buffer pages, what this means is that you can get different
      characteristics that you would like for a certain operation: eg: evict
      most recent storage if you know the thing is only going to be used once.

#+DOWNLOADED: screenshot @ 2021-05-20 11:51:10
[[file:images/From_Applications_to_Solutions/2021-05-20_11-51-10_screenshot.png]]

** External Memory Sorting: Strategic Buffering
- *Problem (Sorting)*
  - Let R be a relation stored in n pages. Let m be the number of frames of the
    buffer pool. Assumes that n >> m (much greater than). Sort the tuples of R
    while optimizing the number of IO operations incurred
- * The algorithm of external memory sorting sorts the tuples in R in
  O(nlog_m(n)) IOs
1. *Create Runs: Repeatedly create runs as below till reaching the end of the R
1.1 Read m blocks of R into the buffer pool
1.2 Sort the m blocks in memory
1.3 Write the sorted m blocks to form the i-th (i is 0 initially) run
R^0_i. Increment i
Finally produce I = [n/m] runes. Note that the last run R_I may have less than m
blocks while other runs each has m blocks

** External Memory Sorting: Example

#+DOWNLOADED: screenshot @ 2021-05-20 11:57:58
[[file:images/From_Applications_to_Solutions/2021-05-20_11-57-58_screenshot.png]]

** External Memory Sorting
- *Problem (Sorting)*
  - Let R be a relation stored in n pages. Let m be the number of frames of the
    buffer pool. Assumes that n >> m. Sort the tuples of R while optimizing the
    number of IO operations incurred

1. *Create runs*. Create I = [n/m] (remember that you have to ceil this) sorted
   runs
2. *Merge Runs*, ie (m-1)-way Merge

Input: m-1 sorted runs
Output: 1 sorted run the includes all the input tuples


#+DOWNLOADED: screenshot @ 2021-05-20 12:01:41
[[file:images/From_Applications_to_Solutions/2021-05-20_12-01-41_screenshot.png]]

** External Memory Sorting
- *Problem (Sorting)*
  - Let R be a relation stored in n pages. Let m be the number of frames of the
    buffer pool. Assumes that n >> m. Sort the tuples of R while optimizing the
    number of IO operations incurred

1. *Create Runs*: Create I = ceil(n/m) sorted runs
2. *Merge Runs*: ie: (m-1)-way Merge
   Input: m-1 sorted runs. Output: 1 sorted run that includes all the input
   tuples
   - *Input buffer*: m-1 frames, initially load one page of the i-th run to the
     i-th frame
   - *Output buffer*: 1 fram initially empty
 Repeat until all input buffers become empty
   - Find the smallest tuple t in the m-1 input buffer, assume that from j-th
     frame
   - If the output buffer is full, flush the output buffer to the output run on
     disk
   - Append t to the output buffer,
   - Erase t from the j-th buffer, if the jth buffer becomes empty, load a new
     page if possible from the jth run

<<<<<<< HEAD

* Indexing (B+-tree)
** Ordered File

#+DOWNLOADED: screenshot @ 2021-05-21 10:21:36
[[file:images/Indexing_(B+-tree)/2021-05-21_10-21-36_screenshot.png]]
=======
*** Example
#+DOWNLOADED: screenshot @ 2021-05-20 12:05:49
[[file:images/From_Applications_to_Solutions/2021-05-20_12-05-49_screenshot.png]]


** Analysis

#+DOWNLOADED: screenshot @ 2021-05-20 12:21:27
[[file:images/From_Applications_to_Solutions/2021-05-20_12-21-27_screenshot.png]]

** Example using analysis

#+DOWNLOADED: screenshot @ 2021-05-20 12:21:45
[[file:images/From_Applications_to_Solutions/2021-05-20_12-21-45_screenshot.png]]
* Index (B+- Tree)
What is an index? This is something that you have created to speed up the search
of what you want to get
There are two kinda:
- You create a file then create an index to get a part of a file
- You create the information within the file and then create the index ontop of
  that
- B trees are the most important data structure

** Ordered file

#+DOWNLOADED: screenshot @ 2021-05-23 20:23:41
[[file:images/Index_(B+-_Tree)/2021-05-23_20-23-41_screenshot.png]]
- Records are sorted by *ordering field* (called ordering key if ordering field
  is a key field)
- Advantages
  - Efficiently read records in the order of the ordering key value
  - Efficiently find the next record
  - *Binary search*

#+DOWNLOADED: screenshot @ 2021-05-21 10:22:53
[[file:images/Indexing_(B+-tree)/2021-05-21_10-22-53_screenshot.png]]

#+DOWNLOADED: screenshot @ 2021-05-21 10:23:01
[[file:images/Indexing_(B+-tree)/2021-05-21_10-23-01_screenshot.png]]

** Ordered File
- Records are sorted by *ordering field* (called ordering key if ordering field
  is a key field)
- Advantages
  - Efficiently read records in the order of the ordering key value
  - Efficiently find the next record
  - Binary search
- Disadvantages:
  - Linear time insertion in keeping the order
  - Searching in log2(the number of blocks in the file)
    - I/Os can still be too expensive
  - Can only support binary search on the ordering field
- Solution: Indexes

=======

#+DOWNLOADED: screenshot @ 2021-05-23 20:24:47
[[file:images/Index_(B+-_Tree)/2021-05-23_20-24-47_screenshot.png]]

#+DOWNLOADED: screenshot @ 2021-05-23 20:25:01
[[file:images/Index_(B+-_Tree)/2021-05-23_20-25-01_screenshot.png]]

- So in this table you're recording a relation Name, Ssn, etc
- Each row is organized into a block
- You are ordering the tuples based on alphabetical ordering of names
- If you were to do a sql queries on this (listed above) then you would do a
  linear search (O(n) time. Or do a binary search Olog(n). This is not good
  enough because 1000 entries becomes 10. We want to control tne number of IOs
  within around 5

** Index
1. Indexes are used to speed up record retrieval in response to certain search
   conditions
2. Any field can be used to create an index
  - Multiple indexes can be constructed
3. The index on the ordering field of the ordered file
  - Primary index: the index on the *ordering field* which has a "unique"
    constraint
  - *Clustering index*: the index on the *ordering field*, multiple records can
    have the same value
4. Secondary index: on attributes other than the ordering field
  - Index stores each value of the *index field* (attribute) with a list of
    pointers to all disk blocks that contain records with that field value
  - Values in the index are ordered

- *Index entry* (key, block pointer): a pair with a value of the index field and
  a pointer to a disk block
- Indexes may be dense or sparse
  - *Dense index* has a index entry for each search key value in the data file
  - *Spare index* has entries for only some search values, eg: one index entry
    in the index file for each block in the data file
  - Example of primary index below


#+DOWNLOADED: screenshot @ 2021-05-23 20:34:05
[[file:images/Index_(B+-_Tree)/2021-05-23_20-34-05_screenshot.png]]
- So in this we are dragging the first tuple of each block to an index file so
  that we can look up more quickly. This is a sparse block exampled above. If
  you wanted a dense index then you would have a index to *each* entry
- You don't have to pay anything to search for a block but with the trade off
  with that the file can get really large (might not even fit on disk

** Clustering Index
- Cultering field: file records are physically ordered on a non-key field
  without a distinct value for each record
- The clustering index is a file on index entries <field value, block pointer


#+DOWNLOADED: screenshot @ 2021-05-23 20:38:34
[[file:images/Index_(B+-_Tree)/2021-05-23_20-38-34_screenshot.png]]

** Secondary Indexes
- A file of index entries
- Improve search time for the indexing field
- A secondary index must be dense
- More access time than primary index for "range queries"


#+DOWNLOADED: screenshot @ 2021-05-23 20:41:44
[[file:images/Index_(B+-_Tree)/2021-05-23_20-41-44_screenshot.png]]

** Multilevel Indexes
- *Fanout*, the maximum number of "children" a block can have
- The number of tuples in the file
- The number of blocks in the file
- The cost of finding a tuple with a specific key ceil(log_f(n/2)) + 1 = 3 I/Os
- Finding all tuples iwth keys in range [a, b]
  - Find the first tuple with key >= a, and then sequential scan the data file
- *How to efficiently insert a tuple without degrading the following searching?*


#+DOWNLOADED: screenshot @ 2021-05-23 20:44:40
[[file:images/Index_(B+-_Tree)/2021-05-23_20-44-40_screenshot.png]]


#+DOWNLOADED: screenshot @ 2021-05-23 20:44:56
[[file:images/Index_(B+-_Tree)/2021-05-23_20-44-56_screenshot.png]]
- Where f, n are defined above
This is a super flat tree because of multilevel indexing
This is a set of clustering indexes where you're clustering over multiple
levels. Your pointer points to the first tuple of the value. All the other
values will be obtained by cointinued scans

** Indexes

#+DOWNLOADED: screenshot @ 2021-05-23 20:50:36
[[file:images/Index_(B+-_Tree)/2021-05-23_20-50-36_screenshot.png]]

spares primary index: 100
clustering index y
secondary index: 1000

** Multilevel indexes
- Drawback of single level index:
1. Index file is a file, scanning a file can still be costly
2. To keep the index file in the order of the index field, update cost is linear
We will first derive *multilevel indexes* for better access and then introduce
the best data structure in databases, B+-tree, a multilevel index that is
optimized for both update and query

- Second level: Primary index on the first level index
- Third level: Primary index on the second level index
- *Fanout*, the maximum number of children a block can have
- the number of tuples in the file
- The number of blocks in the file
- The cost of finding a tupe with a specific key
- Finding all tuples with keys in range [a,b]. Find the first tuple with key >=
  a, and then sequentially scan the data file
- How to efficiently insert a tuple without degrading the following searching?

#+DOWNLOADED: screenshot @ 2021-05-30 14:00:43
[[file:images/Index_(B+-_Tree)/2021-05-30_14-00-43_screenshot.png]]

** Multi-level indexes
- Fanout: the maximum number of children a block of the index file can have,
  denoted as f
- If the first level has r_1 index entries
- The second level has r_2 = ceil(r_1/f) indexes
- The third level has r_3 = ceil(r_2/f) index entries
- And so on and so forth
- The top index level has one block of at most f index entries
- There will be approximately t = ceil(log_f(r_1)) levels

#+DOWNLOADED: screenshot @ 2021-05-30 14:02:38
[[file:images/Index_(B+-_Tree)/2021-05-30_14-02-38_screenshot.png]]
Above is the naming scheme of this shit

** Tree data structure terminology
- Tree is formed of nodes
- Each node (except root) has one parent; the root has no parent
- Leaf node has no child nodes
- Nonleaf nodes are called internal nodes
- Subtree of node consists of node and all descendant nodes
- The level of a node is its number of hops from the root
- A tree is balance if all leaf nodes are at the same level

#+DOWNLOADED: screenshot @ 2021-05-30 14:04:14
[[file:images/Index_(B+-_Tree)/2021-05-30_14-04-14_screenshot.png]]

** Search Tree
And internal nodes in an array of alternative tree pointers and key values
routing the search

#+DOWNLOADED: screenshot @ 2021-05-30 14:07:17
[[file:images/Index_(B+-_Tree)/2021-05-30_14-07-17_screenshot.png]]


** B-Tree Family
The name of B-Tree is usually used to represent a family of *self-balanced tree
structures* that keeps data sorted and allows search, sequential access,
insertions and deletions in O(log_f(n)) I/Os. Here f refers to the fanout which
is usually significantly larger than 2
- B+ tree is the most popular data structure in this family, in which data
  entry/data pointers are stored only at the leaf nodes, as we shall see later

#+DOWNLOADED: screenshot @ 2021-05-30 14:19:43
[[file:images/Index_(B+-_Tree)/2021-05-30_14-19-43_screenshot.png]]



** B-Tree: Internal Node and Leaf Node

#+DOWNLOADED: screenshot @ 2021-05-30 14:19:52
[[file:images/Index_(B+-_Tree)/2021-05-30_14-19-52_screenshot.png]]

** B+-Tree: A Dynamic Multi-Level Index
A B+-Tree is a multi-way search tree with the following properties
- Each node is stored in a page
- Data or pointers to data are stored only on leaf nodes
- All leaves are of the same level
- Each node (except for the root node) is a *atleast* half full
- In each internal node, the number of keys is one less than the number of
  children

#+DOWNLOADED: screenshot @ 2021-05-30 14:21:33
[[file:images/Index_(B+-_Tree)/2021-05-30_14-21-33_screenshot.png]]

** B+-Tree: Leaf Node
- *Leaf node values of a B+-tree*
  - Approach #1 Record Ids: a pointer to the location of the tuple that the
    index entry corresponds to
  - Approach #2 Tuple Data: the actual contents of the tuple is stored in the
    leaf node
- *Two parameters of a B+-Tree* The blocking factor f_l of the leaf represents
  the number of tuples or (key, pointer) pairs that a block can hold. The fanout
  f_i of an internal node represents the  maximum number  k such that a block
  can contain k pointers together with k-1 keys

** B+-Tree search
Relation A has two attributes, an ID (integer) and a binary state (0 or 1)

#+DOWNLOADED: screenshot @ 2021-05-30 19:47:02
[[file:images/Index_(B+-_Tree)/2021-05-30_19-47-02_screenshot.png]]
** B+-Tree Insertion
f_i = 3, f_l = 2, insert (2,0)
1. Find, from the root, the leaf where the tuple should be inserted based on the
   key
2. Deal with possible overflow
   2.1 By inserting (2,0) the leaf node becomes overflowed: three tuples
   2.2 *Evenly* divide the tuples of the overflow node into two nodes. Note that
   if the number of tuples is odd, *make the first node larger*


#+DOWNLOADED: screenshot @ 2021-05-30 19:49:55
[[file:images/Index_(B+-_Tree)/2021-05-30_19-49-55_screenshot.png]]

#+DOWNLOADED: screenshot @ 2021-05-30 19:50:05
[[file:images/Index_(B+-_Tree)/2021-05-30_19-50-05_screenshot.png]]



#+DOWNLOADED: screenshot @ 2021-05-30 19:57:14
[[file:images/Index_(B+-_Tree)/2021-05-30_19-57-14_screenshot.png]]

** B+-Tree: Insert: recursive split

#+DOWNLOADED: screenshot @ 2021-05-30 19:57:38
[[file:images/Index_(B+-_Tree)/2021-05-30_19-57-38_screenshot.png]]

#+DOWNLOADED: screenshot @ 2021-05-30 19:57:55
[[file:images/Index_(B+-_Tree)/2021-05-30_19-57-55_screenshot.png]]

#+DOWNLOADED: screenshot @ 2021-05-30 19:58:04
[[file:images/Index_(B+-_Tree)/2021-05-30_19-58-04_screenshot.png]]

#+DOWNLOADED: screenshot @ 2021-05-30 19:58:09
[[file:images/Index_(B+-_Tree)/2021-05-30_19-58-09_screenshot.png]]

1. Find, from the root, the leaf where the tuple should be inserted based on the
   key
2. Deal with possible overflow
3. Deal with possible *recursive* overflow
   - Find the meduan K of the list of the key: let l be the number of *pointers*
     in the overflow node (at this moment we can ensure that l = f_i + 1), let K
     be the ceil(l/2)th key
   - Evenly divide the keys (all keys apart from K) and pointers into two nodes:
     - First node, all keys smaller than K and all pointers before K
     - Second node: all keys larger than K and all pointers after K
     - Register K to the parent
- *remark*: ceil(l/2) is set such that when the split cannot be made even, let
  the first node be larger than the second node by one pointer

** B+-Tree: Delete
- Start at the root, find the leaf L where the entry belongs
- Remove the entry
- If L is at least half full, done; otherwise process the underflow
  1. Try to redistribute, borrowing from sibling (adjacent node under the same
     parent of L)
  2. If redistribution fails, marge L and its sibling and then delete the
     corresponding key & pointers int the parent
  3. Deal with possible recursive merges


#+DOWNLOADED: screenshot @ 2021-05-30 20:06:03
[[file:images/Index_(B+-_Tree)/2021-05-30_20-06-03_screenshot.png]]

#+DOWNLOADED: screenshot @ 2021-05-30 20:06:11
[[file:images/Index_(B+-_Tree)/2021-05-30_20-06-11_screenshot.png]]

#+DOWNLOADED: screenshot @ 2021-05-30 20:06:16
[[file:images/Index_(B+-_Tree)/2021-05-30_20-06-16_screenshot.png]]

#+DOWNLOADED: screenshot @ 2021-05-30 20:06:24
[[file:images/Index_(B+-_Tree)/2021-05-30_20-06-24_screenshot.png]]

** B+-Tree: Complexity

#+DOWNLOADED: screenshot @ 2021-05-30 20:07:34
[[file:images/Index_(B+-_Tree)/2021-05-30_20-07-34_screenshot.png]]

** B+-Tree: External Memory Sort
- *Batch Construction of B+- Tree*
  - Sort the tuples in the file
  - Construct internal nodes level by level: f_i = 4, f_l = 2
  - Cheaper than repeatedly inserting tuples by a factor of O(f_l)

** Buffered Tree
- Concurrent updates on a B+-tree
- Write intensive workload
- Buffered tree


#+DOWNLOADED: screenshot @ 2021-05-30 20:13:18
[[file:images/Index_(B+-_Tree)/2021-05-30_20-13-18_screenshot.png]]

- The update on a B+-tree
- Write intensive workload
- Buffered tree

#+DOWNLOADED: screenshot @ 2021-05-30 20:13:47
[[file:images/Index_(B+-_Tree)/2021-05-30_20-13-47_screenshot.png]]

** Indexes
- Hashing: Better index for equality search
- Spatial Database: kd-tree

** Hash
- Representing data in (key, value) pairs becomes popular
- A *hash table (hash map)* is an unordered associative array that maps keys to
  values
- A *hash function* is used to compute an offset into the array for a given key,
  from which the desired value can be found
- Space complexity O(N)
  - N denotes the number of tuples of the relation
- Operation complexity (insertion/deletion/search)
  - Average O(1)
  - Worst: O(N)

#+DOWNLOADED: screenshot @ 2021-05-30 20:16:11
[[file:images/Index_(B+-_Tree)/2021-05-30_20-16-11_screenshot.png]]

** Hashing with Chaining
- Modular hash functions: a typical hash function of h(k) = k mod p, the
  remainder of k divided by p, where p can normally take prime number, eg: 3, 5,
  7 etc


#+DOWNLOADED: screenshot @ 2021-05-30 20:16:59
[[file:images/Index_(B+-_Tree)/2021-05-30_20-16-59_screenshot.png]]

** Collision
- Collision: Two or more keys are mapped to the same hash value

#+DOWNLOADED: screenshot @ 2021-05-30 20:26:16
[[file:images/Index_(B+-_Tree)/2021-05-30_20-26-16_screenshot.png]]

- *Resolve collision by Chaining*: Use a linked list to store keys that are
  mapped to the same hash value


#+DOWNLOADED: screenshot @ 2021-05-30 20:26:48
[[file:images/Index_(B+-_Tree)/2021-05-30_20-26-48_screenshot.png]]

** TODO FINISH HERE

* Query Processing
1. Parsign and translation: SQL to *relation algebra expression*
2. Optimization: Explore *equivalent relation algebra expressions* to minimize
   the estimated cost (especially the number of IOs), generate *query-evaluation plan*
3. Evaluation: The query execution engine takes a *query evaluation plan*,
   executes that plan and returns the answers to the query

#+DOWNLOADED: screenshot @ 2021-05-30 20:29:10
[[file:images/Query_Processing/2021-05-30_20-29-10_screenshot.png]]

** Query Processing
#+DOWNLOADED: screenshot @ 2021-05-30 20:29:22
[[file:images/Query_Processing/2021-05-30_20-29-22_screenshot.png]]
- In order to do a natural join it should be student cid not student
  first-course
- So what's happening here?
  - You have a query that is a relational algebra expression
  - You are then transforming it into a (hopefully more efficient) expression
  - Then you are tagging it with an evaluation plan such as "File scan", and
    "Indexed Nested-Loop join". I'm assuming this is HOW the engine should
    evaluate each of these subcommands
- We have a whole lot of equivlaent relational algebra expressions. so we are
  picking one that makes the query the most efficient while still creating the
  same result.
- After that we are going to use some evaluation plan which tags some syntax
  onto the relational algebra expressions. With these the execution engine will
  know how to execute it.
** Query Processing
#+DOWNLOADED: screenshot @ 2021-05-30 20:33:23
[[file:images/Query_Processing/2021-05-30_20-33-23_screenshot.png]]
- Groupby is kinda aggregation, sorting based on some attributes then grouping

** Index Scan for Equality usigma_z = val & Range usigma_z = val
- *Primary index*
  - Go from the root then find a root to leaf both in which value should belong
    to.
  - If you're scanning you're just following the pointers that are between
    leaves (they are not shown on the diagram), so you have to find the first
    tuple of the result (46) and then you're going to follow the link between
    the leaves
- *Cluster Index*
  - There might be multiple tuples with the same value 46 so you're going to
    have to visit multiple blocks

#+DOWNLOADED: screenshot @ 2021-06-01 12:00:19
[[file:images/Query_Processing/2021-06-01_12-00-19_screenshot.png]]

** Index Scan for Equality usigma_z = val & Range usigma_z > val
- *Secondary Index: 1 I/O is required for each output tuple*

#+DOWNLOADED: screenshot @ 2021-06-01 12:02:26
[[file:images/Query_Processing/2021-06-01_12-02-26_screenshot.png]]
You're going to have to pay 1 I/O for each tuple that you want
- If you want z > val (Range) for Gold
  - You need GOld, Katz, Kim, Mozart, ...
- If you're going to do in index scan is based on statistics, if your stats for
  doing a different method is better then you should do the other one

** Index scan
- Given a key v, *equality search* usigma_{z = v} or *range search usigma_{z >
  v} with an index
- *A2* Primary index, equality search: H_B I/Os. H_B denotes the height of the
  B+-Tree
- *A3* Cluster index, equality search: tuples are sequentially sotred; cost is
  H_B + number of *blocks* of the output (called the reporting cost). This is
  because in order to find all equalities you have to travel over several blocks
  because there will be some equality in a neighbouring block
- *A4* Secondary index, equality search: H_B I/O for the first tuple, then 1 I/O
  for each reported tuple. Very expensive!
  - This is because you must follow pointers down (H_B pointers) to get the
    first value, then for every sub
- *A5* Primary index, range search: tuples are sequentially stored or not. Same
  with *A3*
- *A6* Secondary index, range search: each output tuple costs 1 I/O, very
  expensive
- *remarks*. The cost (number of IOs) of the first output tuple is
  H_B. Non-unique equality = search or range search >, < , >=, <= leads to
  multiple output tuples. The cost of other output tuples depends on whether
  these tuples are sequentially stored. If 1 I/O is required for each output
  tuple, sometimes file scan is cheaper

** Complex selection
- *Conjunction*
#+DOWNLOADED: screenshot @ 2021-06-01 12:15:01
[[file:images/Query_Processing/2021-06-01_12-15-01_screenshot.png]]
- A7 COnjeunctive selection using one index: one utheta_i for selection, the
  other conditions for testing/filtering
=======
* TODO Insert notes here
* NoSQL
** Storage and Retrieval
- Traditional database, B+-Tree
- NoSQL: Not only SQL

#+DOWNLOADED: screenshot @ 2021-05-31 12:07:27
[[file:images/NoSQL/2021-05-31_12-07-27_screenshot.png]]
- *Search* build hash table in main memory. *Constraint* keys must fit in main
  memory
- *Update* append only - log. *Strength* fast read and write
- See database Bitcast
- This figure above is just showing you that you can update values in a
  key/value thing
#+DOWNLOADED: screenshot @ 2021-05-31 12:09:20
[[file:images/NoSQL/2021-05-31_12-09-20_screenshot.png]]
n
** Log Structured Storage
- *The log file may run out of space. How to resolve?*
  - Break the log into segments of a certain size by
    - Closing a seg file when it reaches a certain size
    - Making the subseqent write to a new seg file
  - Compact the segments: remove duplicated keys and keep the latest value of
    each key

#+DOWNLOADED: screenshot @ 2021-05-31 12:12:44
[[file:images/NoSQL/2021-05-31_12-12-44_screenshot.png]]
- Figure: Count the number of times each cat video was played, retaining only
  the *most recent value*

** Log Structure Storage
- Compact multiple segments in one pass
- Compact in a back group thread, write the compacted pairs to a new file


#+DOWNLOADED: screenshot @ 2021-05-31 12:22:57
[[file:images/NoSQL/2021-05-31_12-22-57_screenshot.png]]
** From an Idea to Practice
- Deleting records: tombtone records
- Crash recovery: reconstruct hash maps from scratch/ hashmap snapshots of each
  segments
- Concurrency control: single write and concurrent read
- Limitations:
  - Keys must fit in main memory
  - Poor support for range serach
- Solution
  - Sorted String Table (SSTable)
    - Enforce each segment to store the key-value pairs in the order of the key
  - Log-Structured Merge Tree (LSM-Tree)
    - Maintain tree-based sparse index (on keys) on the segments for searching

** Log Structured Storage
- Merge: Multi-way Merge Sort => Sorted String Table (SSTable)

#+DOWNLOADED: screenshot @ 2021-05-31 13:16:12
[[file:images/NoSQL/2021-05-31_13-16-12_screenshot.png]]
So what you're doing here is that segment 3 is the segment that is the segment 3
takes highest priority, then segment 2 if there is no value for the key in
segment 3. etc until you have covered everything
** Log Structured Storage
- Index: instead of keeping all keys in main memory, build sparse index in
  sorted file, each index entry for each block

#+DOWNLOADED: screenshot @ 2021-05-31 13:20:22
[[file:images/NoSQL/2021-05-31_13-20-22_screenshot.png]]
- Maintain SSTables with a log structured merge tree (LSM-Tree)

** Log Structured Storage
- Maintain SStables with a log-structured merge tree (LSM-Tree)
- Main memory, keep a balanced tree to index newly arrived key-value pairs

- When the memory is full, exported the pairs to the disk as the i^th SSTable
- And the increase i by 1 (i was 0 initially)

#+DOWNLOADED: screenshot @ 2021-05-31 13:23:28
[[file:images/NoSQL/2021-05-31_13-23-28_screenshot.png]]

- Query SSTables on a Log-Structured Merge-Tree (LSM-Tree)
- Main memory, keep a balanced tree to index newly arrived key value pairs
- Search the balanced tree in main memory of the key
- If fail, visit the indexes of the green files until finding the key

#+DOWNLOADED: screenshot @ 2021-05-31 13:35:37
[[file:images/NoSQL/2021-05-31_13-35-37_screenshot.png]]

** Log Structured Storage: leveled

#+DOWNLOADED: screenshot @ 2021-05-31 13:36:15
[[file:images/NoSQL/2021-05-31_13-36-15_screenshot.png]]

** Log structured Storage: Leveled
- Bloom Filter: Speed up the query when the serach key is not in the data set

#+DOWNLOADED: screenshot @ 2021-05-31 13:38:21
[[file:images/NoSQL/2021-05-31_13-38-21_screenshot.png]]

** Transaction Processing and Analytics
- *OLTP*: Online transaction processing
  - Lookup: Al small number of records
  - Insert/update: a small number of records that are related to the user
  - ACID
- *OLAP*: Online analytical processing
  - Retrieve: Massive number of records, a few columns per record
  - Query: aggregation, statics for
    - Decision amking
  - Examples
    - What was the total revenue of each of our stores in January?
    - How many more bananas than usual did we sell during our latest promotion?
    - Loan
    - Advertisement

** Data Warehouse

#+DOWNLOADED: screenshot @ 2021-05-31 13:42:17
[[file:images/NoSQL/2021-05-31_13-42-17_screenshot.png]]

#+DOWNLOADED: screenshot @ 2021-05-31 13:42:26
[[file:images/NoSQL/2021-05-31_13-42-26_screenshot.png]]

** Schemas for Analytics: Stars and Snowflakes
- *Star schema*
  - *Fact table*: records for individual events recording who, when, where ,why
    and how of the event, typically 100+ columns
  - Column:
    - An attribute or
    - A foreign key to a dimension table
  - *Dimension table*: A smaller table on one attribute, can also be wide

#+DOWNLOADED: screenshot @ 2021-05-31 13:44:01
[[file:images/NoSQL/2021-05-31_13-44-01_screenshot.png]]

** Storage and Retrieval
- *Typical Query*: Aggregate over all rows, only relate to a few column
- *Example*: Get data to analyze whether people are more inclined to buy candy,
  depending on the day of the week

#+DOWNLOADED: screenshot @ 2021-05-31 13:49:41
[[file:images/NoSQL/2021-05-31_13-49-41_screenshot.png]]
- For fact table with trillions of rows, and hundreds of columns, column store
  is popular way of storing data

** Storage and retrieval: Column Store

#+DOWNLOADED: screenshot @ 2021-05-31 13:50:32
[[file:images/NoSQL/2021-05-31_13-50-32_screenshot.png]]

** Column Store

#+DOWNLOADED: screenshot @ 2021-05-31 13:52:53
[[file:images/NoSQL/2021-05-31_13-52-53_screenshot.png]]

- *Bit operation*. AND, OR, XOR
  -A batch of 64 bits can be compuated in one operation
- *Bitmap encoding*
  - WHERE product_sk IN (30,68.69)
  - Load of bitmaps of
    - product_sk = 30
    - product_sk = 68 and
    - product_sk = 69 and
    - CALCULATE THE BITWISE OR

- *Further optimization*
  - Vectorized processing on Single-Instruction-Multi-Data (SIMD)
  - Sort the data in multiple orders for
    - RAID
    - Fast query processing
    - Concurrency

